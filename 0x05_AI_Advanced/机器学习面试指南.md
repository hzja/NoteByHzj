机器学习<br />数据预处理

- 变量分箱的做法<br />有监督：ID3等决策树模型、IV值最大化<br />无监督：等宽、等频、聚类
- 特征选择<br />信息增益法、使用统计量，互信息、TF-IDF。
- 如何使用模型进行特征选择？<br />对特征的输入加一个noise，进行之后看树的效果有多少下降，作为的重要性。<br />使用xgboost筛选特征时，可根据以下三个维度评价特征重要性：
   1. 特征在节点分裂时被使用的次数。
   2. 特征被选中后的平均增益。
   3. 特征在分裂时所覆盖的样本数量（正相关与根节点距分裂的节点之间的路径长度）
- NLP场景中如何做数据增强（在数据量较小时提升泛化能力）
   1. 随机挑选k个进行同义词替换
   2. 随机插入噪声或删除一些词。
   3. 随机交换其中的一些词。
<a name="iAUvC"></a>
### 逻辑回归

- 模型假设、建立
- 损失函数推导（极大似然函数）
- 梯度更新推导
- 与SVM模型的异同点
   1. LR是参数模型，即假设数据来源于某个分布，而SVM是非参数模型。
   2. 使用的损失函数分别是交叉熵和Hinge loss。
   3. LR更新为，根据预测值降低较远点对模型参数的影响，而SVM仅使用支持向量。
   4. LR难以解决线性不可分的问题，而SVM可以使用各种核函数解决。
   5. SVM只需要计算少数几个支持向量。
- 与回归的关系和区别
   1. 均可用最大似然函数建模，并使用SGD学习参数。
   2. 均可使用t检验、F检验评价结果及参数有效性。
   3. 解决的问题分别是分类与回归。
   4. 逻辑回归可看作是广义的线性回归，对进行回归。
- 如何解决多分类问题？<br />可以输入属于每个类别的概率，计算损失函数，也可以使用多个二分类器，判别样本是否属于每一类。
- 有哪些处理离散、连续特征的方法？

优缺点
<a name="qSCfM"></a>
### 支持向量机

- 模型建立
- 为什么要求解SVM的对偶问题？
- 核函数的原理及作用
   1. 从模型的角度，使得模型具有拟合非线性函数的能力。根据Cover Theomem，高维空间比低维空间更容易线性可分。
   2. 从优化角度，对偶的表示能够带来内积，从而便于求解。
- 常用核函数
- SMO算法
- 与逻辑回归 异同点
<a name="ywsOA"></a>
### 聚类
变量聚类的作用是减少相关性，剔除冗余变量

- k-means算法、k-means++、带有惩罚项的k-means算法
- 能否使用类似于注意力机制中的方法定义距离
- 编程实现k-means算法
- 系统聚类法
- 变量聚类
- 如何对聚类结果进行评价

外部指标、内部指标

- 高斯混合聚类
- k-means算法是否一定会收敛，是否一定会收敛到最优解

k-means算法一定会收敛，因为点到类内中心的距离之和会单调递减，而根据单调有界原理，最终一定会收敛，但是有可能收敛到局部最小值，如A(-1,10)，B(1,10)，C(-1,-10)，D(1,10)，则将AC划分为一组，BD划分为另一组同样会收敛。
<a name="fPNO8"></a>
### 熵

- 自信息、熵、交叉熵、KL散度、互信息
- 最小化交叉熵损失与最大化对数似然的关系
<a name="BzCw3"></a>
### 决策树

- 决策树建立过程
- 三种分裂方式（ID3、C4.5、CART）
- 如何建立回归树
- 如何处理缺失值

在训练模型时根据不缺失的样本进行特征评估，在预测时认为缺失值可以等可能得取所有值。

- 有哪些优缺点

优点：解释性强、预测速度快、不需要特征工程、不需要特征标准化、能较好处理缺失、受异常点影响不大<br />缺点:用于回归时不够平滑，只能得到有限个值、对于高维稀疏数据可能因为过于重视某些特征而将其他的忽略，造成过拟合，泛化能力差（相比于LR中可以加入正则化项），一些缺点可以使用梯度提升来缓解。

- 有哪些剪枝策略

预剪枝、后剪枝
<a name="FVKa9"></a>
### 强化学习

- 几大要素
- 马尔可夫决策过程（状态、动作、奖励函数、转移概率）
- Bellman方程
- 策略迭代与值迭代<br />值迭代在每次迭代中选择最优策略，而策略迭代在策略提升时才考虑。
- Q-learning
- Deep Q Network
<a name="aq4ad"></a>
### 集成学习

- 使用集成学习的优势

精度相比于单学习器显著提升，预测的误差与方差均能减小。<br />解决局部极小值问题。<br />表示能力更强大。

- Boosting与Bagging的思想、效果提升原理
- 二者分别有哪些模型
- Adaboost、GBDT、Xgboost的原理、公式推导

GBDT使用CART作为基模型

- Adaboost算法使用什么损失函数？如何解决回归问题？
- GBDT如何用于分类

选用deviance loss或者exponential loss，每一轮训练k个数（对应k个分类），将输出进行归一化，并用标签值减去概率值，得到剩下待预测的概率。<br />最小化指数损失

- GBDT与Xgboost的区别

一阶与二阶。

- 如何使用决策树做特征工程（GBDT+LR）
- Xgboost如何加速

训练之前先将数据进行排序，保存为block结构。<br />模型本身是串行结构，无法并行构建树，但模型的内循环（在节点处计算各个特征分裂带来的收益，从而选择特征）与外循环（处理节点的各个子节点）均可进行并行计算。

- 随机森林的优点

可并行、能处理高维特征、缺省值不影响特征筛选、降低方差、受异常值影响很小。

- Light GBM的主要思想

解决boosting算法耗时的问题（Xgboost等算法每次分裂都需要遍历整个数据集）<br />直方图算法<br />两个优化：

1. GOSS（Gradient-based One-side Sampling）计算样本的梯度，并按照绝对值逆序排序，保留梯度大的部分样本点，而对于梯度小的样本只进行采样。提升了效率，且采样带来了样本多样性。
2. EFB（Exclusive-feature Bundling）构造特征图，边上的权重为特征之间的总冲突。将特征基于其在图中的度进行降序排序，并遍历，将每个特征分给具有小冲突的现有bundling或者开创新的bundling，从而减少特征。
<a name="h4VYQ"></a>
### 概率图模型

- 贝叶斯网络
- 隐马尔科夫模型、BIO标注、命名实体识别 维特比算法递推求概率
- 最大熵模型  某分布只知道其中的一部分，最大化熵对未知的部分进行估算。优点：约束可以灵活设置，解决平滑问题。缺点：只记录了特征是否出现，即二值，但无关联性、记忆性，且可能出现标注偏置，每一步都进行归一化，倾向于选择状态比较少的。
- 条件随机场、图像去噪    不像隐马尔科夫模型需要严格的独立性假设，输出状态前后独立，状态只和前一个状态有关，且融入了上下文信息。每个状态根据全局归一化计算，不会出现标注偏置，所以需要训练的参数也更多。
- 以上三者的区别与联系
- LSTM+CRF模型用于实体抽取
<a name="iUZnI"></a>
### 半监督学习

- 思想<br />未标记的样本自身的分布中包含的信息也是有价值的。
- 具体做法
<a name="dwsfm"></a>
### 评价指标

- 准确率、召回率
- 推荐系统的评价指标（AP@n、ndcg@n、rr等）
- roc曲线、auc的含义、计算<br />auc 计算auc实际上需要计算“逆序对”的个数，即负样本排在正样本前面的样本对数。可以使用O(n)的方法，将所有样本按照预测概率排序，将其中正样本的秩相加，再减去其中的正样本对数目即可得到逆序对个数。
- roc曲线与prc曲线的区别
- auc的缺点
1. 忽略了预测值，将样本预测为(0.51, 0.49)与预测为(1, 0)得到的auc是相同的。
2. 只能反映模型对正负样本的区分能力，无法反映查准率、查全率等。
3. 不关心正负样本内部的排序，即无法反应什么样本更容易出错，难以排查。
4. 两个模型的能力不同，但可能auc是相等的（即两条曲线完全不同，但曲线下的总面积相同），这种模型能力的差别无法由auc反应。
- auc是否受正阳样本比例影响<br />auc实际上是负样本排序在正样本之前的概率，分母是正负样本对，所以受正负样本比例的影响较小。
- 有哪些对auc的改进版本 gauc，即根据样本数量对auc进行加权。
- F、
