PyTorch
<a name="XgGLZ"></a>
## 准备数据
首先导入所需的模块和类。<br />![](https://cdn.nlark.com/yuque/0/2022/jpeg/396745/1659254824215-2becd2d3-d01e-4949-ad57-1cf0a4704516.jpeg#clientId=u3d5c844a-edb9-4&from=paste&id=ubf973d18&originHeight=271&originWidth=1000&originalType=url&ratio=1&rotation=0&showTitle=false&status=done&style=shadow&taskId=u18508ff6-9c8f-4cda-9055-99ef709d06e&title=)<br />使用 `torchvision.datasets` 的 MNIST 类下载数据并创建一个 PyTorch 数据集。<br />![](https://cdn.nlark.com/yuque/0/2022/png/396745/1659254824306-19f6ca54-2f53-4bc2-8373-38e9f7664775.png#clientId=u3d5c844a-edb9-4&from=paste&id=ue3d0e49f&originHeight=147&originWidth=1000&originalType=url&ratio=1&rotation=0&showTitle=false&status=done&style=shadow&taskId=u805cef9f-264d-44f4-80cf-471e57af294&title=)<br />接下来，定义并使用一个函数 split_indices 来随机选取 20% 图像作为验证集。<br />![](https://cdn.nlark.com/yuque/0/2022/jpeg/396745/1659254824220-a14b6704-2132-4979-b024-3ecef304a07b.jpeg#clientId=u3d5c844a-edb9-4&from=paste&id=ud8c9d7c9&originHeight=512&originWidth=1000&originalType=url&ratio=1&rotation=0&showTitle=false&status=done&style=shadow&taskId=u6fbd1366-c11b-46b7-88bf-cae188499c5&title=)<br />现在，可以使用 SubsetRandomSampler 为每个子集创建 PyTorch 数据加载器，它可从一个给定的索引列表中随机地采样元素，同时创建分批数据。<br />![](https://cdn.nlark.com/yuque/0/2022/jpeg/396745/1659254824234-88eff3ba-a807-4bfd-a13f-b80ee77c1f8a.jpeg#clientId=u3d5c844a-edb9-4&from=paste&id=u69a5c592&originHeight=475&originWidth=1000&originalType=url&ratio=1&rotation=0&showTitle=false&status=done&style=shadow&taskId=udcd456e6-8102-4621-bca3-db6d8ffab28&title=)
<a name="jzEcQ"></a>
## 模型
要在 logistic 回归的基础上实现进一步提升，将创建一个带有一个隐藏层（hidden layer）的神经网络。这做法：

- 不再使用单个 nn.Linear 对象将输入批（像素强度）转换成输出批（类别概率），而是将使用两个 nn.Linear 对象。其中每一个对象都被称为一层，而该模型本身则被称为一个网络。
- 第一层（也被称为隐藏层）可将大小为 batch_size x 784 的输入矩阵转换成大小为 batch_size x hidden_size 的中间输出矩阵，其中 hidden_size 是一个预配置的参数（比如 32 或 64）。
- 然后，这个中间输出会被传递给一个非线性激活函数，它操作的是这个输出矩阵的各个元素。
- 这个激活函数的结果的大小也为 batch_size x hidden_size，其会被传递给第二层（也被称为输出层）。该层可将隐藏层的结果转换成一个大小为 batch_size x 10 的矩阵，这与 logistic 回归模型的输出一样。

引入隐藏层和激活函数让模型学习输入与目标之间更复杂的、多层的和非线性的关系。看起来像是这样（蓝框表示单张输入图像的层输出）：<br />![](https://cdn.nlark.com/yuque/0/2022/jpeg/396745/1659254824297-cea05cc0-468a-4df8-80e5-0d3c092ce788.jpeg#clientId=u3d5c844a-edb9-4&from=paste&id=u87be9614&originHeight=470&originWidth=700&originalType=url&ratio=1&rotation=0&showTitle=false&status=done&style=shadow&taskId=u544de727-dbf7-455d-bfcf-de6d0964143&title=)<br />这里将使用的激活函数是整流线性单元（ReLU），它的公式很简单：relu(x) = max(0,x)，即如果一个元素为负，则将其替换成 0，否则保持不变。<br />为了定义模型，对 `nn.Module` 类进行扩展，就像使用 logistic 回归时那样。<br />![](https://cdn.nlark.com/yuque/0/2022/jpeg/396745/1659254824646-20a8f1e9-c935-44f1-9398-2d75e02f84a0.jpeg#clientId=u3d5c844a-edb9-4&from=paste&id=uad9f8d0a&originHeight=793&originWidth=1000&originalType=url&ratio=1&rotation=0&showTitle=false&status=done&style=shadow&taskId=ucbcd4946-0a5f-4b8b-91eb-6c46f645da4&title=)<br />将创建一个带有 32 个激活的隐藏层的模型。<br />![](https://cdn.nlark.com/yuque/0/2022/jpeg/396745/1659254824662-8f14a8eb-e71c-4325-a129-763dbc8865a0.jpeg#clientId=u3d5c844a-edb9-4&from=paste&id=u9f195557&originHeight=205&originWidth=1000&originalType=url&ratio=1&rotation=0&showTitle=false&status=done&style=shadow&taskId=ucaeb6aa9-f501-4c63-9567-3210af74c52&title=)<br />看看模型的参数。可以预见每一层都有一个权重和偏置矩阵。<br />![](https://cdn.nlark.com/yuque/0/2022/jpeg/396745/1659254824723-97d6326e-db23-4796-8451-0742be0eedd5.jpeg#clientId=u3d5c844a-edb9-4&from=paste&id=u01be1e35&originHeight=247&originWidth=1000&originalType=url&ratio=1&rotation=0&showTitle=false&status=done&style=shadow&taskId=u60a7eae9-90c2-415b-8a11-da51c81045c&title=)<br />试试用模型生成一些输出。从数据集取第一批 100 张图像，并将其传入模型。<br />![](https://cdn.nlark.com/yuque/0/2022/jpeg/396745/1659254824662-0687e761-31da-4ffb-8ca8-b37cf81ec73c.jpeg#clientId=u3d5c844a-edb9-4&from=paste&id=u168d6c93&originHeight=579&originWidth=1000&originalType=url&ratio=1&rotation=0&showTitle=false&status=done&style=shadow&taskId=u92db7475-e9cd-44af-b90a-10364051e45&title=)
<a name="G757s"></a>
## 使用 GPU
随着模型和数据集规模增大，为了在合理的时间内完成模型训练，需要使用 GPU（图形处理器，也被称为显卡）来训练模型。GPU 包含数百个核，这些核针对成本高昂的浮点数矩阵运算进行了优化，可以在较短时间内完成这些计算；这也因此使得 GPU 非常适合用于训练具有很多层的深度神经网络。可以在 Kaggle kernels 或 Google Colab 上免费使用 GPU，也可以租用 Google Cloud Platform、Amazon Web Services 或 Paperspace 等 GPU 使用服务。可以使用 `torch.cuda.is_available` 检查 GPU 是否可用以及是否已经安装了所需的英伟达驱动和 CUDA 库。<br />![](https://cdn.nlark.com/yuque/0/2022/png/396745/1659254825057-53938706-0c87-48c1-8d56-6a671b69f425.png#clientId=u3d5c844a-edb9-4&from=paste&id=ud8801943&originHeight=121&originWidth=1000&originalType=url&ratio=1&rotation=0&showTitle=false&status=done&style=shadow&taskId=u0925f41b-48d0-45bf-b9a1-4dfba3c602e&title=)<br />定义一个辅助函数，以便在有 GPU 时选择 GPU 为目标设备，否则就默认选择 CPU。<br />![](https://cdn.nlark.com/yuque/0/2022/jpeg/396745/1659254825036-62e098e1-b4b5-4659-9cb2-04d3551e2ffc.jpeg#clientId=u3d5c844a-edb9-4&from=paste&id=u6fabdba8&originHeight=325&originWidth=1000&originalType=url&ratio=1&rotation=0&showTitle=false&status=done&style=shadow&taskId=u917c489c-8746-494e-9a00-5a48fec7f13&title=)<br />接下来，定义一个可将数据移动到所选设备的函数。<br />![](https://cdn.nlark.com/yuque/0/2022/jpeg/396745/1659254825085-8a496fc0-0d7c-4c71-9ec7-79ac79224eca.jpeg#clientId=u3d5c844a-edb9-4&from=paste&id=u9b3aecda&originHeight=209&originWidth=1000&originalType=url&ratio=1&rotation=0&showTitle=false&status=done&style=shadow&taskId=u986d04c7-5301-4e79-af14-642b5ed9201&title=)<br />最后，定义一个 DeviceDataLoader 类（受 FastAI 的启发）来封装已有的数据加载器并在读取数据批时将数据移动到所选设备。有意思的是，不需要扩展已有的类来创建 PyTorch 数据加载器。只需要用 `__iter__` 方法来检索数据批并使用 __len__ 方法来获取批数量即可。<br />![](https://cdn.nlark.com/yuque/0/2022/jpeg/396745/1659254825144-fc2720dc-76b8-4852-b155-60ec0bda5922.jpeg#clientId=u3d5c844a-edb9-4&from=paste&id=ude8dbe44&originHeight=489&originWidth=1000&originalType=url&ratio=1&rotation=0&showTitle=false&status=done&style=shadow&taskId=u4e20a78c-1479-4744-bb82-71ef6003d03&title=)<br />现在可以使用 DeviceDataLoader 来封装数据加载器了。<br />![](https://cdn.nlark.com/yuque/0/2022/png/396745/1659254825124-414c0c51-fb94-4974-945c-a253a7947c2d.png#clientId=u3d5c844a-edb9-4&from=paste&id=u9ac12b1c&originHeight=102&originWidth=1000&originalType=url&ratio=1&rotation=0&showTitle=false&status=done&style=shadow&taskId=ud35bb477-125b-4d5a-a4aa-03d2f2bdf07&title=)<br />已被移动到 GPU 的 RAM 的张量有一个 device 属性，其中包含 cuda 这个词。通过查看 valid_dl 的一批数据来验证这一点。<br />![](https://cdn.nlark.com/yuque/0/2022/jpeg/396745/1659254825292-49593a7e-6443-4901-aedc-42bac5f4383d.jpeg#clientId=u3d5c844a-edb9-4&from=paste&id=u39d746cd&originHeight=516&originWidth=1000&originalType=url&ratio=1&rotation=0&showTitle=false&status=done&style=shadow&taskId=u1bed1086-bca8-413d-9884-dc70823b389&title=)
<a name="NlqWt"></a>
## 训练模型
和 logistic 回归一样，可以使用交叉熵作为损失函数，使用准确度作为模型的评估指标。训练循环也是一样的，所以可以复用前一个教程的 loss_batch、evaluate 和 fit 函数。<br />loss_batch 函数计算的是一批数据的损失和指标值，并且如果提供了优化器就可选择执行梯度下降。<br />![](https://cdn.nlark.com/yuque/0/2022/jpeg/396745/1659254825370-e167881e-f3a1-40dd-9e44-0b818d499e40.jpeg#clientId=u3d5c844a-edb9-4&from=paste&id=u24015d8a&originHeight=616&originWidth=1000&originalType=url&ratio=1&rotation=0&showTitle=false&status=done&style=shadow&taskId=u347a8310-6b32-44d6-b754-534359ad033&title=)<br />evaluate 函数是为验证集计算整体损失（如果有，还计算一个指标）。<br />![](https://cdn.nlark.com/yuque/0/2022/jpeg/396745/1659254825339-4b92780c-c6fd-4ab8-8cfc-c81586d68854.jpeg#clientId=u3d5c844a-edb9-4&from=paste&id=ub3365873&originHeight=501&originWidth=1000&originalType=url&ratio=1&rotation=0&showTitle=false&status=done&style=shadow&taskId=ucc34f78c-8ec9-4165-b024-fe8a0827a09&title=)<br />和之前教程中定义的一样，fit 函数包含实际的训练循环。将对 fit 函数进行一些改进：

- 没有人工地定义优化器，而是将传入学习率并在该函数中创建一个优化器。这在有需要时能以不同的学习率训练模型。
- 将记录每 epoch 结束时的验证损失和准确度，并返回这个历史作为 fit 函数的输出。

![](https://cdn.nlark.com/yuque/0/2022/jpeg/396745/1659254825501-2acc218c-a248-4004-a8b5-1765d090611e.jpeg#clientId=u3d5c844a-edb9-4&from=paste&id=u630ad6bf&originHeight=914&originWidth=1000&originalType=url&ratio=1&rotation=0&showTitle=false&status=done&style=shadow&taskId=u20876e26-9236-4d36-9f6e-612892b5901&title=)<br />还要定义一个 accuracy 函数，其计算的是模型在整批输出上的整体准确度，所以可将其用作 fit 中的指标。<br />![](https://cdn.nlark.com/yuque/0/2022/png/396745/1659254825470-12c7d128-0620-4bb4-bc80-723ef6e914ed.png#clientId=u3d5c844a-edb9-4&from=paste&id=u0940585f&originHeight=146&originWidth=998&originalType=url&ratio=1&rotation=0&showTitle=false&status=done&style=shadow&taskId=u3e94da65-0157-4e46-bc84-01b96c5a0ae&title=)<br />在训练模型之前，需要确保数据和模型参数（权重和偏置）都在同一设备上（CPU 或 GPU）。可以复用 to_device 函数来将模型参数移至正确的设备。<br />![](https://cdn.nlark.com/yuque/0/2022/jpeg/396745/1659254825693-d0930a8b-68a8-4637-a62d-c6e02853c023.jpeg#clientId=u3d5c844a-edb9-4&from=paste&id=u9c2601ba&originHeight=259&originWidth=1000&originalType=url&ratio=1&rotation=0&showTitle=false&status=done&style=shadow&taskId=uf4b20993-a896-40db-b0d6-540dfa95cab&title=)<br />看看使用初始权重和偏置时，模型在验证集上的表现。<br />![](https://cdn.nlark.com/yuque/0/2022/jpeg/396745/1659254825678-86a5d2ea-e52d-4140-839d-949173c8748f.jpeg#clientId=u3d5c844a-edb9-4&from=paste&id=ua133c2ef&originHeight=173&originWidth=1000&originalType=url&ratio=1&rotation=0&showTitle=false&status=done&style=shadow&taskId=u9529be2d-8a81-4a9e-92b2-0da78dfe924&title=)<br />初始准确度大约是 10%，这符合对随机初始化模型的预期（其有十分之一的可能性得到正确标签）。<br />现在可以开始训练模型了。先训练 5 epoch 看看结果。可以使用相对较高的学习率 0.5。<br />![](https://cdn.nlark.com/yuque/0/2022/jpeg/396745/1659254825796-f7a0b530-db4b-4645-9cec-5137e29e4cad.jpeg#clientId=u3d5c844a-edb9-4&from=paste&id=ua8d29c62&originHeight=292&originWidth=1000&originalType=url&ratio=1&rotation=0&showTitle=false&status=done&style=shadow&taskId=u987b43d5-b812-42b4-94e1-69ab9c435d5&title=)<br />95% 非常好了！再以更低的学习率 0.1 训练 5 epoch，以进一步提升准确度。<br />![](https://cdn.nlark.com/yuque/0/2022/jpeg/396745/1659254825820-c3c82cdc-19fa-4632-8ce3-c0c9cc23db04.jpeg#clientId=u3d5c844a-edb9-4&from=paste&id=u21676edf&originHeight=294&originWidth=1000&originalType=url&ratio=1&rotation=0&showTitle=false&status=done&style=shadow&taskId=u4332377f-97ee-45fb-8cb9-8a212bf1d7f&title=)<br />现在可以绘制准确度图表，看看模型随时间的提升情况。<br />![](https://cdn.nlark.com/yuque/0/2022/jpeg/396745/1659254825829-a1ca8c17-c7ba-4f65-8fc3-84b44966d120.jpeg#clientId=u3d5c844a-edb9-4&from=paste&id=u9ed40e4d&originHeight=868&originWidth=1000&originalType=url&ratio=1&rotation=0&showTitle=false&status=done&style=shadow&taskId=u375451dc-b377-4b4f-83ae-c4315955456&title=)<br />当前的模型极大地优于 logistic 模型（仅能达到约 86% 的准确度）！它很快就达到了 96% 的准确度，但没能实现进一步提升。如果要进一步提升准确度，需要让模型更加强大。可能也已经猜到了，通过增大隐藏层的规模或添加更多隐藏层可以实现这一目标。
<a name="bmJPT"></a>
## 提交和上传笔记
最后，可以使用 jovian 库保存和提交成果。<br />![](https://cdn.nlark.com/yuque/0/2022/jpeg/396745/1659254825933-754cc984-ca64-415b-a69c-98c13ad32254.jpeg#clientId=u3d5c844a-edb9-4&from=paste&id=u9c4a662c&originHeight=443&originWidth=1000&originalType=url&ratio=1&rotation=0&showTitle=false&status=done&style=shadow&taskId=ua7882a74-0694-445f-b85a-4bfba1c48c6&title=)<br />jovian 会将笔记上传到 https://jvn.io，并会获取其 Python 环境并为该笔记创建一个可分享的链接。可以使用该链接共享成果，让任何人都能使用 jovian 克隆命令轻松复现它。jovian 还有一个强大的评论接口，可以让其他人都能讨论和点评笔记的各个部分。
<a name="ZjDGP"></a>
## 总结与进阶阅读
本教程涵盖的主题总结如下：

- 创建了一个带有一个隐藏层的神经网络，以在前一个教程的 logistic 回归模型基础上实现进一步提升。
- 使用了 ReLU 激活函数来引入非线性，让模型可以学习输入和输出之间的更复杂的关系。
- 定义了 get_default_device、to_device 和 DeviceDataLoader 等一些实用程序，以便在可使用 GPU 时利用它，并将输入数据和模型参数移动到合适的设备。
- 可以使用之前定义的同样的训练循环：fit 函数，来训练模型以及在验证数据集上评估它。

其中有很多可以实验的地方，建议使用 Jupyter 的交互性质试试各种不同的参数。这里有一些想法：

- 试试修改隐藏层的大小或添加更多隐藏层，看能否实现更高的准确度。
- 试试修改批大小和学习率，看能否用更少的 epoch 实现同样的准确度。
- 比较在 CPU 和 GPU 上的训练时间。可以看到存在显著差异吗？数据集的大小和模型的大小（权重和参数的数量）对其有何影响？
- 试试为不同的数据集构建模型，比如 CIFAR10 或 CIFAR100 数据集。

最后，分享一些适合进一步学习的好资源：

- 神经网络可以计算任何函数的视觉式证明，也被称为通用近似定理：[http://neuralnetworksanddeeplearning.com/chap4.html](http://neuralnetworksanddeeplearning.com/chap4.html)
- 神经网络究竟是什么？——通过视觉和直观的介绍解释了神经网络以及中间层所表示的内容：[https://www.youtube.com/watch?v=aircAruvnKk](https://www.youtube.com/watch?v=aircAruvnKk)
- 斯坦福大学 CS229 关于反向传播的讲义——更数学地解释了多层神经网络计算梯度和更新权重的方式：[http://cs229.stanford.edu/notes/cs229-notes-backprop.pdf](http://cs229.stanford.edu/notes/cs229-notes-backprop.pdf)
- 吴恩达的 Coursera 课程：关于激活函数的视频课程：[https://www.coursera.org/lecture/neural-networks-deep-learning/activation-functions-4dDC1](https://www.coursera.org/lecture/neural-networks-deep-learning/activation-functions-4dDC1)
