时间序列<br />在 Python 中用箱线图、傅里叶变换、熵、自相关和 PCA 分析时间序列数据。数据可视化是任何数据相关项目中最重要的阶段之一。根据数据可视化的对象，有：

1. 数据可视化报告结果。 
2. 数据可视化来分析数据，换句话说，数据科学家内部使用的可视化来提取有关数据的信息，然后实施模型。

这里主要关注后一种，因为它解释了一些有助于分析时间序列数据的方法。
<a name="uZTli"></a>
## 什么是时间序列？
基本数值时间序列是有序的、带时间戳的观测值（测量值）的集合，其中每个观测值都是从同一测量过程中获得的数值标量。
<a name="pFDxM"></a>
## 什么是时间戳？
在将“时间”捕获为数据点之前，不会深入探讨需要精确定义的许多细节（准确性、格式、日历约定、时区等等）。将时间戳定义为具有所需精度的时间点的表示就足够了。例如，这可能是根据某个日历的日期约定（例如“08-06-2020”），或者自 1970 年以来以整数表示的毫秒数（这实际上是 UNIX 纪元约定！）
<a name="B2MPA"></a>
## Python类库
首先，这些是与 notebook 一起使用的库。大多数代码都围绕 NumPy 和 Pandas库，因为数据主要以 Pandas Dataframe 表现的 NumPy 数组。
```python
import numpy as np
import pandas as pd
import seaborn as sns

import matplotlib
import matplotlib.pyplot as plt
from matplotlib.colors import LogNorm
from matplotlib.dates import date2num

from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.decomposition import PCA
from sklearn.pipeline import make_pipeline
from sklearn.feature_selection import SelectKBest, chi2

from statsmodels.tsa.stattools import adfuller
from statsmodels.graphics.tsaplots import plot_acf

import warnings
warnings.filterwarnings("ignore")
```
<a name="Ot4YD"></a>
## 导入文件
数据文件：[data.csv](https://www.yuque.com/attachments/yuque/0/2022/csv/396745/1663204924741-5ba4fc8e-56ab-426c-8729-ce728bd71097.csv?_lake_card=%7B%22src%22%3A%22https%3A%2F%2Fwww.yuque.com%2Fattachments%2Fyuque%2F0%2F2022%2Fcsv%2F396745%2F1663204924741-5ba4fc8e-56ab-426c-8729-ce728bd71097.csv%22%2C%22name%22%3A%22data.csv%22%2C%22size%22%3A877548%2C%22type%22%3A%22text%2Fcsv%22%2C%22ext%22%3A%22csv%22%2C%22source%22%3A%22%22%2C%22status%22%3A%22done%22%2C%22download%22%3Atrue%2C%22taskId%22%3A%22u4a8bddc1-924f-4c12-a358-99175e21505%22%2C%22taskType%22%3A%22upload%22%2C%22__spacing%22%3A%22both%22%2C%22id%22%3A%22ucf81c0bf%22%2C%22margin%22%3A%7B%22top%22%3Atrue%2C%22bottom%22%3Atrue%7D%2C%22card%22%3A%22file%22%7D)<br />下载数据后，运行以下代码将其导入。
```python
df_orig = pd.read_csv('data/data.csv', 
          usecols=['datetime', 'machine_status', 
          'sensor_00', 'sensor_10', 'sensor_20', 
          'sensor_30', 'sensor_40', 'sensor_50'])
df_orig['datetime'] = pd.to_datetime(df_orig['datetime'])
cond_1 = df_orig['datetime'] >= '2018-04-12 00:00:00'
cond_2 = df_orig['datetime'] <= '2018-04-19 00:00:00'
df_orig = df_orig[cond_1 & cond_2]
```
![](https://cdn.nlark.com/yuque/0/2022/png/396745/1663204382994-43f658bc-9716-4362-85ab-46d8f76b44a1.png#clientId=u81d3986c-2799-4&from=paste&id=u872d440c&originHeight=247&originWidth=1080&originalType=url&ratio=1&rotation=0&showTitle=false&status=done&style=shadow&taskId=u892082d9-a999-434a-968d-428fcb3a5a0&title=)<br />正如所观察到的，数据包含六个传感器的传感器数据、每个数据点的日期时间以及机器状态。这是“BROKEN”、“NORMAL”或“RECOVERING”，但为了简化可视化，它被分组如下：
```python
{'BROKEN', 'NORMAL', 'RECOVERING'}
0: {'NORMAL', 'RECOVERING'}
1: {'BROKEN'}
```
在任何编程语言中使用日期时间总是具有挑战性的，Python 也不例外。尽管处理日期时间有多种方法，但这里使用函数 `pandas.to_datetime` 将 datetime 列（读取为字符串）转换为时间戳。
```python
df_orig = pd.read_csv('data/data.csv')
print(type(df_orig['sensor_00'].iloc[0]), 
      type(df_orig['datetime'].iloc[0]))
df_orig['datetime'] = pd.to_datetime(df_orig['datetime'])
print(type(df_orig['sensor_00'].iloc[0]), 
      type(df_orig['datetime'].iloc[0]))
```
```python
<class 'str'> 
<class 'pandas._libs.tslibs.timestamps.Timestamp'>
```
<a name="GPvIe"></a>
## 数据预处理
在进行可视化之前，分析了本次数据的重复值和缺失值。并且删除重复项的函数：
```python
def drop_duplicates(df: pd.DataFrame(), subset: list = ['DATE_TIME']) -> pd.DataFrame():
    df = df.drop_duplicates((subset))
    return df
```
填充缺失值的函数：
```python
def fill_missing_date(df: pd.DataFrame(), column_datetime: str ='DATE_TIME'):
    print(f'输入形状: {df.shape}')

    data_s = df.drop([column_datetime], axis=1)
    datetime_s = df[column_datetime].astype(str)
    
    start_date = min(df[column_datetime])
    end_date = max(df[column_datetime])
    date_s = pd.date_range(start_date, end_date, freq="min").strftime('%Y-%m-%d %H:%M:%S')
    
    data_processed_s = []
    for date_val in date_s:
        pos = np.where(date_val == datetime_s)[0]        
        assert len(pos) in [0, 1]
        if len(pos) == 0:
            data = [date_val] + [0] * data_s.shape[1]
        elif len(pos) == 1:
            data = [date_val] + data_s.iloc[pos].values.tolist()[0]
        data_processed_s.append(data)
    
    df_processed = pd.DataFrame(data_processed_s, columns=[column_datetime] + data_s.columns.values.tolist())
    df_processed[column_datetime] = pd.to_datetime(df_processed[column_datetime])
    print(f'输出形状: {df_processed.shape}')
    
    return df_processed
```
这是预处理阶段的整个管道。此外，数据分为输入数据和输出数据。
```python
df_processed = drop_duplicates(df_orig, subset=['datetime'])
df = fill_missing_date(df_processed, column_datetime='datetime')
```
输入形状：(10081, 7) <br />输出形状：(10081, 2)<br />![](https://cdn.nlark.com/yuque/0/2022/png/396745/1663204383082-ed749811-3759-4634-9993-1820ca478bde.png#clientId=u81d3986c-2799-4&from=paste&id=u68356030&originHeight=498&originWidth=1080&originalType=url&ratio=1&rotation=0&showTitle=false&status=done&style=shadow&taskId=ub66763e7-39b9-41b4-980d-521b3aa8ce9&title=)
```python
df_data = df.drop(columns=['machine_status'], axis=1)
df_labels = df[['datetime', 'machine_status']].copy()
df_labels['machine_status'][df_labels['machine_status'] != 'BROKEN'] = 0
df_labels['machine_status'][df_labels['machine_status'] == 'BROKEN'] = 1
```
<a name="uAClY"></a>
## 数据可视化
现在，准备开始数据可视化。这是传感器数据和异常情况的图。
```python
df_data_hour = df_data.groupby(
          pd.Grouper(key='datetime', 
                    axis=0, freq='H')).mean()
df_labels_hour = df_labels.groupby(
          pd.Grouper(key='datetime', 
                    axis=0, freq='H')).sum()
for name in df.columns:
    if name not in ['datetime', 'machine_status']:
        fig, axs = plt.subplots(1, 1, figsize=(15, 2))
        axs.plot(df_data_hour[name], color='blue')
        axs_twinx = axs.twinx()
        axs_twinx.plot(df_labels_hour['machine_status'], color='red')
        axs.set_title(name)
        plt.show()
```
![](https://cdn.nlark.com/yuque/0/2022/png/396745/1663204383085-e1969ad4-3a59-4b32-8840-eaa15485f82c.png#clientId=u81d3986c-2799-4&from=paste&id=u5cca7c78&originHeight=156&originWidth=900&originalType=url&ratio=1&rotation=0&showTitle=false&status=done&style=shadow&taskId=ub91ea943-1f54-4113-95b2-5a228f65294&title=)<br />![](https://cdn.nlark.com/yuque/0/2022/png/396745/1663204383075-80dcb235-8dbf-4b9e-9ad9-f0911fa7ba9e.png#clientId=u81d3986c-2799-4&from=paste&id=u907b2ad0&originHeight=156&originWidth=900&originalType=url&ratio=1&rotation=0&showTitle=false&status=done&style=shadow&taskId=uc45c3901-4c48-48c1-9b92-c00030d36d3&title=)![](https://cdn.nlark.com/yuque/0/2022/png/396745/1663204383121-79bba195-8ca8-4f80-8a66-9162d5afa4c4.png#clientId=u81d3986c-2799-4&from=paste&id=u1e1c5fc9&originHeight=156&originWidth=906&originalType=url&ratio=1&rotation=0&showTitle=false&status=done&style=shadow&taskId=u06824ec8-61d6-4fc5-ae95-4cd8efc3d82&title=)![](https://cdn.nlark.com/yuque/0/2022/png/396745/1663204383582-a27aab51-6536-4539-a46a-ecb68154e80e.png#clientId=u81d3986c-2799-4&from=paste&id=udae4cb73&originHeight=156&originWidth=906&originalType=url&ratio=1&rotation=0&showTitle=false&status=done&style=shadow&taskId=uf1e46c6d-3427-4ae0-bb85-e99794fe0fe&title=)![](https://cdn.nlark.com/yuque/0/2022/png/396745/1663204383728-6427d300-a510-483d-9342-120fbc457e2d.png#clientId=u81d3986c-2799-4&from=paste&id=u1e33eb69&originHeight=156&originWidth=906&originalType=url&ratio=1&rotation=0&showTitle=false&status=done&style=shadow&taskId=u6ef6de63-4882-4aca-aed9-213e16ca576&title=)![](https://cdn.nlark.com/yuque/0/2022/png/396745/1663204383746-1851852e-aa1b-4b1e-8c7f-f0e977900e29.png#clientId=u81d3986c-2799-4&from=paste&id=u6a76fc65&originHeight=156&originWidth=906&originalType=url&ratio=1&rotation=0&showTitle=false&status=done&style=shadow&taskId=u7cbd6edc-b6d8-4bbc-b903-74b030ebba8&title=)
<a name="gWVXx"></a>
## 均值和标准
可以更好地总结数据随时间变化的行为的最基本图之一是均值标准图，在其中显示按时间范围分组的均值和标准差。这主要有助于分析指定时间范围内的基线和噪声。
```python
df_data_hour = df_data.groupby(pd.Grouper(key='datetime', axis=0, freq='H')).mean()
df_labels_hour = df_labels.groupby(pd.Grouper(key='datetime', axis=0, freq='H')).sum()

df_rollmean = df_data_hour.resample(rule='D').mean()
df_rollstd = df_data_hour.resample(rule='D').std()

for name in df.columns:
    if name not in ['datetime', 'machine_status']:
        fig, axs = plt.subplots(1, 1, figsize=(15, 2))
        axs.plot(df_data_hour[name], color='blue', label='Original')
        axs.plot(df_rollmean[name], color='red', label='Rolling Mean')
        plt.plot(df_rollstd[name], color='black', label='Rolling Std' )
        axs.set_title(name)
        plt.legend()
        plt.show()
```
![](https://cdn.nlark.com/yuque/0/2022/png/396745/1663204383745-a1c14f8f-fa92-49bb-bbb5-b9fa203c9031.png#clientId=u81d3986c-2799-4&from=paste&id=uf77549b1&originHeight=156&originWidth=864&originalType=url&ratio=1&rotation=0&showTitle=false&status=done&style=shadow&taskId=u13cbfc60-1808-4f90-b986-df412a80eb7&title=)<br />![](https://cdn.nlark.com/yuque/0/2022/png/396745/1663204383911-5c5ffa0e-8bfa-4b92-aa0d-fe76592dc4b2.png#clientId=u81d3986c-2799-4&from=paste&id=uaf19f974&originHeight=156&originWidth=871&originalType=url&ratio=1&rotation=0&showTitle=false&status=done&style=shadow&taskId=u1cb726f5-8896-4cd8-963a-4416b44955a&title=)![](https://cdn.nlark.com/yuque/0/2022/png/396745/1663204383915-318f82dd-273d-48f9-ab8c-476775f550ab.png#clientId=u81d3986c-2799-4&from=paste&id=udd6fb101&originHeight=156&originWidth=877&originalType=url&ratio=1&rotation=0&showTitle=false&status=done&style=shadow&taskId=ucd9dcac2-d35e-49eb-95da-875d01dce7e&title=)![](https://cdn.nlark.com/yuque/0/2022/png/396745/1663204384112-fb8403de-0802-4f2c-850c-7bb37018b29b.png#clientId=u81d3986c-2799-4&from=paste&id=u895d5d68&originHeight=156&originWidth=877&originalType=url&ratio=1&rotation=0&showTitle=false&status=done&style=shadow&taskId=u80f8d2ba-f634-4aa4-9f2a-9deee812f7e&title=)![](https://cdn.nlark.com/yuque/0/2022/png/396745/1663204384127-7f8d5374-8fb4-4761-b2aa-6babe897b7fa.png#clientId=u81d3986c-2799-4&from=paste&id=ud82a320f&originHeight=156&originWidth=877&originalType=url&ratio=1&rotation=0&showTitle=false&status=done&style=shadow&taskId=u3d5403bb-1b9f-41bf-a8ff-dcaeb21e962&title=)![](https://cdn.nlark.com/yuque/0/2022/png/396745/1663204384270-65b3cfa4-8b4e-4329-8b18-89f8e59af82b.png#clientId=u81d3986c-2799-4&from=paste&id=u2cf3184e&originHeight=156&originWidth=877&originalType=url&ratio=1&rotation=0&showTitle=false&status=done&style=shadow&taskId=u4a907e51-6ad6-4a35-ac1f-2c6f31cd979&title=)
<a name="tsL4c"></a>
## 箱形图
另一个有趣的图表是通过箱线图显示的。箱线图是一种通过四分位数以图形方式显示数值数据的局部性、扩散性和偏度组的方法。有两个主要框表示从第25个百分位数到第75个百分位数的数据，两者之间用分布的中位数隔开。除了盒子之外，还有从盒子延伸出来的晶须，表明上四分位和下四分位之外的变异性。与数据集其他部分显著不同的异常值也被绘制为箱线图上须之外的单独点。<br />这一个类似于平均和标准图，因为它表明数据的平稳性。但是，它也可以显示异常值，这有助于从视觉上检测异常和数据之间的任何关系。
```python
df_boxplot = df_data.copy()
df_boxplot['date'] = df_boxplot['datetime'].dt.strftime('%Y-%m-%d')
for name in df_boxplot.columns:
    if name not in ['datetime', 'date']:
        fig, axs = plt.subplots(1, 1, figsize=(15, 2))
        sns.boxplot(y=name, x='date', data=df_boxplot)
        axs.set_ylabel('Value')
        axs.set_title(name)
        plt.show()
```
![](https://cdn.nlark.com/yuque/0/2022/png/396745/1663204384337-d03eb362-78b1-4675-9e94-4c4b1fc8ea4b.png#clientId=u81d3986c-2799-4&from=paste&id=u25848710&originHeight=170&originWidth=878&originalType=url&ratio=1&rotation=0&showTitle=false&status=done&style=shadow&taskId=u8182f579-751a-44c1-a923-d84188138a9&title=)<br />![](https://cdn.nlark.com/yuque/0/2022/png/396745/1663204384432-f6d8aba4-24d7-4c26-9374-046b30358750.png#clientId=u81d3986c-2799-4&from=paste&id=uc95619aa&originHeight=170&originWidth=885&originalType=url&ratio=1&rotation=0&showTitle=false&status=done&style=shadow&taskId=u312c8489-f983-4120-b372-a1d2eabe7b5&title=)![](https://cdn.nlark.com/yuque/0/2022/png/396745/1663204384634-00e06b78-9092-46f0-add1-08327557e365.png#clientId=u81d3986c-2799-4&from=paste&id=u30afa2c7&originHeight=170&originWidth=891&originalType=url&ratio=1&rotation=0&showTitle=false&status=done&style=shadow&taskId=u65585717-65e8-416c-a05a-cd4962db2b1&title=)![](https://cdn.nlark.com/yuque/0/2022/png/396745/1663204384700-433e7f6e-0cbe-48ec-9cf5-b300234ee297.png#clientId=u81d3986c-2799-4&from=paste&id=uff5eadd1&originHeight=170&originWidth=891&originalType=url&ratio=1&rotation=0&showTitle=false&status=done&style=shadow&taskId=u6787b0af-51b1-482b-9125-857cc184cfb&title=)![](https://cdn.nlark.com/yuque/0/2022/png/396745/1663204384904-b755bee6-dbc5-40f6-a697-4b681b25f938.png#clientId=u81d3986c-2799-4&from=paste&id=ubdaf50dd&originHeight=170&originWidth=891&originalType=url&ratio=1&rotation=0&showTitle=false&status=done&style=shadow&taskId=u9148d822-0b65-4951-8e9e-36bce0ebe97&title=)![](https://cdn.nlark.com/yuque/0/2022/png/396745/1663204384839-485d758c-8c34-41df-a8ce-1feac1525a5a.png#clientId=u81d3986c-2799-4&from=paste&id=u730fbe2c&originHeight=170&originWidth=891&originalType=url&ratio=1&rotation=0&showTitle=false&status=done&style=shadow&taskId=u5f5ea152-4702-4650-9ed1-23ea6179986&title=)
<a name="YBgfR"></a>
## 傅里叶变换
快速傅里叶变换(FFT)是一种计算序列离散傅里叶变换的算法。这种类型的图很有趣，因为它是处理时间序列时特征提取的主要方法之一。通常的做法不是用时间序列来训练模型，而是应用傅里叶变换来提取频率，然后训练模型。<br />为此，必须选择一个滑动窗口来计算FFT。滑动窗口越宽，频率数越高。缺点是您将得到更少的时间戳，从而丢失数据的时间分辨率。当减小窗口的大小时，得到了相反的结果：更少的频率但更高的时间分辨率。然后，窗口的大小应该取决于任务。<br />![](https://cdn.nlark.com/yuque/0/2022/png/396745/1663204385041-dfa4608b-03ff-406d-b981-a9fc63c59c74.png#clientId=u81d3986c-2799-4&from=paste&id=u8cb45c1c&originHeight=409&originWidth=1080&originalType=url&ratio=1&rotation=0&showTitle=false&status=done&style=shadow&taskId=u1cc8afda-a4da-4b35-ad93-13f21382ae6&title=)<br />FFT的滑动窗口 对于如下图所示，这里选择了一个包含64个数据的时间窗口。因此，频率从1 - 32hz。
```python
def fft(data, nwindow=64, freq = 32):
    ffts = []
    for i in range(0, len(data)-nwindow, nwindow//2):
        sliced = data[i:i+nwindow]
        fft = np.abs(np.fft.rfft(sliced*np.hamming(nwindow))[:freq])
        ffts.append(fft.tolist())
    ffts = np.array(ffts)
    return ffts

def data_plot(date_time, data, labels, ax):
    ax.plot(date_time, data)
    ax.set_xlim(date2num(np.min(date_time)), date2num(np.max(date_time)))
    axs_twinx = ax.twinx()
    axs_twinx.plot(date_time, labels, color='red')
    ax.set_ylabel('Label')

def fft_plot(ffts, ax):
    ax.imshow(np.flipud(np.rot90(ffts)), aspect='auto', cmap=matplotlib.cm.bwr, 
               norm=LogNorm(vmin=np.min(ffts), vmax=np.max(ffts)))
    ax.set_xlabel('Timestamp')
    ax.set_ylabel('Freq')
    
df_fourier = df_data.copy()
for name in df_boxplot.columns:
    if name not in ['datetime', 'date']:
        fig, axs = plt.subplots(2, 1, figsize=(15, 6))
        data = df_fourier[name].to_numpy()
        ffts = fft(data, nwindow=64, freq = 32)
        data_plot(df_fourier['datetime'], data, df_labels['machine_status'], axs[0])
        fft_plot(ffts, axs[1])
        axs[0].set_title(name)
        plt.show()
```
![](https://cdn.nlark.com/yuque/0/2022/png/396745/1663204385069-d12343d5-f81c-4f30-83fe-8da582706d16.png#clientId=u81d3986c-2799-4&from=paste&id=u7dd64c9d&originHeight=387&originWidth=917&originalType=url&ratio=1&rotation=0&showTitle=false&status=done&style=shadow&taskId=u9bda56ac-a877-42db-953e-6ec3c4650a2&title=)<br />![](https://cdn.nlark.com/yuque/0/2022/png/396745/1663204385108-dfe43524-e0c8-4aa9-9dbf-0550f9338f26.png#clientId=u81d3986c-2799-4&from=paste&id=u935db55f&originHeight=387&originWidth=914&originalType=url&ratio=1&rotation=0&showTitle=false&status=done&style=shadow&taskId=u809a9fc0-5f57-42cf-8f4a-9f3acc5ac1c&title=)![](https://cdn.nlark.com/yuque/0/2022/png/396745/1663204385342-e90aef5d-b829-4808-b189-de8aa3a5d108.png#clientId=u81d3986c-2799-4&from=paste&id=u5ebd75c8&originHeight=387&originWidth=920&originalType=url&ratio=1&rotation=0&showTitle=false&status=done&style=shadow&taskId=u3c5cd2f3-154c-4bf5-85dc-e674666d254&title=)![](https://cdn.nlark.com/yuque/0/2022/png/396745/1663204385425-01f23536-c081-4735-9fa1-337c06da045f.png#clientId=u81d3986c-2799-4&from=paste&id=uf7c433c2&originHeight=387&originWidth=920&originalType=url&ratio=1&rotation=0&showTitle=false&status=done&style=shadow&taskId=uefef09ce-815c-4e1a-9fd5-d408ee8aa19&title=)![](https://cdn.nlark.com/yuque/0/2022/png/396745/1663204385561-5213343b-325a-4598-a2cb-e1a96a8b1753.png#clientId=u81d3986c-2799-4&from=paste&id=u8c72e8e8&originHeight=387&originWidth=920&originalType=url&ratio=1&rotation=0&showTitle=false&status=done&style=shadow&taskId=ud903a48e-2340-4bd3-bd56-94f0dd61588&title=)![](https://cdn.nlark.com/yuque/0/2022/png/396745/1663204385694-1ed235b5-e7c0-4946-bfee-78eead57af3f.png#clientId=u81d3986c-2799-4&from=paste&id=u86177a8d&originHeight=387&originWidth=920&originalType=url&ratio=1&rotation=0&showTitle=false&status=done&style=shadow&taskId=u961b915e-c3a8-49e8-86c5-f7b67fab98c&title=)
<a name="ZpBa5"></a>
## 熵
可视化信息和熵是机器学习中的一个有用工具，因为它们是许多特征选择、构建决策树和拟合分类模型的基础。<br />熵的计算如下：<br />归一化频率分布<br />![](https://cdn.nlark.com/yuque/0/2022/png/396745/1663204385843-01c04825-30fa-42c4-848e-1b862d3511e3.png#clientId=u81d3986c-2799-4&from=paste&id=u107606fe&originHeight=125&originWidth=1080&originalType=url&ratio=1&rotation=0&showTitle=false&status=done&style=shadow&taskId=u4d380d88-97a6-436b-af43-04e78de74ed&title=)<br />计算熵<br />![](https://cdn.nlark.com/yuque/0/2022/png/396745/1663204386207-062950dc-b406-4dd4-ad29-9ab215fc14d9.png#clientId=u81d3986c-2799-4&from=paste&id=uf6386efe&originHeight=84&originWidth=1080&originalType=url&ratio=1&rotation=0&showTitle=false&status=done&style=shadow&taskId=u896aacce-f2b9-4db6-9e6e-4f51d483327&title=)<br />最低熵是针对某一随机变量计算的，该随机变量的单个事件的概率为1.0，即确定性。一个随机变量的最大熵是当所有事件都是等可能的。
```python
def entropy(data, nwindow=64, freq = 32):
    entropy_s = []
    for i in range(0, len(data)-nwindow, nwindow//2):
        sliced = data[i:i+nwindow]
        fft = np.abs(np.fft.rfft(sliced*np.hamming(nwindow))[:nwindow//2])
        p = fft / np.sum(fft)
        entropy = - np.sum(p * np.log(p))
        entropy_s.append(entropy)
    entropy_s = np.array(entropy_s)
    return entropy_s

def data_plot(date_time, data, labels, ax):
    ax.plot(date_time, data)
    axs_twinx = ax.twinx()
    axs_twinx.plot(date_time, labels, color='red')
    ax.set_xlabel('Value')
    ax.set_ylabel('Label')

def entropy_plot(data, ax):
    ax.plot(data, c='k')
    ax.set_xlabel('Timestamp')
    ax.set_ylabel('Entropy')

df_entropy = df_data.copy()
for name in df_boxplot.columns:
    if name not in ['datetime', 'date']:
        fig, axs = plt.subplots(2, 1, figsize=(15, 6))
        data = df_entropy[name].to_numpy()
        entropy_s = entropy(data, nwindow=64, freq = 32)
        data_plot(df_entropy['datetime'], data, df_labels['machine_status'], axs[0])
        entropy_plot(entropy_s, axs[1])
        axs[0].set_title(name)
        plt.show()
```
![](https://cdn.nlark.com/yuque/0/2022/png/396745/1663204386380-e0f9e95b-d5f4-4149-bc9a-90d05d1a1266.png#clientId=u81d3986c-2799-4&from=paste&id=ude4f7a01&originHeight=387&originWidth=911&originalType=url&ratio=1&rotation=0&showTitle=false&status=done&style=shadow&taskId=u280a96cd-6a91-44c3-8c7d-557a954f526&title=)<br />![](https://cdn.nlark.com/yuque/0/2022/png/396745/1663204386533-b4d44cc6-10ea-46a5-97a5-502f1274bae7.png#clientId=u81d3986c-2799-4&from=paste&id=u1f019e13&originHeight=387&originWidth=911&originalType=url&ratio=1&rotation=0&showTitle=false&status=done&style=shadow&taskId=ubcc3a599-98e1-44d6-b82d-948ff125054&title=)![](https://cdn.nlark.com/yuque/0/2022/png/396745/1663204386730-c9f9c567-420c-4e6b-a554-94473de850c0.png#clientId=u81d3986c-2799-4&from=paste&id=u49543c60&originHeight=387&originWidth=917&originalType=url&ratio=1&rotation=0&showTitle=false&status=done&style=shadow&taskId=udc9259ca-c1e7-4c4d-b644-268c4249fa9&title=)![](https://cdn.nlark.com/yuque/0/2022/png/396745/1663204386695-bd6caf1a-200c-4016-b852-40a3d29b91e6.png#clientId=u81d3986c-2799-4&from=paste&id=u6cfe29d3&originHeight=387&originWidth=917&originalType=url&ratio=1&rotation=0&showTitle=false&status=done&style=shadow&taskId=u3c6c9ea0-a7d6-40f1-8093-ee02c660c2d&title=)![](https://cdn.nlark.com/yuque/0/2022/png/396745/1663204386974-be1c7f78-4229-4982-8c66-844a23c27f8c.png#clientId=u81d3986c-2799-4&from=paste&id=ua63d2d34&originHeight=387&originWidth=914&originalType=url&ratio=1&rotation=0&showTitle=false&status=done&style=shadow&taskId=u5eb878d0-1be4-4d6f-a07f-dac976bff9a&title=)![](https://cdn.nlark.com/yuque/0/2022/png/396745/1663204386965-50555f97-627a-4972-982d-fbaf6ec1c6e6.png#clientId=u81d3986c-2799-4&from=paste&id=u884460f2&originHeight=387&originWidth=914&originalType=url&ratio=1&rotation=0&showTitle=false&status=done&style=shadow&taskId=u69b84d03-a5a8-4918-ae58-19f52ef4f54&title=)
<a name="pPumN"></a>
## 降维
当有多个传感器时，实现一种降维方法来获得包含大部分信息的1、2或3个主要组件总是很有趣的。<br />对于这个例子，实现了主成分分析(PCA)。这是计算主要组件并使用它们对数据进行基础更改的过程。<br />被解释方差比率是每一个被选择的组成部分的方差百分比。
```python
x = df_data[df_data.columns].drop(columns=['datetime'])
scaler = StandardScaler()
pca = PCA()
pipeline = make_pipeline(scaler, pca)
pipeline.fit(x)

features = range(pca.n_components_)
plt.figure(figsize=(22, 5))
plt.bar(features, pca.explained_variance_ratio_)
plt.xlabel('PCA feature')
plt.ylabel('Variance')
plt.xticks(features)
plt.title("Importance of the Principal Components based on inertia")
plt.show()
```
![](https://cdn.nlark.com/yuque/0/2022/png/396745/1663204387148-09f14034-21cc-492b-b934-580a2f5f7755.png#clientId=u81d3986c-2799-4&from=paste&id=u06992e6d&originHeight=280&originWidth=1080&originalType=url&ratio=1&rotation=0&showTitle=false&status=done&style=shadow&taskId=u986129db-306d-493e-9ff1-73e40c01cc4&title=)<br />对于第一个PCA组件，可以绘制数据，并直观地检查异常和时间序列之间是否存在关系。
```python
pca = PCA(n_components=2)
principalComponents = pca.fit_transform(x)
principalDf = pd.DataFrame(data = principalComponents, columns = ['pc1', 'pc2'])

df_pca = df_data.copy()
df_pca['pca1'] = pd.Series(principalDf['pc1'].values, index=df.index)
df_pca['pca2'] = pd.Series(principalDf['pc2'].values, index=df.index)
print(df_pca.shape)
print(df_pca.head())
```
![](https://cdn.nlark.com/yuque/0/2022/png/396745/1663204387482-00078cda-fb7b-448b-9a75-f47c8b3d2909.png#clientId=u81d3986c-2799-4&from=paste&id=u37d917bf&originHeight=392&originWidth=1080&originalType=url&ratio=1&rotation=0&showTitle=false&status=done&style=shadow&taskId=u99aa5c87-dc81-45f1-8a88-ca7f370a91f&title=)
```python
df_pca_hour = df_pca.groupby(pd.Grouper(key='datetime', axis=0, freq='H')).mean()
df_labels_hour = df_labels.groupby(pd.Grouper(key='datetime', axis=0, freq='H')).sum()
for name in df_pca.columns:
    if name in ['pca1', 'pca2']:
        fig, axs = plt.subplots(1, 1, figsize=(15, 2))
        axs.plot(df_pca_hour[name], color='blue')
        axs_twinx = axs.twinx()
        axs_twinx.plot(df_labels_hour['machine_status'], color='red')
        axs.set_title(name)
        plt.show()
```
![](https://cdn.nlark.com/yuque/0/2022/png/396745/1663204387361-1a30d54e-7386-4141-a6a0-3460b53f42a2.png#clientId=u81d3986c-2799-4&from=paste&id=u76b208ec&originHeight=156&originWidth=914&originalType=url&ratio=1&rotation=0&showTitle=false&status=done&style=shadow&taskId=u15f0fb92-58fb-4e4a-92fa-53fe323b8bf&title=)<br />![](https://cdn.nlark.com/yuque/0/2022/png/396745/1663204387551-afc3bc10-e348-49b7-84b7-01650a20eef2.png#clientId=u81d3986c-2799-4&from=paste&id=ub3217801&originHeight=156&originWidth=908&originalType=url&ratio=1&rotation=0&showTitle=false&status=done&style=shadow&taskId=u626b58b7-f0fb-4ff6-b24c-80182b4f12c&title=)
<a name="ioslt"></a>
## 自相关
最后，特别是对于预测任务，绘制数据的自相关性是很有趣的。这个表示给定的时间序列和它自己在连续时间间隔中的滞后版本之间的相似程度。
```python
pca1 = principalDf['pc1'].pct_change()
autocorrelation = pca1.dropna().autocorr()
print('Autocorrelation is: ', autocorrelation)
plot_acf(pca1.dropna(), lags=20, alpha=0.05)
plt.show()

# Autocorrelation is:  0.024363541344977133
```
![](https://cdn.nlark.com/yuque/0/2022/png/396745/1663204387564-33e485b8-15e0-4fcc-804d-7828af42073f.png#clientId=u81d3986c-2799-4&from=paste&id=ucfe528c2&originHeight=264&originWidth=372&originalType=url&ratio=1&rotation=0&showTitle=false&status=done&style=shadow&taskId=ue30a10f3-dea0-46b1-837a-784e02284b9&title=)<br />与自相关相关的是增强迪基-富勒统计检验，用于检验给定的时间序列是否平稳。
```python
for name in df_pca.columns:
    if name not in ['datetime', 'date']:
        print(f'{name}: ', adfuller(df_pca[name]))
```
```python
sensor_00:  (-4.288982026391745, 0.00046413778677505943, 13, 10067, 
{'1%': -3.4309997435394877, '5%': -2.861827148204747, 
'10%': -2.5669228438492597}, -31804.0714734676)
sensor_10:  (-2.221933316576291, 0.19838144551185893, 12, 10068, 
{'1%': -3.4309996789875097, '5%': -2.861827119679712, 
'10%': -2.5669228286653536}, 27766.511868448022)
sensor_20:  (-3.592564245109739, 0.0059047849892509, 37, 10043, 
{'1%': -3.4310012966444754, '5%': -2.8618278345100223, 
'10%': -2.566923209170298}, 57403.646550974)
sensor_30:  (-4.647724174292243, 0.00010538788279015939, 39, 10041, 
{'1%': -3.4310014264051087, '5%': -2.861827891850243, 
'10%': -2.566923239692562}, 86925.92362313878)
sensor_40:  (-10.29474568760144, 3.494931297495308e-18, 4, 10076, 
{'1%': -3.430999163033074, '5%': -2.8618268916832887, 
'10%': -2.566922707302626}, 40904.2633950389)
sensor_50:  (-3.896834428400123, 0.00205826787914369, 5, 10075, 
{'1%': -3.430999227482557, '5%': -2.8618269201630375, 
'10%': -2.566922722462425}, 38661.12678622437)
pca1:  (-3.729843319395405, 0.003712434586758561, 5, 10075, 
{'1%': -3.430999227482557, '5%': -2.8618269201630375, 
'10%': -2.566922722462425}, 44790.12927828672)
pca2:  (-10.096863463457167, 1.0850760399116373e-17, 12, 10068, 
{'1%': -3.4309996789875097, '5%': -2.861827119679712, 
'10%': -2.5669228286653536}, 68774.31281795294)
```
