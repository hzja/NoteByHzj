**机器学习按照模型类型分为监督学习模型、无监督学习模型两大类。**<br />![](https://cdn.nlark.com/yuque/0/2023/png/396745/1694958616102-964a246c-9a1a-410f-95d8-15f18455961e.png#averageHue=%23fdfcfc&clientId=u55baab5b-6ac7-4&from=paste&id=u8e630f2a&originHeight=810&originWidth=780&originalType=url&ratio=2.5&rotation=0&showTitle=false&status=done&style=none&taskId=ue0637a78-2b18-49d9-bc71-e45ee488691&title=)
<a name="YFlQc"></a>
## 1、有监督学习
有监督学习通常是利用带有_专家标注的标签的训练数据_，学习一个从输入变量X到输入变量Y的函数映射。Y = f (X)，训练数据通常是(n×x,y)的形式，其中n代表训练样本的大小，x和y分别是变量X和Y的样本值。<br />有监督学习可以被分为两类：

- 分类问题：预测某一样本所属的类别（离散的）。比如判断性别，是否健康等。
- 回归问题：预测某一样本的所对应的实数输出（连续的）。比如预测某一地区人的平均身高。

除此之外，集成学习也是一种有监督学习。它是将多个不同的相对较弱的机器学习模型的预测组合起来，用来预测新的样本。
<a name="bxjrU"></a>
### 1.1 单模型
<a name="FO4u7"></a>
#### 1.1.1 线性回归
![](https://cdn.nlark.com/yuque/0/2023/png/396745/1694958616085-f794b161-1797-4e69-abc3-c47c506f57c3.png#averageHue=%23fefefe&clientId=u55baab5b-6ac7-4&from=paste&id=ue011b830&originHeight=372&originWidth=524&originalType=url&ratio=2.5&rotation=0&showTitle=false&status=done&style=none&taskId=uf213b454-2c4b-444e-9e20-40a556b7523&title=)<br />线性回归是指完全由线性变量组成的回归模型。在线性回归分析中，只包括一个自变量和一个因变量，且二者的关系可用一条直线近似表示，这种回归分析称为一元线性回归分析。如果回归分析中包括两个或两个以上的自变量，且因变量和自变量之间是线性关系，则称为多元线性回归分析。
<a name="fhYld"></a>
#### 1.1.2 逻辑回归
![](https://cdn.nlark.com/yuque/0/2023/png/396745/1694958616096-8c1ff5b0-61ec-434c-83cd-2dbde6b16a7e.png#averageHue=%23f7fafe&clientId=u55baab5b-6ac7-4&from=paste&id=uab1153da&originHeight=354&originWidth=469&originalType=url&ratio=2.5&rotation=0&showTitle=false&status=done&style=none&taskId=ueacba051-333d-45f6-924b-38b79dcade5&title=)<br />用于研究**Y为定类数据**时X和Y之间的影响关系情况，如果Y为两类比如0和1（比如1为愿意和0为不愿意，1为购买和0为不购买），此时就叫二元逻辑回归；如果Y为三类以上,此时就称为多分类逻辑回归。<br />自变量并不一定非要定类变量，它们也可以是定量变量。如果X是定类数据，此时需要对X进行哑变量设置。
<a name="S2W8P"></a>
#### 1.1.3 Lasso
![](https://cdn.nlark.com/yuque/0/2023/png/396745/1694958616078-bbac9b2b-b765-4077-ae67-59f5bafe3684.png#averageHue=%23fefefd&clientId=u55baab5b-6ac7-4&from=paste&id=u3898674c&originHeight=375&originWidth=535&originalType=url&ratio=2.5&rotation=0&showTitle=false&status=done&style=none&taskId=ua2bf8ac6-cca1-47ae-8108-539a43dae5e&title=)<br />Lasso方法是一种替代最小二乘法的压缩估计方法。Lasso的基本思想是建立一个L1正则化模型，在模型建立过程中会压缩一些系数和设定一些系数为零，当模型训练完成后，这些权值等于0的参数就可以舍去，从而使模型更为简单，并且有效防止模型过拟合。被广泛用于存在多重共线性数据的拟合和变量选择。
<a name="BLyoi"></a>
#### 1.1.4 K近邻(KNN)
KNN做回归和分类的主要区别在于最后做预测时候的决策方式不同。KNN做分类预测时，一般是选择多数表决法，即训练集里和预测的样本特征最近的K个样本，预测为里面有最多类别数的类别。KNN做回归时，一般是选择平均法，即最近的K个样本的样本输出的平均值作为回归预测值。但它们的理论是一样的。
<a name="XkXw5"></a>
#### 1.1.5 决策树
![](https://cdn.nlark.com/yuque/0/2023/png/396745/1694958616093-1af2840e-a8f7-4347-9e11-55e2c2a36881.png#averageHue=%23f6f9fe&clientId=u55baab5b-6ac7-4&from=paste&id=u9f43d1ac&originHeight=392&originWidth=569&originalType=url&ratio=2.5&rotation=0&showTitle=false&status=done&style=none&taskId=u6837428c-d672-4414-8a3c-7e70fedcd1e&title=)<br />决策树中每个内部节点都是一个分裂问题：指定了对实例的某个属性的测试，它将到达该节点的样本按照某个特定的属性进行分割，并且该节点的每一个后继分支对应于该属性的一个可能值。**分类树**叶节点所含样本中，其输出变量的**众数**就是分类结果。**回归树**的叶节点所含样本中，其输出变量的**平均值**就是预测结果。
<a name="snbaK"></a>
#### 1.1.6 bp神经网络
![](https://cdn.nlark.com/yuque/0/2023/png/396745/1694958616428-2888c39f-21f8-4134-930f-f29d6e457a49.png#averageHue=%23fefefe&clientId=u55baab5b-6ac7-4&from=paste&id=u0f58b3b7&originHeight=379&originWidth=544&originalType=url&ratio=2.5&rotation=0&showTitle=false&status=done&style=none&taskId=u261d03a6-9b64-4bc4-8717-f72a4038f18&title=)<br />bp神经网络是一种按误差逆传播算法训练的多层前馈网络，是目前应用最广泛的神经网络模型之一。bp神经网络的学习规则是使用最速下降法，通过反向传播来不断调整网络的权值和阈值，使网络的分类错误率最小（误差平方和最小）。<br />BP 神经网络是一种多层的前馈神经网络，其主要的特点是：信号是前向传播的，而误差是反向传播的。具体来说，对于如下的只含一个隐层的神经网络模型：<br />BP 神经网络的过程主要分为两个阶段，第一阶段是信号的前向传播，从输入层经过隐含层，最后到达输出层；第二阶段是误差的反向传播，从输出层到隐含层，最后到输入层，依次调节隐含层到输出层的权重和偏置，输入层到隐含层的权重和偏置。
<a name="pvrxX"></a>
#### 1.1.7 支持向量机(SVM)
![](https://cdn.nlark.com/yuque/0/2023/png/396745/1694958616450-3282731e-e09e-4ee5-8564-6d3112b00144.png#averageHue=%23fefefe&clientId=u55baab5b-6ac7-4&from=paste&id=u35988b81&originHeight=378&originWidth=510&originalType=url&ratio=2.5&rotation=0&showTitle=false&status=done&style=none&taskId=uff9d36b5-2449-45af-9d93-0a8c5a5ae14&title=)<br />支持向量机回归（SVR）用非线性映射将数据映射到高维数据特征空间中，使得在高维数据特征空间中自变量与因变量具有很好的线性回归特征，在该特征空间进行拟合后再返回到原始空间。<br />支持向量机分类（SVM）是一类按监督学习方式对数据进行二元分类的广义线性分类器，其决策边界是对学习样本求解的最大边距超平面。
<a name="BWDyn"></a>
#### 1.1.8 朴素贝叶斯
![](https://cdn.nlark.com/yuque/0/2023/png/396745/1694958616438-c7eb185d-aa78-46ef-ab52-9cbaf5da6d87.png#averageHue=%23f3f7fe&clientId=u55baab5b-6ac7-4&from=paste&id=udaf71491&originHeight=392&originWidth=645&originalType=url&ratio=2.5&rotation=0&showTitle=false&status=done&style=none&taskId=u64501ab9-09ac-4cb9-8c2f-26c7b984678&title=)<br />在给定一个事件发生的前提下，计算另外一个事件发生的概率——会使用贝叶斯定理。假设先验知识为d，为了计算假设h为真的概率，要使用如下贝叶斯定理：<br />![](https://cdn.nlark.com/yuque/0/2023/png/396745/1694958616453-9b2a9aa0-3b6f-42f2-bc61-c6e735f65b53.png#averageHue=%23f6f5f4&clientId=u55baab5b-6ac7-4&from=paste&id=u5b0f30e5&originHeight=80&originWidth=237&originalType=url&ratio=2.5&rotation=0&showTitle=false&status=done&style=none&taskId=u5e5b8da7-44f2-4085-8bd0-d57e95ab5e5&title=)<br />该算法假定所有的变量都是相互独立的。
<a name="wCtVm"></a>
### 1.2 集成学习
集成学习是一种将不同学习模型（比如分类器）的结果组合起来，通过投票或平均来进一步提高准确率。一般，对于分类问题用投票；对于回归问题用平均。这样的做法源于“众人拾材火焰高”的想法。<br />集成算法主要有三类：Bagging，Boosting 和Stacking。本文将不谈及stacking。<br />![](https://cdn.nlark.com/yuque/0/2023/jpeg/396745/1694958616575-ba595306-c2ec-45c1-803a-7eae923a7cdf.jpeg#averageHue=%23f2f2f1&clientId=u55baab5b-6ac7-4&from=paste&id=u2857121a&originHeight=473&originWidth=1080&originalType=url&ratio=2.5&rotation=0&showTitle=false&status=done&style=none&taskId=u3f84b00f-dbaa-4aa8-8abc-e6f592cffec&title=)

- **Boosting**

![](https://cdn.nlark.com/yuque/0/2023/png/396745/1694958616817-7ee30299-7a90-40ca-bf9d-1cc335c0e8db.png#averageHue=%23f6f9fe&clientId=u55baab5b-6ac7-4&from=paste&id=u1556ab8a&originHeight=348&originWidth=1080&originalType=url&ratio=2.5&rotation=0&showTitle=false&status=done&style=none&taskId=u2be0b55e-c420-487a-951f-bb7220118a3&title=)
<a name="APmRx"></a>
#### 1.2.1 GBDT
GBDT 是以 CART 回归树为基学习器的 Boosting 算法，是一个加法模型，它串行地训练一组 CART 回归树，最终对所有回归树的预测结果加和，由此得到一个强学习器，每一颗新树都拟合当前损失函数的负梯度方向。最后输出这一组回归树的加和，直接得到回归结果或者套用 sigmod 或者 softmax 函数获得二分类或者多分类结果。
<a name="zyJQv"></a>
#### 1.2.2 adaboost
adaboost给予误差率低的学习器一个高的权重，给予误差率高的学习器一个低的权重，结合弱学习器和对应的权重，生成强学习器。回归问题与分类问题算法的不同点在于误差率计算的方式不同，分类问题一般都采用0/1损失函数，而回归问题一般都是平方损失函数或者是线性损失函数。
<a name="u3Mn3"></a>
#### 1.2.3 XGBoost
XGBoost 是"极端梯度上升"(Extreme Gradient Boosting)的简称，XGBoost 算法是一类由基函数与权重进行组合形成对数据拟合效果佳的合成算法。由于 XGBoost 模型具有较强的泛化能力、较高的拓展性、较快的运算速度等优势， 从2015年提出后便受到了统计学、数据挖掘、机器学习领域的欢迎。<br />xgboost是GBDT的一种高效实现，和GBDT不同，xgboost给损失函数增加了正则化项；且由于有些损失函数是难以计算导数的，xgboost使用损失函数的二阶泰勒展开作为损失函数的拟合。
<a name="CRP3j"></a>
#### 1.2.4 LightGBM
LightGBM 是 XGBoost 一种高效实现，其思想是将连续的浮点特征离散成 k 个离散值，并构造宽度为 k 的直方图。然后遍历训练数据，计算每个离散值在直方图中的累计统计量。在进行特征选择时，只需要根据直方图的离散值，遍历寻找最优的分割点；且使用带有深度限制的按叶子生长（leaf-wise）策略，节省了不少时间和空间上的开销。
<a name="kjXbu"></a>
#### 1.2.5 CatBoost
catboost 是一种基于对称决策树算法的 GBDT 框架，主要解决的痛点是高效合理地处理类别型特征和处理梯度偏差、预测偏移问题，提高算法的准确性和泛化能力。

- **Bagging**

![](https://cdn.nlark.com/yuque/0/2023/png/396745/1694958616837-de4b10ed-467d-43ef-b7aa-6797ce33b8d9.png#averageHue=%23f4f7fd&clientId=u55baab5b-6ac7-4&from=paste&id=uf89b7e4d&originHeight=411&originWidth=904&originalType=url&ratio=2.5&rotation=0&showTitle=false&status=done&style=none&taskId=uac0e93b9-442c-4dda-87cf-b00369c632b&title=)
<a name="VE7Qg"></a>
#### 1.2.6 随机森林
随机森林分类在生成众多决策树的过程中，是通过对建模数据集的样本观测和特征变量分别进行随机抽样，每次抽样结果均为一棵树，且每棵树都会生成符合自身属性的规则和分类结果(判断值)，而森林最终集成所有决策树的规则和分类结果(判断值)，实现随机森林算法的分类(回归)。
<a name="ekDS8"></a>
#### 1.2.7 Extra Trees
extra-trees (极其随机的森林)和随机森林非常类似，这里的“及其随机”表现在决策树的结点划分上，它干脆直接使用随机的特征和随机的阈值划分，这样每一棵决策树形状、差异就会更大、更随机。
<a name="thCXS"></a>
## 2、无监督学习
无监督学习问题处理的是，只有输入变量X没有相应输出变量的训练数据。它利用没有专家标注训练数据，对数据的结构建模。
<a name="QXcv6"></a>
### 2.1 聚类
将相似的样本划分为一个簇（cluster）。与分类问题不同，聚类问题预先并不知道类别，自然训练数据也没有类别的标签。
<a name="a3hdY"></a>
#### 2.1.1 K-means算法
![](https://cdn.nlark.com/yuque/0/2023/png/396745/1694958616835-b635d60e-e539-43cc-9556-d2990ce3a26c.png#averageHue=%23fefdf9&clientId=u55baab5b-6ac7-4&from=paste&id=u203f8fbe&originHeight=389&originWidth=555&originalType=url&ratio=2.5&rotation=0&showTitle=false&status=done&style=none&taskId=u332c7ae5-f203-4535-a477-560db9fc9ff&title=)<br />聚类分析是一种基于中心的聚类算法（K 均值聚类），通过迭代，将样本分到 K 个类中，使得每个样本与其所属类的中心或均值的距离之和最小。与分层聚类等按照字段进行聚类的算法不同的是，快速聚类分析是按照样本进行聚类。
<a name="ugCgr"></a>
#### 2.1.2 分层聚类
![](https://cdn.nlark.com/yuque/0/2023/png/396745/1694958616836-8c3ea1d1-0c04-4a24-be15-944c43eebe6e.png#averageHue=%23fcfcfc&clientId=u55baab5b-6ac7-4&from=paste&id=u821b7807&originHeight=449&originWidth=549&originalType=url&ratio=2.5&rotation=0&showTitle=false&status=done&style=none&taskId=u3863567f-bd55-4489-9680-5e1501488e2&title=)<br />分层聚类法作为聚类的一种，是对给定数据对象的集合进行层次分解，根据分层分解采用的分解策略。层次聚类算法按数据分层建立簇，形成一棵以簇为节点的树。如果按自底向上进行层次分解，则称为**凝聚的层次聚类**，比如 AGNES。而按自顶向下的进行层次分解，则称为**分裂法层次聚类**，比如 DIANA。一般用的比较多的是凝聚层次聚类。
<a name="omIsM"></a>
### 2.2 降维
降维指减少数据的维度同时保证不丢失有意义的信息。利用特征提取方法和特征选择方法，可以达到降维的效果。特征选择是指选择原始变量的子集。特征提取是将数据从高纬度转换到低纬度。广为熟知的主成分分析算法就是特征提取的方法。
<a name="MYCiG"></a>
#### 2.2.1 PCA主成分分析
![](https://cdn.nlark.com/yuque/0/2023/png/396745/1694958616947-ba1b4c54-016f-4fd1-9759-4b6c928e35c5.png#averageHue=%23fefefe&clientId=u55baab5b-6ac7-4&from=paste&id=ud6c30b79&originHeight=383&originWidth=544&originalType=url&ratio=2.5&rotation=0&showTitle=false&status=done&style=none&taskId=u8e3188bc-fc7f-4a50-ad8c-2c294240744&title=)<br />主成分分析将多个有一定相关性的指标进行线性组合，以最少的维度解释原数据中尽可能多的信息为目标进行降维，降维后的各变量间彼此线性无关，最终确定的新变量是原始变量的线性组合，且越往后主成分在方差中的比重也小，综合原信息的能力越弱。
<a name="oFAl8"></a>
#### 2.2.2 SVD奇异值分解
奇异值分解（SVD）是在机器学习领域广泛运用的算法，他不光可以用在降维算法中的特征值分解，还可以用于推荐系统，以及自然语言处理等领域，是很多算法的基石。
<a name="CmLIZ"></a>
#### 2.2.3 LDA线性判别
![](https://cdn.nlark.com/yuque/0/2023/png/396745/1694958617128-d87eb95e-4633-4c6c-8859-d2ec57d77cd5.png#averageHue=%23d9eee3&clientId=u55baab5b-6ac7-4&from=paste&id=u3e6d01a2&originHeight=325&originWidth=500&originalType=url&ratio=2.5&rotation=0&showTitle=false&status=done&style=none&taskId=u6e59bd89-f1a5-43ee-8306-23d4ee6293e&title=)<br />线性判别的原理是将样本投影到一条直线上，使得同类样本的投影点尽可能接近，不同样本的投影点尽可能远离；在对新样本进行分类时，将其投影到同样的直线上，再根据投影点的位置来确定新样本的类别。
