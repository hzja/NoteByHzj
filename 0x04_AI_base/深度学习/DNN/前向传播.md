假设我们选择的激活函数是![](./img/cb19dd57dd7eeeb1dc9b1d9beed54f88.svg)；隐藏层和输出层的输入值为![](./img/fbade9e36a3f36d3d676c1b808451dd7.svg)，上标表示第几层，下标表示本层单元的index；隐藏层和输出层的输出值为![](./img/0cc175b9c0f1b6a831c399e269772661.svg)，上标表示第几层，下标表示本层单元的index。则对于下图的三层DNN，利用和感知机一样的思路，我们可以利用上一层的输出计算下一层的输出，也就是所谓的DNN前向传播算法。建议边在[http://playground.tensorflow.org/](http://playground.tensorflow.org/)上操作，边看本文。

![前向传播1.png](./img/1593780859707-4bde80b1-ea6b-46f4-ad36-b1925178e39a.png)

对于第二层的输出![](./img/cb5f4785123da45c81dc34dc0fd503c3.svg)，![](./img/c7318cd18071c816de50cae9ea321526.svg)，![](./img/5efcba1df3346d3a9b230a9c3909c478.svg)。![](./img/f1290186a5d0b1ceab27f4e77c0c5d68.svg)的下标第一个数表示当前层的index，第二个数表示前一层的index，上标![](./img/e4b00b4a65a415cf9ebaa9f83719c071.svg)代表第一层到第二层，我们有

![](./img/4b5f0a319a8edd5f1eec20caedc41c94.svg)<br />![](./img/4a6c7692e649a883d62a8e50e981bfd1.svg)<br />![](./img/873b8f79b11316f34d54281d687b4998.svg)

对于第三层的输出![](./img/96dd28b17b676d75a65b0e55f4094b29.svg)，我们有

![](./img/c95e8af801bdf03d7c51eb2ee2b2d0f4.svg)

将上面的例子一般化，假设第![](./img/968dab8a0671a7033e8129b532ebc055.svg)层共有![](./img/6f8f57715090da2632453988d9a1501b.svg)个神经元，则对于第![](./img/2db95e8e1a9267b7a1188556b2013b33.svg)层的第![](./img/363b122c528f54df4a0446b6bab05515.svg)个神经元的输出![](./img/086394c2a26d141f2e5860cfdc69a288.svg)：

![](./img/cf59e2a8ac2566c624e6bb7f8d07cd20.svg)

可以看出，使用代数法一个个的表示输出比较复杂，而如果使用矩阵法则比较的简洁。假设第![](./img/968dab8a0671a7033e8129b532ebc055.svg)层共有![](./img/6f8f57715090da2632453988d9a1501b.svg)个神经元，而第![](./img/2db95e8e1a9267b7a1188556b2013b33.svg)层共有![](./img/7b8b965ad4bca0e41ab51de7b31363a1.svg)个神经元，则第![](./img/2db95e8e1a9267b7a1188556b2013b33.svg)层的线性系数![](./img/f1290186a5d0b1ceab27f4e77c0c5d68.svg)组成了一个![](./img/252d3754c0db62a55b9e25c870a524a5.svg)的矩阵![](./img/5e95de488fc446fbe29319c68dfb26aa.svg)，第![](./img/2db95e8e1a9267b7a1188556b2013b33.svg)层的偏倚![](./img/92eb5ffee6ae2fec3ad71c777531578f.svg)组成了一个![](./img/fb06b64b7286f2e4d7a804779873bf76.svg)的向量![](./img/716cb64a436cf1ec31b663f00f2e9d59.svg)，第![](./img/968dab8a0671a7033e8129b532ebc055.svg)层的输出![](./img/0cc175b9c0f1b6a831c399e269772661.svg)组成了一个![](./img/cc2faedbdd323a45c0ab50147de20467.svg)的向量![](./img/393aa089d709e5875849521faed655e7.svg)，第![](./img/2db95e8e1a9267b7a1188556b2013b33.svg)层的未激活前线性输出![](./img/fbade9e36a3f36d3d676c1b808451dd7.svg)组成了一个![](./img/fb06b64b7286f2e4d7a804779873bf76.svg)的向量![](./img/d06931cdb33dee904eecb92f9c13a619.svg)，第![](./img/2db95e8e1a9267b7a1188556b2013b33.svg)层的输出![](./img/0cc175b9c0f1b6a831c399e269772661.svg)组成了一个![](./img/fb06b64b7286f2e4d7a804779873bf76.svg)的向量![](./img/bdbecbf153380720bec0551b78be4567.svg)。则用矩阵法表示，第![](./img/2db95e8e1a9267b7a1188556b2013b33.svg)层的输出为：

![](./img/638be1d981f2380fcb4728d5ea2f6d19.svg)

所谓的DNN的前向传播算法也就是利用我们的若干个权重系数矩阵![](./img/61e9c06ea9a85a5088a499df6458d276.svg)和偏倚向量![](./img/92eb5ffee6ae2fec3ad71c777531578f.svg)来和输入值向量![](./img/9dd4e461268c8034f5c8564e155c67a6.svg)进行一系列线性运算和激活运算，从输入层开始，一层层的向后计算，一直到输出层，得到输出结果为止。

输入：总层数![](./img/d20caec3b48a1eef164cb4ca81ba2587.svg)，所有隐藏层和输出层对应的矩阵![](./img/61e9c06ea9a85a5088a499df6458d276.svg)，偏倚向量![](./img/92eb5ffee6ae2fec3ad71c777531578f.svg)，输入值向量![](./img/9dd4e461268c8034f5c8564e155c67a6.svg)<br />输出：输出层的输出![](./img/af7bf7c36b160d3fbecdb642201fd9d0.svg)

1. 初始化![](./img/bb9e34b96e0e1da2e032bdcadd35e590.svg)
2. ![](./img/578ad72e7f49713387bd8cdd373efb0c.svg)：
   1. ![](./img/638be1d981f2380fcb4728d5ea2f6d19.svg)

最后结果即输出![](./img/af7bf7c36b160d3fbecdb642201fd9d0.svg)

<a name="Source"></a>
## Source

[https://blog.csdn.net/anshuai_aw1/article/details/84615935](https://blog.csdn.net/anshuai_aw1/article/details/84615935)
