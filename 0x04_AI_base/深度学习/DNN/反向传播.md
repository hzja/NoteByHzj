当我们使用前馈神经网络接收输入![](./img/9dd4e461268c8034f5c8564e155c67a6.svg)并产生输出![](./img/5d28a7ba1a44a73b8c2ed21321697c59.svg)时，信息通过网络向前流动。输入![](./img/9dd4e461268c8034f5c8564e155c67a6.svg)提供初始信息，然后传播到每一层的隐藏单元，最终产生输出![](./img/5d28a7ba1a44a73b8c2ed21321697c59.svg)，这称之为前向传播。在训练过程中，前向传播可以持续向前直到它产生一个标量代价函数![](./img/698e62e708a07405c6154a447463ef91.svg)。反向传播算法(Back Propagation)，允许来自代价函数的信息通过网络向后流动，以便计算梯度。建议边在[http://playground.tensorflow.org/](http://playground.tensorflow.org/)上操作，边看本文。

<a name="9a2ee50d"></a>
## 链式法则

微积分中的链式法则（为了不与概率中的链式法则相混淆）用于计算复合函数的导数。反向传播是一种计算链式法则的算法，使用高效的特定运算顺序。

设![](./img/9dd4e461268c8034f5c8564e155c67a6.svg)是实数，![](./img/8fa14cdd754f91cc6554c9e71929cce7.svg)和![](./img/b2f5ff47436671b6e533d8dc3614845d.svg)是从实数映射到实数的函数。![](./img/d13f453d5a25ba83fd2f3ecc74880538.svg)并且![](./img/961f0f38250317a33299383ef38b277a.svg)。那么链式法则是说：

![](./img/6b5893859a30ecdf5e5be20af79b276a.svg)

我们可以将这种标量情况进行拓展。假设假设![](./img/2c4c8a670f38447be2fd3eef8eda1078.svg)，![](./img/a9ed29d4c71e8a3fa9793a2e7396a06b.svg)，![](./img/b2f5ff47436671b6e533d8dc3614845d.svg)是从![](./img/cf048f74f71721abd7b8df49453d1310.svg)到![](./img/cf048f74f71721abd7b8df49453d1310.svg)的映射，![](./img/8fa14cdd754f91cc6554c9e71929cce7.svg)是 ![](./img/cf048f74f71721abd7b8df49453d1310.svg)到 ![](./img/cf048f74f71721abd7b8df49453d1310.svg)的映射。如果![](./img/d13f453d5a25ba83fd2f3ecc74880538.svg)并且![](./img/3ef3cadf29d48026f675ae9b1c31cc7b.svg)，那么

![](./img/da48ad36e59290f36118c178dcb7c044.svg)

使用向量记法，可以等价地写成<br />![](./img/9f5acad613c180137c726de97c584334.svg)

这里![](./img/92f064b136ab3fd020347c5f60dffdf2.svg)是![](./img/b2f5ff47436671b6e533d8dc3614845d.svg)的![](./img/252d3754c0db62a55b9e25c870a524a5.svg)的Jacobian矩阵。

通常我们将反向传播算法应用于任意维度的张量，而不仅仅用于向量。从概念上讲，这与使用向量的方向传播完全相同。唯一的区别是如何将数字排列称网格以形成张量。我们可以想象，在运行反向传播之前，将每个张量扁平为一个向量，计算一个向量值梯度，然后将该梯度重新构造成一个张量。从这种重新排列的观点上看，反向传播仍然只是将Jacobian乘以梯度。

<a name="4ae61d8a"></a>
## 反向传播

在进行DNN反向传播算法前，我们需要选择一个损失函数，来度量训练样本计算出的输出和真实的训练样本输出之间的损失。DNN可选择的损失函数有不少，为了专注算法，这里我们使用最常见的均方差来度量损失。当然，针对不同的任务，可以选择不同的损失函数。即对于每个样本，我们期望最小化下式：

![](./img/c310e98e76186eb917f9f47e0f265241.svg)

其中，![](./img/af7bf7c36b160d3fbecdb642201fd9d0.svg)和![](./img/415290769594460e2e485922904f345d.svg)为![](./img/55c920206bf7571ea944b948d12625c7.svg)_维度的向量，而_![](./img/028bef5798958895e7bb080d6a0d62dd.svg)为![](./img/5dbc98dcc983a70728bd082d1a47546e.svg)的![](./img/07cbd6c155424e110559a84df364be5a.svg)范数。损失函数有了，现在我们开始用梯度下降法迭代求解每一层的![](./img/61e9c06ea9a85a5088a499df6458d276.svg)和![](./img/92eb5ffee6ae2fec3ad71c777531578f.svg)。

<a name="863a8583"></a>
#### 第一步
首先是输出层（第![](./img/d20caec3b48a1eef164cb4ca81ba2587.svg)层）。输出层的![](./img/61e9c06ea9a85a5088a499df6458d276.svg)和![](./img/92eb5ffee6ae2fec3ad71c777531578f.svg)满足下式：

![](./img/ace8cc6924412b1026445a095795caba.svg)

这样对于输出层的参数，我们的损失函数变为：

![](./img/2b3049ea67bec30f390cfa94f0a3849f.svg)

这样求解![](./img/61e9c06ea9a85a5088a499df6458d276.svg)和![](./img/92eb5ffee6ae2fec3ad71c777531578f.svg)的梯度就简单了：

![](./img/94cc7651a30ce35eeceaf120857465b2.svg)

![](./img/56f7226e752599099c3cd3f4be261c7e.svg)

上面式子前两项之所以是Hadamard积![](./img/319d584a4a5166ee6c51f4b8348856ea.svg)形式，是因为![](./img/28866e9af11431696e399ed44d588e09.svg)都是针对同一层的神经元。如果我们考虑对于![](./img/d20caec3b48a1eef164cb4ca81ba2587.svg)层的第![](./img/363b122c528f54df4a0446b6bab05515.svg)个神经元，即![](./img/21748687e94e73eb29058f1e73f2c72b.svg)，那么整合这一层的神经元，自然是![](./img/f8526ea07d061129cd01e13d97dd2468.svg)这样Hadamard积的形式。

![](./img/446bef0fad2802c311c230173d20b74a.svg)在第一个式子的最后是因为若![](./img/c5f2a02aebce92b6f1dab0d4b736b463.svg)，那么![](./img/173e2a24998cad6682b0dd2f0bc42f61.svg)。

<a name="9757f2c5"></a>
#### 第二步
我们注意到在求解输出层的![](./img/61e9c06ea9a85a5088a499df6458d276.svg)和![](./img/92eb5ffee6ae2fec3ad71c777531578f.svg)时，有公共的部分![](./img/46ac5a6937cefe1c3e045e4b6040d230.svg)，因此我们可以把公共的部分即对![](./img/1a22c6b428fe0c6c9f7d44bfbd06b0a7.svg)先算出来，记为

![](./img/d869ad7cf234267d99dc74d5fe7d1d91.svg)

根据第一步的公式我们可以把输出层的梯度计算出来，计算上一层![](./img/9ec3e65a53670d1e23fae033c83803bb.svg) ，上上层![](./img/97cd627b62808fc2e31a701418417b71.svg)...的梯度就需要步步递推了：对于第![](./img/2db95e8e1a9267b7a1188556b2013b33.svg)层的未激活输出![](./img/d06931cdb33dee904eecb92f9c13a619.svg)，它的梯度可以表示为

![](./img/eedb3618573045962c19f5d0765d57b1.svg)

如果我们可以依次计算出第![](./img/2db95e8e1a9267b7a1188556b2013b33.svg)层的![](./img/8d940e91cf682d59984cb74470ceff12.svg)，则该层的![](./img/734c86fd8d7bdc30951f9ecced92c10c.svg)和![](./img/df4261db186a72590b2839a7e8d951cc.svg)就很好计算了，因为根据前向传播：

![](./img/3b88164f06ad85a874b2f633d1e53f1a.svg)

所以我们可以很方便的计算出第![](./img/2db95e8e1a9267b7a1188556b2013b33.svg)层的![](./img/734c86fd8d7bdc30951f9ecced92c10c.svg)和![](./img/df4261db186a72590b2839a7e8d951cc.svg)的梯度如下

![](./img/05805c9ce7658582be416bd19a48e4e2.svg)

![](./img/bd0a8d2006ee1cc4085f2b206f27b963.svg)

<a name="207e30c0"></a>
#### 第三步
现在问题的关键就是求![](./img/8d940e91cf682d59984cb74470ceff12.svg)了。这里我们使用数学归纳法，假设第![](./img/c4ee0bf44a925ee2ad36dd49936b6f54.svg)层的![](./img/8795fd4b6824437ce50d4d5e5afb7fea.svg)已经求出，那我们如何求第 ![](./img/2db95e8e1a9267b7a1188556b2013b33.svg)层的![](./img/8d940e91cf682d59984cb74470ceff12.svg)呢：<br />![](./img/34f4c9d7b1404189ebcf2fa1db54b582.svg)

可见，关键在于求解![](./img/74ec62336bf6a1a8313d37d50cb79605.svg)，而![](./img/512a108801d4316b9ea35b1fcaec915b.svg)和![](./img/d06931cdb33dee904eecb92f9c13a619.svg)的关系很容易求出：

![](./img/ba489d999a7b9f4c454aff24e1e0d7ed.svg)

这样可得

![](./img/b463efc8d774d51bfac6e9c16a961e3e.svg)

上式的意思是![](./img/a0a28aae2a8ddaa3e71644a806211878.svg)的每一列都是Hadamard积![](./img/499d93efcf7967e071b63777a2b9e8f9.svg) ，将上式代入 ![](./img/bc0b5b7dafc169b87d88cc07cb2f4041.svg)，得<br />![](./img/b8f3314d59d312867c78f0ae3aa16e01.svg)<br />![](./img/7e54401f110c8bfc48df30c97f1f6d41.svg)

<a name="25f9c7fa"></a>
#### 总结

其实，对于更新每一层的![](./img/f50bc11c00442759cfcead696d3c95a7.svg)的对应梯度，我们仔细观察整个过程，发现只需要四个公式就可以完整地进行更新。这就是著名的反向传播的四个公式。我们稍加改动，使其可以适用于多种损失函数，即：

![](./img/b9128b96497fcc87692fd9499ab8e429.svg)<br />![](./img/c529ddf924b60531eda5cfb10e83510c.svg)<br />![](./img/8a9d940b7f9d55c332e1b2dba6bd0699.svg)<br />![](./img/f80f528ee75dca7156047bfd96b1e6f6.svg)

<a name="Smosz"></a>
### Source
[https://blog.csdn.net/anshuai_aw1/article/details/84666595](https://blog.csdn.net/anshuai_aw1/article/details/84666595)<br />[https://arxiv.org/pdf/1411.2738.pdf](https://arxiv.org/pdf/1411.2738.pdf)

