<a name="fZmPk"></a>
# 数据类型
```python
from pyspark.sql import SparkSession

spark = SparkSession \
    .builder \
    .appName("test") \
    .config("spark.some.config.option", "setting") \
    .getOrCreate()
    
train = spark.read.csv('./BlackFriday/train.csv', header=True, inferSchema=True)
test = spark.read.csv('./BlackFriday/test.csv', header=True,  inferSchema=True)

# 要查看Dataframe中列的类型，可以使用printSchema()方法。让我们在train上应用printSchema()，
# 它将以树格式打印模式。 
train.printSchema()
"""
root
 |-- User_ID: integer (nullable = true)
 |-- Product_ID: string (nullable = true)
 |-- Gender: string (nullable = true)
 |-- Age: string (nullable = true)
 |-- Occupation: integer (nullable = true)
 |-- City_Category: string (nullable = true)
 |-- Stay_In_Current_City_Years: string (nullable = true)
 |-- Marital_Status: integer (nullable = true)
 |-- Product_Category_1: integer (nullable = true)
 |-- Product_Category_2: integer (nullable = true)
 |-- Product_Category_3: integer (nullable = true)
 |-- Purchase: integer (nullable = true)
"""
```
<a name="FEKSn"></a>
# 缺失值插补
```python
# 通过调用drop()方法，可以检查train上非空数值的个数，并进行测试。
# 默认情况下，drop()方法将删除包含任何空值的行。
# 我们还可以通过设置参数“all”,当且仅当该行所有参数都为null时以删除该行。
# 这与pandas上的drop方法类似。
train.na.drop('any').count(),test.na.drop('any').count()

# 在这里，为了填充简单，我使用-1来填充train和test的null值。
# 虽然这不是一个很好的填充方法，你可以选择其他的填充方式。
train = train.fillna(-1)
test = test.fillna(-1)
```
<a name="hrEMT"></a>
# 数值特征
```python
# 我们还可以使用describe()方法查看Dataframe列的各种汇总统计信息，它显示了数字变量的统计信息。
# 要显示结果，我们需要调用show()方法。
train.describe().show()
"""
+-------+------------------+----------+------+------+------------------+-------------+--------------------------+-------------------+------------------+------------------+------------------+-----------------+
|summary|           User_ID|Product_ID|Gender|   Age|        Occupation|City_Category|Stay_In_Current_City_Years|     Marital_Status|Product_Category_1|Product_Category_2|Product_Category_3|         Purchase|
+-------+------------------+----------+------+------+------------------+-------------+--------------------------+-------------------+------------------+------------------+------------------+-----------------+
|  count|            550068|    550068|550068|550068|            550068|       550068|                    550068|             550068|            550068|            550068|            550068|           550068|
|   mean|1003028.8424013031|      null|  null|  null| 8.076706879876669|         null|         1.468494139793958|0.40965298835780306| 5.404270017525106| 6.419769919355425| 3.145214773446192|9263.968712959126|
| stddev| 1727.591585530871|      null|  null|  null|6.5226604873418115|         null|         0.989086680757309| 0.4917701263173259| 3.936211369201324| 6.565109781181374| 6.681038828257864|5023.065393820593|
|    min|           1000001| P00000142|     F|  0-17|                 0|            A|                         0|                  0|                 1|                -1|                -1|               12|
|    max|           1006040|  P0099942|     M|   55+|                20|            C|                        4+|                  1|                20|                18|                18|            23961|
+-------+------------------+----------+------+------+------------------+-------------+--------------------------+-------------------+------------------+------------------+------------------+-----------------+
"""
# 上面看起来好像比较乱，这里我们选择某一列来看看让我们从一个列中选择一个名为“User_ID”的列，
# 我们需要调用一个方法select并传递我们想要选择的列名。
# select方法将显示所选列的结果。我们还可以通过提供用逗号分隔的列名，从数据框架中选择多个列。
train.select('User_ID','Age').show(5)
"""
+-------+----+
|User_ID| Age|
+-------+----+
|1000001|0-17|
|1000001|0-17|
|1000001|0-17|
|1000001|0-17|
|1000002| 55+|
+-------+----+
only showing top 5 rows
"""
```
<a name="i681k"></a>
# 类别特征
```python
# 为了建立一个模型，我们需要在“train”和“test”中看到分类特征的分布。
# 这里我只对Product_ID显示这个，但是我们也可以对任何分类特性执行相同的操作。
# 让我们看看在“train”和“test”中Product_ID的不同类别的数量。
# 这可以通过应用distinct()和count()方法来实现。
train.select('Product_ID').distinct().count(), test.select('Product_ID').distinct().count()
"""
(3631, 3491)
"""

# 在计算“train”和“test”的不同值的数量后，我们可以看到“train”和“test”有更多的类别。
# 让我们使用相减方法检查Product_ID的类别，这些类别正在"test"中，但不在“train”中。
# 我们也可以对所有的分类特征做同样的处理。
diff_cat_in_train_test=test.select('Product_ID').subtract(train.select('Product_ID'))

diff_cat_in_train_test.distinct().count()
"""
(46, None)
"""

diff_cat_in_train_test.distinct().show(5)
"""
+----------+
|Product_ID|
+----------+
| P00322642|
| P00300142|
| P00077642|
| P00249942|
| P00294942|
+----------+
only showing top 5 rows
"""
# 以上你可以看到46个不同的类别是在"test"中，而不在"train"中。
# 在这种情况下，我们要么收集更多关于它们的数据，要么跳过那些类别(无效类别)的“test”。
```
<a name="C71KC"></a>
# 类别转标签
```python
# 我们还需要通过在Product_ID上应用StringIndexer转换将分类列转换为标签，
# 该转换将标签的Product_ID列编码为标签索引的列。
from pyspark.ml.feature import StringIndexer
plan_indexer = StringIndexer(inputCol = 'Product_ID', outputCol = 'product_id_trans')
labeller = plan_indexer.fit(train)

# 在上面，我们将fit()方法应用于“train”数据框架上，构建了一个标签。
# 稍后我们将使用这个标签来转换我们的"train"和“test”。
# 让我们在labeller的帮助下转换我们的train和test的Dataframe。我们需要调用transform方法。
# 我们将把转换结果存储在Train1和Test1中。
Train1 = labeller.transform(train)
Test1 = labeller.transform(test)
Train1.show(2)
+-------+----------+------+----+----------+-------------+--------------------------+--------------+------------------+------------------+------------------+--------+----------------+
|User_ID|Product_ID|Gender| Age|Occupation|City_Category|Stay_In_Current_City_Years|Marital_Status|Product_Category_1|Product_Category_2|Product_Category_3|Purchase|product_id_trans|
+-------+----------+------+----+----------+-------------+--------------------------+--------------+------------------+------------------+------------------+--------+----------------+
|1000001| P00069042|     F|0-17|        10|            A|                         2|             0|                 3|                -1|                -1|    8370|           766.0|
|1000001| P00248942|     F|0-17|        10|            A|                         2|             0|                 1|                 6|                14|   15200|           183.0|
+-------+----------+------+----+----------+-------------+--------------------------+--------------+------------------+------------------+------------------+--------+----------------+
Train1.select('product_id_trans').show(2)
+----------------+
|product_id_trans|
+----------------+
|           766.0|
|           183.0|
+----------------+
# 上面已经显示了我们在以前的"train" Dataframe中成功的添加了
# 一个转化后的列“product_id_trans”，("Train1" Dataframe)。
```
<a name="ZbAL6"></a>
# 构造样本
```python
# 首先，我们需要从pyspark.ml.feature导入RFormula；然后，我们需要在这个公式中指定依赖和独立的列；
# 我们还必须为为features列和label列指定名称。
from pyspark.ml.feature import RFormula
formula = RFormula(formula="Purchase ~ Age+ Occupation +City_Category+Stay_In_Current_City_Years+Product_Category_1+Product_Category_2+ Gender",
                   featuresCol="features",labelCol="label")

# 在创建了这个公式之后，我们需要将这个公式应用到我们的Train1上，并通过这个公式转换Train1,Test1。
让我们看看如何做到这一点，在拟合变换train1之后
t1 = formula.fit(Train1)
train1 = t1.transform(Train1)
test1 = t1.transform(Test1)
train1.show(2)
+-------+----------+------+----+----------+-------------+--------------------------+--------------+------------------+------------------+------------------+--------+----------------+--------------------+-------+
|User_ID|Product_ID|Gender| Age|Occupation|City_Category|Stay_In_Current_City_Years|Marital_Status|Product_Category_1|Product_Category_2|Product_Category_3|Purchase|product_id_trans|            features|  label|
+-------+----------+------+----+----------+-------------+--------------------------+--------------+------------------+------------------+------------------+--------+----------------+--------------------+-------+
|1000001| P00069042|     F|0-17|        10|            A|                         2|             0|                 3|                -1|                -1|    8370|           766.0|(16,[6,10,13,14],...| 8370.0|
|1000001| P00248942|     F|0-17|        10|            A|                         2|             0|                 1|                 6|                14|   15200|           183.0|(16,[6,10,13,14],...|15200.0|
+-------+----------+------+----+----------+-------------+--------------------------+--------------+------------------+------------------+------------------+--------+----------------+--------------------+-------+

# 在应用了这个公式之后，我们可以看到train1和test1有两个额外的列，称为features和label，
# 并对我们在公式中指定的列进行标记(featuresCol= features和labelCol= label)。
# 直观上，train1和test1中的features列中的所有分类变量都被转换为数值，数值变量与之前应用ML时相同。
我们还可以查看train1和test1中的列特性和标签。
train1.select('features').show(2)
+--------------------+
|            features|
+--------------------+
|(16,[6,10,13,14],...|
|(16,[6,10,13,14],...|
+--------------------+

train1.select('label').show(2)
+-------+
|  label|
+-------+
| 8370.0|
|15200.0|
+-------+
```
<a name="jaMzj"></a>
# 数据透视
```python
rdd = sc.parallelize(
    [
        (0, "A", 223,"201603", "PORT"), 
        (0, "A", 22,"201602", "PORT"), 
        (0, "A", 422,"201601", "DOCK"), 
        (1,"B", 3213,"201602", "DOCK"), 
        (1,"B", 3213,"201601", "PORT"), 
        (2,"C", 2321,"201601", "DOCK")
    ]
)
df_data = sqlContext.createDataFrame(rdd, ["id","type", "cost", "date", "ship"])
df_data.show()
 +---+----+----+------+----+
| id|type|cost|  date|ship|
+---+----+----+------+----+
|  0|   A| 223|201603|PORT|
|  0|   A|  22|201602|PORT|
|  0|   A| 422|201601|DOCK|
|  1|   B|3213|201602|DOCK|
|  1|   B|3213|201601|PORT|
|  2|   C|2321|201601|DOCK|
+---+----+----+------+----+

df_data.groupby(df_data.id, df_data.type).pivot("date").avg("cost").show()
+---+----+------+------+------+
| id|type|201601|201602|201603|
+---+----+------+------+------+
|  2|   C|2321.0|  null|  null|
|  0|   A| 422.0|  22.0| 223.0|
|  1|   B|3213.0|3213.0|  null|
+---+----+------+------+------+

from pyspark.sql.functions import first
(df_data
    .groupby(df_data.id, df_data.type)
    .pivot("date")
    .agg(first("ship"))
    .show())
+---+----+------+------+------+
| id|type|201601|201602|201603|
+---+----+------+------+------+
|  2|   C|  DOCK|  null|  null|
|  0|   A|  DOCK|  PORT|  PORT|
|  1|   B|  PORT|  DOCK|  null|
+---+----+------+------+------+

from pyspark.sql.functions import max, struct
(df_data
    .groupby("id", "type", "date", "ship")
    .count()
    .groupby("id", "type")
    .pivot("date")
    .agg(max(struct("count", "ship")))
    .show())
+---+----+--------+--------+--------+
| id|type|  201601|  201602|  201603|
+---+----+--------+--------+--------+
|  2|   C|[1,DOCK]|    null|    null|
|  0|   A|[1,DOCK]|[1,PORT]|[1,PORT]|
|  1|   B|[1,PORT]|[1,DOCK]|    null|
+---+----+--------+--------+--------+
```



<a name="GDb0c"></a>
# Source
[https://stackoverflow.com/questions/37486910/pivot-string-column-on-pyspark-dataframe](https://stackoverflow.com/questions/37486910/pivot-string-column-on-pyspark-dataframe)<br />[https://zhuanlan.zhihu.com/p/52753778](https://zhuanlan.zhihu.com/p/52753778)
