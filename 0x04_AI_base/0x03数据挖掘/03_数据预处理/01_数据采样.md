## 蓄水池采样法(Reservoir Sampling)

蓄水池算法适用于对一个不清楚规模的数据集进行采样。尤其适合针对数据流进行采样，并不知道这个流什么时候结束，且须保证每单位数据被采样到的几率相同。

假设数据序列规模为![](./img/7b8b965ad4bca0e41ab51de7b31363a1.svg)，需要采样的数量为![](./img/8ce4b16b22b58894aa86c421e8759df3.svg)。内存中开辟可容纳![](./img/8ce4b16b22b58894aa86c421e8759df3.svg)个元素的空间，并将序列前![](./img/8ce4b16b22b58894aa86c421e8759df3.svg)个元素放入空间中。然后从第![](./img/a31a860e7a59c7616c1515ec3ae652a6.svg)个元素开始，以![](./img/fda8af4743433ddceb8ff7765b97fd18.svg)的概率来决定该元素是否被替换到数组中（数组中的元素被替换的概率是相同的）。当遍历完所有元素后，内存中所存储的即为采样的样本。

证明：

对于第![](./img/865c0c0b4ab0e063e5caa3387c1a8741.svg)个数(![](./img/57bac0ff89002fde847fb1d62dd2eb41.svg))，被选中的概率为![](./img/c4ca4238a0b923820dcc509a6f75849b.svg)。当走到第![](./img/a31a860e7a59c7616c1515ec3ae652a6.svg)步时，被第![](./img/a31a860e7a59c7616c1515ec3ae652a6.svg)个元素替换的概率为![](./img/a31a860e7a59c7616c1515ec3ae652a6.svg)个元素被选中的概率乘以![](./img/865c0c0b4ab0e063e5caa3387c1a8741.svg)被选择替换的概率，即![](./img/893a27057712e650aad0305e7bbff73c.svg)。则被保留的概率为![](./img/0973757bf907d85d130f5ccacd4ffb0c.svg)。依次类推，不被![](./img/9276045de13b49323df13884fbab39e2.svg)个元素替换的概率为![](./img/db277f6d0a290a3816c430385a114cc5.svg)。则运行到第![](./img/7b8b965ad4bca0e41ab51de7b31363a1.svg)步时，被保留的概率为被选中的概率乘以不被替换的概率，即![](./img/e39551e3034bc067daa4752e2e1603c6.svg)。

对于第![](./img/363b122c528f54df4a0446b6bab05515.svg)个数(![](./img/a5d67f19fb950a5567b34722844f4d42.svg))。在第![](./img/363b122c528f54df4a0446b6bab05515.svg)步被选中的概率为![](./img/3f742c7cfbefb4135ef0439de11cc7ce.svg)。不被第![](./img/d0316e6933541181a928466f31d962cd.svg)个元素替换的概率为![](./img/870316ef1b7a565244200f396e9e0065.svg)。则运行到第![](./img/7b8b965ad4bca0e41ab51de7b31363a1.svg)步时，被保留的概率为被选中概率乘以不被替换概率，即![](./img/2ebd79dff826011bcdde43036d6dd671.svg)

所以，对于每一个元素，被保留的概率都为![](./img/fda8af4743433ddceb8ff7765b97fd18.svg)。

<a name="d2663090"></a>
## 逆采样(Inverse Sampling)

在蒙特卡罗方法中，有一个关键的问题需要解决，即如何基于概率密度函数去采得![](./img/7b8b965ad4bca0e41ab51de7b31363a1.svg)个![](./img/9dd4e461268c8034f5c8564e155c67a6.svg)的样本。

逆采样(Inverse Sampling)和拒绝采样(Reject Sampling)就是用于解决这个问题的。

我们知道，对于常见的均匀分布![](./img/28398c8992cbaccdefad91eb144f6768.svg)是非常容易采样的，一般通过[线性同余发生器](https://baike.baidu.com/item/%E7%BA%BF%E6%80%A7%E5%90%8C%E4%BD%99%E5%8F%91%E7%94%9F%E5%99%A8/22674963?fr=aladdin)就可以很方便的生成![](./img/b6dbc33006b907f2db1855810abfce98.svg)之间的伪随机数样本。而其他常见的概率分布，无论是离散的分布还是连续的分布，它们的样本都可以通过![](./img/28398c8992cbaccdefad91eb144f6768.svg)的样本转换而得。那么应该如何得到呢？这就是逆采样。

我们以指数分布为例，说明如何通过均匀分布来采样服从指数分布的样本集。指数分布的概率密度函数PDF为：

![](./img/c9114a24aab37bd8a4bcac6eb25ff4ad.svg)

那么它的概率分布函数为：

![](./img/551fee883b4973e8cfffc7c514974ccb.svg)

下图为指数分布和均匀分布的CDF图。从左图上看，在![](./img/d7b89f725185c1fd9984123cbaa89612.svg)的部分是一个单调递增的函数（在定义域上单调非减），定义于和值域是![](./img/62cfe5a9af60c0094c2bc847c7d52e72.svg)，在![](./img/4130c89f2d12c3ac81aba3adbff28685.svg)大的地方它增长快，反之亦然。

![逆采样1.jpeg](./img/1592289666979-2d965bac-3cb6-43e2-881c-ee790beb32d1.jpeg)

因为它是唯一映射的(在大于![](./img/cfcd208495d565ef66e7dff9f98764da.svg)的部分，接下来我们只考虑这一部分)，所以它的反函数可以表示为![](./img/39c45a9323d7a2def0f8336b4b166195.svg)，![](./img/4ec4b782a8856c0f257003cb66b444c8.svg)，值域为![](./img/5b76f27126afc66b22df30bfbd2ae76a.svg)。因为![](./img/800618943025315f869e4e1f09471012.svg)单调递增，所以![](./img/d2712e175cb6f6c9e2a61fa6be1371ad.svg)也是单调递增的：

![](./img/aa11ac09392d38026c108e701bd2df83.svg)<br />![](./img/c4986dbde9282a1bfa00b62553f16b86.svg)

利用反函数的定义，我们有：

![](./img/336371dc459d06952713878d5bb1a770.svg)

接下来，我们定义一下![](./img/ccfcd347d0bf65dc77afe01a3306a96b.svg)均匀分布的CDF，这个很好理解：

![](./img/96ba0caf3ec72017be09854055f155fd.svg)

根据上两式，有：

![](./img/8bae2a6115d7ec885521c054f8cdd160.svg)

因为![](./img/d76f2c4d6bdf142af5106c3f36e9e970.svg)的值域![](./img/e62d239822831122dd571c0d362408f7.svg)，根据均匀分布的CDF，上式可改写为：

![](./img/9dcc67485b6344dceb4cb768006947fb.svg)

据![](./img/d76f2c4d6bdf142af5106c3f36e9e970.svg)的定义，它是![](./img/2ebf242e0f48557d21dd539b8a721035.svg)分布的CDF，所以上式的意思是![](./img/39c45a9323d7a2def0f8336b4b166195.svg)符合![](./img/2ebf242e0f48557d21dd539b8a721035.svg)分布，我们通过![](./img/800618943025315f869e4e1f09471012.svg)的反函数将一个![](./img/cfcd208495d565ef66e7dff9f98764da.svg)到![](./img/c4ca4238a0b923820dcc509a6f75849b.svg)均匀分布的随机数转换成了符合![](./img/2ebf242e0f48557d21dd539b8a721035.svg)分布的随机数，注意，以上推导对于CDF可逆的分布都是一样的。对于![](./img/2ebf242e0f48557d21dd539b8a721035.svg)来说，它的反函数的形式是：

![](./img/a36b84c2ed94bad6bfe42f68bee60e86.svg)

具体的映射关系可以看下图(a)，我们从y轴0-1的均匀分布样本(绿色)映射得到了服从指数分布的样本(红色)

![逆采样2.jpeg](./img/1592290099824-335f81b6-e39b-4c63-8ffc-499dcc4da260.jpeg)

最后绘制出来的直方图可以看出来就是![](./img/2ebf242e0f48557d21dd539b8a721035.svg)分布图，见上图(b)。可以看到随着采样数量的变多，概率直方图和真实的CDF就越接近。以上就是逆采样的过程。我们的结论是：因为CDF是单调函数(累积的概率只能越来越大，直到为1)，因此，只要某分布的CDF可逆，那么就可以通过均匀分布来采样服从该分布的样本集。

<a name="009716d0"></a>
## 拒绝采样(Reject Sampling)

对于常见的分布，如均匀分布，高斯分布，指数分布，t分布，F分布，Beta分布，Gamma分布等，可以采用逆采样的方法进行采样；不过很多时候，我们的概率分布不是常见的分布，这些分布的概率分布函数CDF 不可逆，因此没有办法用逆采样来采样，这意味着我们没法方便的得到这些非常见的概率分布的样本集。拒绝采样就是用来解决这个问题的一种随机采样方法。

我们以求圆周率![](./img/4f08e3dba63dc6d40b22952c7a9dac6d.svg)的例子入手，讲解拒绝采样的思想。通过采样的方法来计算![](./img/4f08e3dba63dc6d40b22952c7a9dac6d.svg)值，也就是在一个![](./img/b99a31f00d8eff828fb2b1657efe2f4b.svg)的范围内随机采样一个点，如果它到原点的距离小于![](./img/c4ca4238a0b923820dcc509a6f75849b.svg)，则说明它在![](./img/eca3bf81573307ec3002cf846390d363.svg)圆内，则接受它，最后通过接受的占比来计算![](./img/eca3bf81573307ec3002cf846390d363.svg)圆形的面积，从而根据公式反算出预估![](./img/81b532981dd96eea9c795e6fd2010aeb.svg)的值，随着采样点的增多，最后的结果![](./img/81b532981dd96eea9c795e6fd2010aeb.svg)会愈加准确。

![拒绝采样.jpeg](./img/1592292080335-4acfc07b-5b9f-401b-b415-366439a23a27.jpeg)

上面这个例子里说明一个问题，我们想求一个空间里均匀分布的集合面积，可以尝试在更大范围内按照均匀分布随机采样，如果采样点在集合中，则接受，否则拒绝。最后的接受概率就是集合在”更大范围“的面积占比。接下来，我们来形式化地说明拒绝采样。

给定一个概率分布![](./img/4c6cb8bf8e14f54bc5d04535d6e6d333.svg)，其中，![](./img/801858210a255fb404e08913eea47b9e.svg)已知，![](./img/5c3bc7216a39c8f0899d0153ef6761d4.svg)为归一化常数，未知。要对该分布![](./img/c07f09e1ca3053942a3e035409faa7c0.svg)进行拒绝采样，首先需要借用一个简单的参考分布(proposal distribution)，记为![](./img/9dbcf8f4523b910764da7f544a192a69.svg)，该分布的采样易于实现，如均匀分布、高斯分布。然后引入常数![](./img/8ce4b16b22b58894aa86c421e8759df3.svg)，使得对所有的![](./img/fbade9e36a3f36d3d676c1b808451dd7.svg)，满足![](./img/98ce48a30ff3f78479097116666f9e1b.svg)，如下图所示，红色的曲线为![](./img/801858210a255fb404e08913eea47b9e.svg)，蓝色的曲线为![](./img/1a46a5a2dfd115b4fed040878abc3039.svg)。

![拒绝采样2.png](./img/1592292199438-fca896b5-2602-4967-8d40-a937b4f0c185.png)

在每次采样中，首先从![](./img/a5827c143f7d49ac84e4a10aac2b490c.svg)采样一个数值![](./img/f82c0544b80586d86d1b04463ed6d686.svg)，然后在区间![](./img/44ea3b02f9d73a0f610ef0aa264a761c.svg)进行均匀采样，得到![](./img/d526fcdf9b10529b54820f729dbb25f4.svg)。如果![](./img/bb8ea7ec0ef1d4bfda6e037631849d06.svg)，则保留该采样值，否则舍弃该采样值。最后得到的数据就是对![](./img/c07f09e1ca3053942a3e035409faa7c0.svg)分布的一个近似采样。结合图，直观来理解上述的过程：在![](./img/d319adad28d7122db7f752ecef9a2b89.svg)这条线上，从![](./img/44ea3b02f9d73a0f610ef0aa264a761c.svg)均匀采样中一个值，如果这个值小于![](./img/0779d65214a469f67b00b3af9efc1d4e.svg)，即这个均匀采样的这个值落在了![](./img/0779d65214a469f67b00b3af9efc1d4e.svg)下方，我们就接受![](./img/f82c0544b80586d86d1b04463ed6d686.svg)这个采样值。

我们知道，每次采样的接受概率计算如下：

![](./img/43c3c35a9f75ed6e05f44f2fab528b0f.svg)

所以，为了提高接受概率，防止舍弃过多的采样值而导致采样效率低下，![](./img/8ce4b16b22b58894aa86c421e8759df3.svg)的选取应该在满足![](./img/78d73822ecd9e87a97edede372077a06.svg)的基础上尽可能小。

拒绝采样问题可以这样理解，![](./img/801858210a255fb404e08913eea47b9e.svg)与![](./img/9dd4e461268c8034f5c8564e155c67a6.svg)轴之间的区域为要估计的问题，类似于求![](./img/4f08e3dba63dc6d40b22952c7a9dac6d.svg)提到的圆形区域，![](./img/1a46a5a2dfd115b4fed040878abc3039.svg)与![](./img/9dd4e461268c8034f5c8564e155c67a6.svg)轴之间的区域为参考区域，类似于上面提到的正方形。由于![](./img/1a46a5a2dfd115b4fed040878abc3039.svg)与![](./img/9dd4e461268c8034f5c8564e155c67a6.svg)轴之间的区域面积为![](./img/8ce4b16b22b58894aa86c421e8759df3.svg)（原来的概率密度函数的面积为![](./img/c4ca4238a0b923820dcc509a6f75849b.svg)，现在扩大了![](./img/8ce4b16b22b58894aa86c421e8759df3.svg)倍）所以，![](./img/801858210a255fb404e08913eea47b9e.svg)与![](./img/9dd4e461268c8034f5c8564e155c67a6.svg)轴之间的区域面积除以![](./img/8ce4b16b22b58894aa86c421e8759df3.svg)即为对![](./img/c07f09e1ca3053942a3e035409faa7c0.svg)的估计。在每一个采样点，以![](./img/44ea3b02f9d73a0f610ef0aa264a761c.svg)为界限，落在![](./img/801858210a255fb404e08913eea47b9e.svg)曲线以下的点就是服从![](./img/c07f09e1ca3053942a3e035409faa7c0.svg)分布的点。

我们必须知道复杂分布的概率密度函数PDF，才可以进行拒绝采样。然而，在现实情况中：

- 对于一些二维分布![](./img/374caa6fc5bcf5b0106543cd2e7876ef.svg)，有时候我们只能得到条件分布![](./img/11eb1188be3d3254efadf336ca1c207e.svg)和![](./img/d2f879a26a2edeaf401ce36604f5a3a6.svg)，却很难得到二维分布的概率密度函数![](./img/374caa6fc5bcf5b0106543cd2e7876ef.svg)的一般形式，这时我们无法用拒绝采样得到其样本集。
- 对于一些高维的复杂非常见分布![](./img/481f480cbe2702982e4f789105b1cc72.svg)，我们要找到一个合适的![](./img/9dbcf8f4523b910764da7f544a192a69.svg)和![](./img/8ce4b16b22b58894aa86c421e8759df3.svg)非常困难。

因此，实际上，我们仍然要找到一种方法可以解决如何方便得到各种复杂概率分布的对应的采样样本集的问题。马尔科夫链有能力帮助找到这些复杂概率分布的对应的采样样本集，而这也是MCMC采样的基础。

<a name="48dfd217"></a>
## 采样和蒙特卡罗方法

有许多原因使我们希望从某个分布中采样。当我们需要以较小的代价近似许多项或某个积分时，采样是一种很灵活的选择。有时候，我们使用它加速一些很费时却易于处理的求和估计，就像我们使用小批量对整个训练代价进行子采样一样。在其他情况下，我们需要近似一个难以处理的求和或积分，例如估计一个无向模型中配分函数对数的梯度时。在许多其他情况下，抽样实际上是我们的目标，例如我们想训练一个可以从训练分布采样的模型。

<a name="d38ebe52"></a>
### [蒙特卡罗 vs 拉斯维加斯](https://www.zhihu.com/question/20254139)

我们知道，随机算法在采样不全时，通常不能保证找到最优解，只能说是尽量找。那么根据怎么个“尽量”法儿，我们我们把随机算法分成两类：

- 蒙特卡罗算法：采样越多，越**近似**最优解；
- 拉斯维加斯算法：采样越多，越**有机会找到**最优解；

举个例子，假如筐里有100个苹果，让我每次闭眼拿1个，挑出最大的。于是我随机拿1个，再随机拿1个跟它比，留下大的，再随机拿1个……我每拿一次，留下的苹果都至少不比上次的小。拿的次数越多，挑出的苹果就越大，但我除非拿100次，否则无法肯定挑出了最大的。这个挑苹果的算法，就属于蒙特卡罗算法——**尽量找好的，但不保证是最好的**。

而拉斯维加斯算法，则是另一种情况。假如有一把锁，给我100把钥匙，只有1把是对的。于是我每次随机拿1把钥匙去试，打不开就再换1把。我试的次数越多，打开（最优解）的机会就越大，但在打开之前，那些错的钥匙都是没有用的。这个试钥匙的算法，就是拉斯维加斯的——**尽量找最好的，但不保证能找到**。

这两类随机算法之间的选择，往往受到问题的局限。如果问题要求在有限采样内，必须给出一个解，但不要求是最优解，那就要用蒙特卡罗算法。反之，如果问题要求必须给出最优解，但对采样没有限制，那就要用拉斯维加斯算法。

比如机器下棋的算法本质都是搜索树，围棋难在它的树宽可以达到好几百。在有限时间内要遍历这么宽的树，就只能牺牲深度（俗称“往后看几步”），但围棋又是依赖远见的游戏，甚至不仅是看“几步”的问题。所以，要想保证搜索深度，就只能放弃遍历，改为随机采样——这就是为什么在没有MCTS（蒙特卡罗搜树）类的方法之前，机器围棋的水平几乎是笑话。而采用了MCTS方法后，搜索深度就大大增加了。


### 蒙特卡罗采样的基础

蒙特卡罗原来是一个赌场的名称，用它作为名字大概是因为蒙特卡罗方法是一种随机模拟的方法，这很像赌博场里面的扔骰子的过程。最早的蒙特卡罗方法都是为了求解一些不太好求解的求和或者积分问题。比如积分：

![](./img/da6e619f47af661b4b4cd34b5e61c59d.svg)

如果我们很难求解出![](./img/50bbd36e1fd2333108437a2ca378be62.svg)的原函数，那么这个积分比较难求解。当然我们可以通过蒙特卡罗方法来模拟求解近似值。如何模拟呢？假设我们函数图像如下图：

![蒙特卡罗1.png](./img/1592292736504-19cda57a-bf68-49d4-b3a5-13a894ad2b0f.png)

一个简单的近似求解方法是在![](./img/2c3d331bc98b44e71cb2aae9edadca7e.svg)之间随机的采样一个点。比如![](./img/3e0d691f3a530e6c7e079636f20c111b.svg)，然后用![](./img/c58b04667bcf6700fa38285f33640500.svg)代表在![](./img/2c3d331bc98b44e71cb2aae9edadca7e.svg)区间上所有的![](./img/50bbd36e1fd2333108437a2ca378be62.svg)的值。那么上面的定积分的近似求解为：

![](./img/a9910d91795d057454e31c6b6b9b2e76.svg)

当然，用一个值代表![](./img/2c3d331bc98b44e71cb2aae9edadca7e.svg)区间上所有的![](./img/50bbd36e1fd2333108437a2ca378be62.svg)的值太粗糙了。我们可以采样![](./img/2c3d331bc98b44e71cb2aae9edadca7e.svg)区间的![](./img/7b8b965ad4bca0e41ab51de7b31363a1.svg)个值：![](./img/cc890b3efe7d3ca99adcca7417d86bd8.svg)，用它们的均值来代表![](./img/2c3d331bc98b44e71cb2aae9edadca7e.svg)区间上所有的![](./img/50bbd36e1fd2333108437a2ca378be62.svg)的值。这样我们上面的定积分的近似求解为：

![](./img/23dd2170b9922bfa93f34878b752ac16.svg)

虽然上面的方法可以一定程度上求解出近似的解，但是它隐含了一个假定，即![](./img/9dd4e461268c8034f5c8564e155c67a6.svg)在![](./img/2c3d331bc98b44e71cb2aae9edadca7e.svg)之间是均匀分布的，而绝大部分情况，![](./img/9dd4e461268c8034f5c8564e155c67a6.svg)在![](./img/2c3d331bc98b44e71cb2aae9edadca7e.svg)之间不是均匀分布的。若我们用上面的方法，则模拟求出的结果很可能和真实值相差甚远

如果我们可以得到![](./img/9dd4e461268c8034f5c8564e155c67a6.svg)在![](./img/2c3d331bc98b44e71cb2aae9edadca7e.svg)的概率分布函数![](./img/4130c89f2d12c3ac81aba3adbff28685.svg)，那么我们的定积分求和可以这样进行：

![](./img/34e2135b2ba71e0eb698e4ce91aebad0.svg)

上式最右边的这个形式就是蒙特卡罗方法的一般形式。当然这里是连续函数形式的蒙特卡罗方法，但是在离散时一样成立。关于上式，我们可以这么理解最后一步转换：由于![](./img/f58154ee0e580776de5b53c37dc7f8a5.svg)可以看做是![](./img/9156a02984e6013066ad583cf16d6734.svg)基于概率分布![](./img/4130c89f2d12c3ac81aba3adbff28685.svg)的期望，那么我们可以用期望的方法来求这个式子的值。而计算期望的一个近似方法是取![](./img/9156a02984e6013066ad583cf16d6734.svg)的若干个基于分布![](./img/4130c89f2d12c3ac81aba3adbff28685.svg)的采样点，然后求平均值得到。

<a name="d5875ac8"></a>
#### Example

假设![](./img/50bbd36e1fd2333108437a2ca378be62.svg)的取值只有2个，![](./img/d764a0cc630a3ef1ec5c6e22972b3470.svg)。对应的![](./img/415290769594460e2e485922904f345d.svg)值分别是![](./img/1ec6923062046ef5efd290790844c161.svg)。其中![](./img/9dd4e461268c8034f5c8564e155c67a6.svg)的取值不是平均的，取![](./img/d37ffc54b67ce8de1f01efb1f2e33689.svg)的概率![](./img/500858e9b80f007abe18f3fd86d42a8a.svg)，取![](./img/a124de930ea74cbb3991c742ba7504d4.svg)的概率![](./img/7d77e95fdb5548ef08074f335f2c3a27.svg)。

那么严格来说，根据![](./img/34e2135b2ba71e0eb698e4ce91aebad0.svg)，对应的![](./img/50bbd36e1fd2333108437a2ca378be62.svg)的积分等于![](./img/9156a02984e6013066ad583cf16d6734.svg)基于概率分布![](./img/4130c89f2d12c3ac81aba3adbff28685.svg)的期望，根据公式，则有![](./img/457e7a08e1808b1e4669d2adb18ea0c0.svg)。

此时我们采样三次，期望求近似结果。假设第一次采样到1，第二三次采样到2，那么最后的近似结果是

![](./img/862ec13f6cb081258905a553274cc1b0.svg)

这个![](./img/001aa5ac21afb53cbb5cb01798fc294e.svg)就是我们![](./img/e4da3b7fbbce2345d7772b0674a318d5.svg)的近似。虽然有些距离，但是由于采样太少原因。若我们采样100次，得到26次1，74次2，那么近似结果是![](./img/f05b2f7faa7e03282c66a4d94f7c9466.svg)。可见结果越来越接近。接近的原因是随着采样数的增多，采样的样本分布越来越接近于![](./img/9dd4e461268c8034f5c8564e155c67a6.svg)本来的分布。

<a name="dcde3eb7"></a>
### 基于马尔科夫链采样

<a name="7f157684"></a>
#### 马尔可夫链转移矩阵性质

马尔科夫链定义本身比较简单，它假设某一时刻状态转移的概率只依赖于它的前一个状态。举个形象的比喻，假如每天的天气是一个状态的话，那个今天是不是晴天只依赖于昨天的天气，而和前天的天气没有任何关系。当然这么说可能有些武断，但是这样做可以大大简化模型的复杂度，因此马尔科夫链在很多时间序列模型中得到广泛的应用，比如循环神经网络RNN，隐式马尔科夫模型HMM等，当然MCMC也需要它。

如果用精确的数学定义来描述，则假设我们的序列状态是![](./img/d4314bbca6da04380f653cf3592c371d.svg)那么我们在时刻![](./img/48dd05b08598d9ff5522f26dce7851ed.svg)的状态的条件概率仅仅依赖于时刻![](./img/ff41056dec337b31678a729b1ecfcfb4.svg)，即：

![](./img/c6ea86334a6dda889e6702091ec15250.svg)

既然某一时刻状态转移的概率只依赖于它的前一个状态，那么我们只要能求出系统中任意两个状态之间的转换概率，这个马尔科夫链的模型就定了。

![马尔科夫1.png](./img/1592295862527-24beefd2-0b6f-4eba-89b7-8c4922df703b.png)

这个马尔科夫链是表示股市模型的，共有三种状态：牛市（Bull market）,熊市（Bear market）和横盘（Stagnant market）。每一个状态都以一定的概率转化到下一个状态。比如，牛市以0.025的概率转化到横盘的状态。这个状态概率转化图可以以矩阵的形式表示。如果我们定义矩阵![](./img/44c29edb103a2872f519ad0c9a0fdaaa.svg)某一位置![](./img/5270ae675fac24f97e172dcd9b18fa92.svg)的值为![](./img/2ccc34e12c17d05e5ab4edce34608a75.svg)，即从状态![](./img/865c0c0b4ab0e063e5caa3387c1a8741.svg)转化到状态![](./img/363b122c528f54df4a0446b6bab05515.svg)的概率，并定义牛市为状态0，熊市为状态1，横盘为状态2。这样我们得到了马尔科夫链模型的状态转移矩阵为：

![](./img/a2771a581272615f372dd6ac04421009.svg)

讲了这么多，那么马尔科夫链模型的状态转移矩阵和我们蒙特卡罗方法需要的概率分布样本集有什么关系呢？这需要从马尔科夫链模型的状态转移矩阵的收敛性质讲起。**马尔科夫链模型的状态转移矩阵收敛到的稳定概率分布与我们的初始状态概率分布无关，且这个稳定分布是唯一的。**这是一个非常好的性质，也就是说，如果我们得到了这个稳定概率分布对应的马尔科夫链模型的状态转移矩阵，则我们可以用任意的概率分布样本开始，带入马尔科夫链模型的状态转移矩阵，这样经过一些序列的转换，最终就可以得到符合对应稳定概率分布的样本。


#### 基于马尔可夫链的采样

如果我们得到了某个平稳分布所对应的马尔科夫链状态转移矩阵![](./img/44c29edb103a2872f519ad0c9a0fdaaa.svg)，我们就很容易采用出这个平稳分布的样本集。

假设我们任意初始的概率分布是![](./img/a7e4e3dfe945c15c398ceab45f9aa7da.svg)，经过第一轮马尔可夫链状态转移后的概率分布是![](./img/9b586255ec70884f8751238193868130.svg)，第![](./img/865c0c0b4ab0e063e5caa3387c1a8741.svg)轮的概率分布是![](./img/cc506a8d13fbd56ed7be2c31f8859ee9.svg)。假设经过![](./img/7b8b965ad4bca0e41ab51de7b31363a1.svg)轮后马尔可夫链收敛到我们的平稳分布![](./img/e5cb16c20d9f01bbbfe8f299e28d1f4b.svg)，即：

![](./img/5fb14e16aa68642f67f6e7778d559f11.svg)

对于每个分布![](./img/cc506a8d13fbd56ed7be2c31f8859ee9.svg)，我们有：

![](./img/b2de8ebe16de3cbfc1eacbb0e905d31e.svg)

现在我们可以开始采样了，首先，基于初始任意简单概率分布比如高斯分布![](./img/a7e4e3dfe945c15c398ceab45f9aa7da.svg)采样得到状态值![](./img/3e0d691f3a530e6c7e079636f20c111b.svg)，基于条件概率分布![](./img/0486dad8d7e19b98b083d0e56232e41e.svg)采样状态值![](./img/aa687da0086c1ea060a8838e24611319.svg)，一直进行下去，当状态转移进行到一定的次数时，比如到![](./img/7b8b965ad4bca0e41ab51de7b31363a1.svg)次时，我们认为此时的采样集![](./img/4f05989bf438666f4e35900ef05937ba.svg)即是符合我们的平稳分布的对应样本集，可以用来做蒙特卡罗模拟求和了。


#### 采样过程

1. 输入马尔可夫链状态转移矩阵![](./img/44c29edb103a2872f519ad0c9a0fdaaa.svg)，设定状态转移次数阈值![](./img/6c773b2b7798e5713845e475d0c4b4c7.svg)，需要的样本个数![](./img/e501ae2ad90dc374410a774da21c5739.svg)
2. 从任意简单概率分布采样得到初始状态值![](./img/3e0d691f3a530e6c7e079636f20c111b.svg)
3. ![](./img/95c3d16036e76cced3cddf4266e5a5f0.svg)：从条件概率分布![](./img/bf276c3bcc375d4850d7e0e6d26003cb.svg)中采样得到样本![](./img/940d6748ef869ab4c373721ae0be26c6.svg)
4. 样本集![](./img/1c171204718cab2ae8997a9ecb622a51.svg)即为我们需要的平稳分布对应的样本集

对采样过程的两点补充说明：

- 第3.步，每对![](./img/bf276c3bcc375d4850d7e0e6d26003cb.svg)采样一次就意味着模拟了一次![](./img/b96daee1fbd5fe8dc0363ceaf8a2f5f4.svg)操作。在此过程中，状态转化矩阵![](./img/44c29edb103a2872f519ad0c9a0fdaaa.svg)没有变化。
- 第4.步，注意的是采样的样本集的下标是![](./img/279911fc64b03a5eb46f7abebc8180f4.svg)，即状态稳定之后采样的样本才是我们需要的样本集。即我们需要从真正趋于稳定时候的分布抽样样本。

如果假定我们可以得到我们需要采样样本的平稳分布所对应的马尔科夫链状态转移矩阵![](./img/44c29edb103a2872f519ad0c9a0fdaaa.svg)，那么我们就可以用马尔科夫链采样得到我们需要的样本集，进而进行蒙特卡罗模拟。但是一个重要的问题是，随意给定一个平稳分布![](./img/4f08e3dba63dc6d40b22952c7a9dac6d.svg)，如何得到它所对应的马尔科夫链状态转移矩阵![](./img/44c29edb103a2872f519ad0c9a0fdaaa.svg)呢？这是个大问题。我们绕了一圈似乎还是没有解决任意概率分布采样样本集的问题。幸运的是，MCMC采样通过迂回的方式解决了上面这个大问题，以及它的使用改进版采样：M-H采样和Gibbs采样。



### MCMC采样

在解决从平稳分布![](./img/4f08e3dba63dc6d40b22952c7a9dac6d.svg)，找到对应的马尔可夫链状态转移矩阵![](./img/44c29edb103a2872f519ad0c9a0fdaaa.svg)之前，我们还需要先看看马尔科夫链的细致平稳条件，定义如下：

如果非周期马尔科夫链的状态转移矩阵![](./img/44c29edb103a2872f519ad0c9a0fdaaa.svg)和概率分布![](./img/e5cb16c20d9f01bbbfe8f299e28d1f4b.svg)对于所有的![](./img/865c0c0b4ab0e063e5caa3387c1a8741.svg)，![](./img/363b122c528f54df4a0446b6bab05515.svg)满足：

![](./img/0859e8ee9b92843dc92647c7ce7ac75a.svg)

则称概率分布![](./img/e5cb16c20d9f01bbbfe8f299e28d1f4b.svg)是状态转移矩阵![](./img/44c29edb103a2872f519ad0c9a0fdaaa.svg)的平稳分布。

证明很简单，由细致平稳条件有：

![](./img/ebcf9bbd66a0c781cb5577ef3ac66112.svg)

将上式用矩阵表示即为：

![](./img/3445181d8abfa69b005bb2e3c46f875e.svg)

即满足马尔可夫链的收敛性质。也就是说，只要我们找到了可以使概率分布![](./img/e5cb16c20d9f01bbbfe8f299e28d1f4b.svg)满足细致平稳分布的矩阵![](./img/44c29edb103a2872f519ad0c9a0fdaaa.svg)即可。这给了我们寻找从平稳分布![](./img/4f08e3dba63dc6d40b22952c7a9dac6d.svg)，找到对应的马尔可夫链状态转移矩阵![](./img/44c29edb103a2872f519ad0c9a0fdaaa.svg)的新思路。

不过不幸的是，仅仅从细致平稳条件还是很难找到合适的矩阵![](./img/44c29edb103a2872f519ad0c9a0fdaaa.svg)。比如我们的目标平稳分布是![](./img/e5cb16c20d9f01bbbfe8f299e28d1f4b.svg)，随机找一个马尔可夫链状态转移矩阵![](./img/f09564c9ca56850d4cd6b3319e541aee.svg)，它是很难满足细致平稳条件的，即：

![](./img/2581f3f12933be5976d61eeaa912fc1f.svg)

​	MCMC采样是这样做的。由于一般情况下，目标平稳分布![](./img/e5cb16c20d9f01bbbfe8f299e28d1f4b.svg)和某一个马尔可夫链状态转移矩阵![](./img/f09564c9ca56850d4cd6b3319e541aee.svg)不满足细致平稳条件，即上式。我们可以对上式做一个改造，使细致平稳条件成立。方法是引入一个![](./img/5c99cffaae1f4cfffd64689c2964660e.svg)，使上式可以去等号，即：

![](./img/78ad0d8c2fe159340b590e7a4ac68977.svg)

​	问题是什么样的![](./img/5c99cffaae1f4cfffd64689c2964660e.svg)可以使等式成立呢？其实很简单，只要满足下两式即可：

![](./img/4498ce8e541ffe11e5eef2ac10298f22.svg)

![](./img/a966371416e90bd6d1958634b1d1c806.svg)

​	这样，我们就得到了我们的分布![](./img/e5cb16c20d9f01bbbfe8f299e28d1f4b.svg)对应的马尔可夫链状态转移矩阵![](./img/44c29edb103a2872f519ad0c9a0fdaaa.svg)，满足：

![](./img/cd8eb098b35fefff434a9968a51aba5c.svg)

​	也就是说，我们的目标矩阵![](./img/44c29edb103a2872f519ad0c9a0fdaaa.svg)可以通过任意一个马尔可夫链状态转移矩阵![](./img/f09564c9ca56850d4cd6b3319e541aee.svg)乘以![](./img/5c99cffaae1f4cfffd64689c2964660e.svg)得到。![](./img/5c99cffaae1f4cfffd64689c2964660e.svg)我们又一般称之为接受率，取值在![](./img/ccfcd347d0bf65dc77afe01a3306a96b.svg)之间。物理意义可以理解为：在原来的马氏链上，从状态![](./img/865c0c0b4ab0e063e5caa3387c1a8741.svg)以![](./img/722a863723d599c0cf0f31b59bc23fa8.svg)的概率转移到状态![](./img/363b122c528f54df4a0446b6bab05515.svg)的时候，我们以![](./img/5c99cffaae1f4cfffd64689c2964660e.svg)的概率接受这个转移。即目标矩阵![](./img/44c29edb103a2872f519ad0c9a0fdaaa.svg)可以通过任意一个马尔科夫链状态转移矩阵![](./img/f09564c9ca56850d4cd6b3319e541aee.svg)以一定的接受率获得。这个很像拒绝采样的思想，那里是以一个常用分布通过一定的接受-拒绝概率得到一个非常见分布，这里是以一个常见的马尔科夫链状态转移矩阵![](./img/f09564c9ca56850d4cd6b3319e541aee.svg)通过一定的接受-拒绝概率得到目标转移矩阵![](./img/44c29edb103a2872f519ad0c9a0fdaaa.svg)，两者的解决问题思路是类似的。





#### 采样过程

1. 输入任意选定的马尔可夫链状态转移矩阵![](./img/f09564c9ca56850d4cd6b3319e541aee.svg)，平稳分布![](./img/e5cb16c20d9f01bbbfe8f299e28d1f4b.svg)，设定状态转移次数阈值![](./img/6c773b2b7798e5713845e475d0c4b4c7.svg)，需要的样本个数![](./img/e501ae2ad90dc374410a774da21c5739.svg)
2. 从任意简单概率分布采样得到初始状态值![](./img/3e0d691f3a530e6c7e079636f20c111b.svg)
3. ![](./img/95c3d16036e76cced3cddf4266e5a5f0.svg)：
   1. 从条件概率分布![](./img/6b4802e17e4fd3f6c512e1c90a8d625b.svg)中采样得到样本![](./img/47093832fc0630ca3242c37c78b346fa.svg)
   2. 从均匀分布采样![](./img/c604d90dc5c3757aa3d36d12a668e9c0.svg)
   3. 如果![](./img/b5702bc0f0de5026165a01cc4faccfd8.svg)，则接受转移![](./img/802376780f62163aef5a3c84af12343a.svg)，即![](./img/80601bb7737e26a1c56c6a658ec0e7ad.svg)
   4. 否则不接受转移，即![](./img/03da768244159b8f585ace5ec9a517c3.svg)

​	样本集![](./img/3912ecf99c98bc0f0d3da9cd306802af.svg)即为我们需要的平稳分布对应的样本集。对于3.的d步，![](./img/bd2839d972fd6c244ee0ab62ff22faad.svg)意味着，尽管不接受转移，但是也把它放入了样本序列。而在有些算法中，被拒绝的样本就不进入采样样本序列。尽管双方有些mismatch，不过对理解算法影响不大。

​	上面这个过程基本上就是MCMC采样的完整采样理论了，但是这个采样算法还是比较难在实际中应用，为什么呢？问题在上面第三步的（c）步骤，接受率这儿。由于![](./img/193e41a119b7273717dda72c56355a13.svg)可能非常的小，比如![](./img/cb5ae17636e975f9bf71ddf5bc542075.svg)，导致我们大部分的采样值都被拒绝转移，采样效率很低。有可能我们采样了上百万次马尔可夫链还没有收敛，也就是上面这个![](./img/6c773b2b7798e5713845e475d0c4b4c7.svg)要非常非常的大，这让人难以接受，怎么办呢？这时就轮到我们的M-H采样出场了。


### M-H采样

M-H采样是Metropolis-Hastings采样的简称，这个算法首先由Metropolis提出，被Hastings改进，因此被称之为Metropolis-Hastings采样或M-H采样。M-H采样解决了MCMC采样接受率过低的问题。

​	我们回到MCMC采样的细致平稳条件：

![](./img/78ad0d8c2fe159340b590e7a4ac68977.svg)

​	我们采样效率低的原因是![](./img/5c99cffaae1f4cfffd64689c2964660e.svg)太小了，比如为![](./img/cb5ae17636e975f9bf71ddf5bc542075.svg)，而![](./img/14798726f9be66bea93fb8ff827951cd.svg)为![](./img/3d522deaf85577451c01974654b36ad3.svg)。即：

![](./img/9ba915a89e8e2908323dce2fc2bb72ff.svg)

​	我们可以看到，如果两边同时扩大五倍，接受率提高到了![](./img/d310cb367d993fb6fb584b198a2fd72c.svg)，但是细致平稳条件却仍然是满足的，即：

![](./img/8dd6465f5b0ee6f94edff552685b3b95.svg)

​	这样我们的接受率可以做如下改进，即：

![](./img/32379dddb5df46bea2aa39c80e90a8f0.svg)

​	对上式多解释一下：此时![](./img/5c99cffaae1f4cfffd64689c2964660e.svg)已经不是原来的意义，变为了![](./img/d8572b6e1f893a1963d4d61cae8a661c.svg)的倍率关系，这里默认右边为![](./img/c4ca4238a0b923820dcc509a6f75849b.svg)，看左边的较大的那个接受率的值。即当原始的![](./img/6df1a59e3e27e7a34106908749851d92.svg)时，![](./img/5c99cffaae1f4cfffd64689c2964660e.svg)扩大了![](./img/d1eeff4637ea42331cee40e3e2a12109.svg)倍；![](./img/6dfdfd018fea7a46b712fe5360eed353.svg)时，![](./img/5c99cffaae1f4cfffd64689c2964660e.svg)直接变为![](./img/c4ca4238a0b923820dcc509a6f75849b.svg)。



#### 采样过程

1. 输入任意选定的马尔可夫链状态转移矩阵![](./img/f09564c9ca56850d4cd6b3319e541aee.svg)，平稳分布![](./img/e5cb16c20d9f01bbbfe8f299e28d1f4b.svg)，设定状态转移次数阈值![](./img/6c773b2b7798e5713845e475d0c4b4c7.svg)，需要的样本个数![](./img/e501ae2ad90dc374410a774da21c5739.svg)
2. 从任意简单概率分布采样得到初始状态值![](./img/3e0d691f3a530e6c7e079636f20c111b.svg)
3. ![](./img/95c3d16036e76cced3cddf4266e5a5f0.svg)：
   1. 从条件概率分布![](./img/6b4802e17e4fd3f6c512e1c90a8d625b.svg)中采样得到样本![](./img/47093832fc0630ca3242c37c78b346fa.svg)
   2. 从均匀分布采样![](./img/c604d90dc5c3757aa3d36d12a668e9c0.svg)
   3. 如果![](./img/ed483f4986707fe8a219c27407f5f691.svg)，则接受转移![](./img/802376780f62163aef5a3c84af12343a.svg)，即![](./img/80601bb7737e26a1c56c6a658ec0e7ad.svg)
   4. 否则不接受转移，即![](./img/03da768244159b8f585ace5ec9a517c3.svg)

样本集![](./img/3912ecf99c98bc0f0d3da9cd306802af.svg)即为我们需要的平稳分布对应的样本集。





### 吉布斯采样(Gibbs sampling)

​	M-H采样已经可以很好的解决蒙特卡罗方法需要的任意概率分布的样本集的问题。但是M-H采样有两个缺点：一是需要计算接受率，在高维时计算量大。并且由于接受率的原因导致算法收敛时间变长。二是有些高维数据，特征的条件概率分布好求，但是特征的联合分布不好求。因此需要一个好方法改进M-H采样，这就是Gibbs采样。

​	从上面内容我们得到细致平稳条件：若非周期马尔科夫链的状态转移矩阵![](./img/44c29edb103a2872f519ad0c9a0fdaaa.svg)和概率分布![](./img/e5cb16c20d9f01bbbfe8f299e28d1f4b.svg)对所有的![](./img/ee813f0ede8664a8049b1b6720f03b60.svg)满足：

![](./img/0859e8ee9b92843dc92647c7ce7ac75a.svg)

​	则称概率分布![](./img/e5cb16c20d9f01bbbfe8f299e28d1f4b.svg)是状态转移矩阵![](./img/44c29edb103a2872f519ad0c9a0fdaaa.svg)的平稳分布。

​	在M-H采样中我们通过引入接受率使细致平稳条件满足。现在我们换一个思路。

​	从二维的数据分布开始，假设![](./img/eecbd1dd86876d247f2bd2dcabd9a190.svg)是一个二维联合数据分布，观察第一个特征维度相同的两个点![](./img/6fade784a52596a618875c44ab114918.svg)和![](./img/7ccbb499f9bfced32035196a50ffd2e4.svg)，容易发现下面两式成立：

![](./img/205710a2fc8e26c7c7d766de4ba3279c.svg)

![](./img/9ef9dcf7fcb27c9ba9d427bf13fc93f7.svg)

成立的原因是：![](./img/fc6b23e2f62464333f86838417a67514.svg)和![](./img/bf114c9927dbdad9e8f46edbe7f5cb46.svg)

由于上面两式右边相等，因此我们有：

![](./img/e5b4185fdf5533ba6ea9c13866a2b6b8.svg)

也就是

![](./img/697e679dc5161f3d0b8b8dedf08ec65b.svg)

观察上式再观察细致平稳条件的公式，我们发现在![](./img/e603df22462ec98a4bd0fa368a9eeff3.svg)这条直线上，如果用条件概率分布![](./img/0ec610072b4f56e4bd08f1c15f00514d.svg)作为马尔科夫链的状态转移概率，则任意两个点之间的转移满足细致平稳条件！这真是一个开心的发现，同样的道理，在![](./img/33560fd3c6b5c38bda3c81401bdb6727.svg)这条直线上，如果用条件概率分布![](./img/ee508fa5566dfc8a4579043458dec52b.svg)作为马尔科夫链的状态转移概率，则任意两个点之间的转移也满足细致平稳条件。那是因为假如有一点![](./img/a5abccf500d716a0b759c431149b7931.svg)，我们可以得到：

![](./img/02b9ab05e85abff4d74d8e109bd1d0fa.svg)

基于上面的发现，我们可以这样构造分布![](./img/eecbd1dd86876d247f2bd2dcabd9a190.svg)的马尔可夫链对应的状态转移矩阵![](./img/44c29edb103a2872f519ad0c9a0fdaaa.svg)：

![](./img/1b0e843330e4659c57c55766d11835e7.svg)<br />![](./img/682658d6477da4be4c51b331a4733910.svg)<br />![](./img/e7409319ea1dec6deece92f8e4889bad.svg)

有了上面这样的状态转移矩阵![](./img/44c29edb103a2872f519ad0c9a0fdaaa.svg)，我们很容易验证平面上的任意两点，![](./img/800618943025315f869e4e1f09471012.svg)，满足细致平稳条件：

![](./img/50ec11cd11f3dcbba798e4bd4e4b3fef.svg)





#### 二维Gibbs采样

利用上一节找到的状态转移矩阵，我们就得到了二维Gibbs采样，这个采样必须要知道两个维度之间的条件概率。具体过程如下：

1. 输入平稳分布![](./img/eecbd1dd86876d247f2bd2dcabd9a190.svg)，设定状态转移次数阈值![](./img/6c773b2b7798e5713845e475d0c4b4c7.svg)，需要的样本个数![](./img/e501ae2ad90dc374410a774da21c5739.svg)
2. 随机初始化初始状态值![](./img/8d1fd01bb2b1dd1a58bf696781f12616.svg)和![](./img/d90ebd16653208328b64650bd94dbc54.svg)
3. ![](./img/95c3d16036e76cced3cddf4266e5a5f0.svg)：
   1. 从条件概率分布![](./img/aade11ab95dd07f638b4fa38066d9d77.svg)中采样得到样本![](./img/d58ceb3c51627ecbce36146e5c43f47e.svg)
   2. 从条件概率分布![](./img/23fe8d158a512cb3e33f8aa252957cce.svg)中采样得到样本![](./img/2b28a237fb720845e7a32f20b02b2020.svg)

样本![](./img/fa53fb0581f90c1f6e8777580f185d34.svg)即为我们需要的平稳分布对应的样本集。整个采样过程中，我们通过轮换坐标轴，采样的过程为：

![](./img/68d0be3e2e2a418a698f449e0fd023ec.svg)

用下图可以很直观的看出，采样是在两个坐标轴上不停的轮换的。当然，坐标轴轮换不是必须的，我们也可以每次随机选择一个坐标轴进行采样。不过常用的Gibbs采样的实现都是基于坐标轴轮换的。

![吉布斯采样.png](./img/1592310805365-ee6f5c1c-37ae-46e2-b0db-69f983b726ee.png)





#### 多维Gibbs采样

上面的这个算法推广到多维的时候也是成立的。比如一个![](./img/7b8b965ad4bca0e41ab51de7b31363a1.svg)维的概率分布![](./img/699efcc802bec1295e7877a5ddfc24fe.svg)，我们可以通过![](./img/7b8b965ad4bca0e41ab51de7b31363a1.svg)个坐标轴上轮换采样，来得到新的样本。对于轮换到的任意一个坐标轴![](./img/1ba8aaab47179b3d3e24b0ccea9f4e30.svg)上的转移，马尔科夫链的状态转移概率为![](./img/11e9eff1a90f89e537bab7480e6505ad.svg)，即固定![](./img/a438673491daae8148eae77373b6a467.svg)个坐标轴，在某一个坐标轴上移动。



#### 采样过程

1. 输入平稳分布![](./img/699efcc802bec1295e7877a5ddfc24fe.svg)或者对应的所有特征的条件概率分布，设定状态转移次数阈值![](./img/6c773b2b7798e5713845e475d0c4b4c7.svg)，需要的样本个数![](./img/e501ae2ad90dc374410a774da21c5739.svg)
2. 随机初始化初始状态值![](./img/337b04db430379e5f401653d5b16c6d2.svg)
3. ![](./img/95c3d16036e76cced3cddf4266e5a5f0.svg)：
   1. 从条件概率分布![](./img/de85db6afcd7859bba664320bb1f5b87.svg)中采样得到样本![](./img/2b28a237fb720845e7a32f20b02b2020.svg)
   2. 从条件概率分布![](./img/ce8e07ac7309bd0097ed3d31ffa3cf8b.svg)中采样得到样本![](./img/d58ceb3c51627ecbce36146e5c43f47e.svg)
   3. ...
   4. 从条件概率分布![](./img/6bbc0d0894118de27a1bd7b81f930c99.svg)中采样得到样本![](./img/ea194e1a7985edd0a0d7f6f8fe98f9c3.svg)
   5. ...
   6. 从条件概率分布![](./img/5a3c12e2d9c4e2af660d88f63f9b02ae.svg)中采样得到样本![](./img/d126103cc0e7f1f46dbf5859708b498f.svg)

样本![](./img/1615da4033fedd172e49d0587a8d6e72.svg)即为我们需要的平稳分布对应的样本集。整个采样过程和Lasso回归的坐标轴下降法算法非常类似，只不过Lasso回归是固定![](./img/a438673491daae8148eae77373b6a467.svg)个特征，对某一个特征求极值。而Gibbs采样是固定![](./img/a438673491daae8148eae77373b6a467.svg)个特征在某一个特征采样。

同样的，轮换坐标轴不是必须的，我们可以随机选择某一个坐标轴进行状态转移，只不过常用的Gibbs采样的实现都是基于坐标轴轮换的。

## Source

[https://www.zhihu.com/question/20254139/answer/33572009](https://www.zhihu.com/question/20254139/answer/33572009)<br />[https://www.jiqizhixin.com/articles/061001](https://www.jiqizhixin.com/articles/061001)<br />[https://blog.csdn.net/anshuai_aw1/article/details/84792383](https://blog.csdn.net/anshuai_aw1/article/details/84792383)<br />[https://blog.csdn.net/anshuai_aw1/article/details/84875521](https://blog.csdn.net/anshuai_aw1/article/details/84875521)<br />[https://blog.csdn.net/anshuai_aw1/article/details/84883617](https://blog.csdn.net/anshuai_aw1/article/details/84883617)<br />[https://www.cnblogs.com/snowInPluto/p/5996269.html](https://www.cnblogs.com/snowInPluto/p/5996269.html)
