根据一系列独立样本![](./img/45f67a0837412dfbb7b9fd4311fc2670.svg)，以及模型![](./img/fe25ecc2f45953ed240f3fda68648e07.svg)，找到参数![](./img/2554a2bb846cffd697389e5dc8912759.svg)




## [最大似然估计(Maximum likelihood estimation)](https://www.jianshu.com/p/f1d3906e4a3e)

![](./img/696da1acc9597a53362f0d77c2b843b4.svg)

                                          ![](./img/92241444428513e539ecd6f0f529438d.svg)



### 计算过程

1. 写出似然函数；
2. 如果无法直接求导的话，对似然函数取对数；
3. 求导数；
4. 求解模型中参数的最优值。


### Example1：抽球

假设一个袋子装有白球与红球，比例未知，现在抽取10次（每次抽完都放回，保证事件独立性），假设抽到了7次白球和3次红球，在此数据样本条件下，可以采用最大似然估计法求解袋子中白球的比例（最大似然估计是一种“模型已定，参数未知”的方法）。当然，这种数据情况下很明显，白球的比例是70%，但如何通过理论的方法得到这个答案呢？一些复杂的条件下，是很难通过直观的方式获得答案的，这时候理论分析就尤为重要了，这也是学者们为何要提出最大似然估计的原因。我们可以定义从袋子中抽取白球和红球的概率如下：

![](./img/aa687da0086c1ea060a8838e24611319 (2).svg)为第一次采样，![](./img/8732099f74d777a67257cb2f04ead3d8 (2).svg)为第二次，![](./img/8fa14cdd754f91cc6554c9e71929cce7.svg)为模型，![](./img/2554a2bb846cffd697389e5dc8912759 (1).svg)模型参数

![](./img/0da1072bcf56027c23b738f86a412f82.svg)

我们定义似然![](./img/d20caec3b48a1eef164cb4ca81ba2587.svg)为：

![](./img/8ae93acd953ebd3c39f798766c362e63.svg)

两边取![](./img/5204fde33a37283fecb530a5edc7016a.svg)，取![](./img/5204fde33a37283fecb530a5edc7016a.svg)是为了将右边的乘号变为加号，方便求导，左边的通常称之为对数似然：

![](./img/c375ef61fa5754ef8335f5c9528f4435.svg)

平均似然对数：

![](./img/74b166dcc8a85453a19b039135c694a1.svg)

最大似然估计的过程就是找一个合适的![](./img/2554a2bb846cffd697389e5dc8912759 (2).svg)，使得平均对数似然的值为最大。因此，可以得到以下最大估计的公式：

![](./img/2d6e76f198fc2c9c914121f60b7a96bb.svg)

这里讨论的是2次采样的情况，拓展到多次采样的情况，![](./img/7b8b965ad4bca0e41ab51de7b31363a1 (7).svg)次采样最大似然估计公式：

![](./img/cb5dc8e8167b2942d94319c25c56710e.svg)

我们定义![](./img/69691c7bdcc3ce6d5d8a1361f22d04ac (1).svg)为模型（也就是之前公式中的![](./img/8fa14cdd754f91cc6554c9e71929cce7 (1).svg)），表示抽到白球的概率为![](./img/2554a2bb846cffd697389e5dc8912759 (3).svg)，而抽到红球的概率为![](./img/8d3fb7ae3ddd76958d236cd612291996.svg)，因此10次抽取抽到白球7次的概率可以表示为：

![](./img/8fa3760db744d74d462ffaa6412c3e34.svg)

将其描述为平均似然可得：

![](./img/be15514a41a55edd0133728a27710803.svg)

最大似然就是找到一个合适的![](./img/2554a2bb846cffd697389e5dc8912759 (4).svg)，获得最大的平均似然。我们可以对平均似然的公式对![](./img/2554a2bb846cffd697389e5dc8912759 (4).svg)求导，并令导数为0。

![](./img/8e4a8decea6422d8f0b0a3d1e93995eb.svg)

由此可得，当抽取白球的概率![](./img/2554a2bb846cffd697389e5dc8912759 (4).svg)为0.7时，最可能产生10次抽取抽到白球7次的事件。



### Example2：正态分布

假如有一组采样值![](./img/7916ef50651d77b60f89e843decb35d2.svg)，我们知道其服从正态分布，且标准差已知。当这个正态分布的期望为多少时，产生这个采样数据的概率为最大？

这个例子中正态分布就是模型![](./img/69691c7bdcc3ce6d5d8a1361f22d04ac (2).svg)，而期望就是前文提到的![](./img/2554a2bb846cffd697389e5dc8912759 (5).svg)，似然函数如下：

![](./img/b04d7defc2b61dda74b0a37fbaede536.svg)

正态分布的公式，当第一参数（期望）为0，第二参数（方差）为1时，分布为标准正态分布：

![](./img/8da080d035f7ddc1505e77b7518cbc05.svg)

所以似然值为：

![](./img/b8d4b7cb7ee4d8de21aa7e9270adbd7c.svg)

对上式求导可得：

![](./img/561256b870b7e8afcf0fcca36596c178.svg)


## [最大后验概率估计(Maximum a posteriori)](https://www.cnblogs.com/liliu/archive/2010/11/24/1886110.html)

假设有五个袋子，各袋中都有无限量的饼干(樱桃口味或柠檬口味)，已知五个袋子中两种口味的比例分别是

1.樱桃 100%  2.樱桃 75% + 柠檬 25%  3.樱桃 50% + 柠檬 50%  4.樱桃 25% + 柠檬 75%  5.柠檬 100%

如果只有上所述条件，从同一个袋子中连续拿到2个柠檬饼干，那这个袋子最有可能是上述五个的哪一个？

我们首先采用最大似然估计来解这个问题，写出似然函数。假设从袋子中能拿出柠檬饼干的概率为![](./img/83878c91171338902e0fe0fb97a8c47a (3).svg)(我们通过这个概率![](./img/83878c91171338902e0fe0fb97a8c47a (3).svg)来确定是从哪个袋子中拿出来的)，则似然函数可以写作

![](./img/c6e92aa3510c7f01a1746177f448bbd6.svg)

由于![](./img/83878c91171338902e0fe0fb97a8c47a (3).svg)的取值是一个离散值，即上面描述中的0,25%，50%，75%，1。我们只需要评估一下这五个值哪个值使得似然函数最大即可，得到为袋子5。这里便是最大似然估计的结果。上述最大似然估计有一个问题，就是没有考虑到模型本身的概率分布，下面我们扩展这个饼干的问题。

假设拿到袋子1或5的概率![](./img/b2f5ff47436671b6e533d8dc3614845d.svg)都是0.1，拿到2或4的概率都是0.2，拿到3的概率是0.4，那同样上述问题的答案呢？这个时候就变MAP了（若本例中拿到五个袋子概率都是0.2，即![](./img/2554a2bb846cffd697389e5dc8912759 (6).svg)为均匀分布时，MAL与MLE问题是等价；当![](./img/02cb092dd6953f1bc7c0f25ec5d100db.svg)不同时，即本例子，两问题不等价)。我们根据公式

![](./img/0d1a9ea807045c10310fe428a728bbb3.svg)

写出我们的

![](./img/9ed9b8d83a6b229e3be218a8d112e6a6.svg)

根据题意的描述可知，![](./img/83878c91171338902e0fe0fb97a8c47a (4).svg)的取值分别为0,25%, 50%, 75%, 1，![](./img/b2f5ff47436671b6e533d8dc3614845d (1).svg)的取值分别为0.1, 0.2, 0.4, 0.2, 0.1.分别计算出MAP函数的结果为：0, 0.0125, 0.125, 0.28125, 0.1.由上可知，通过MAP估计可得结果是从第四个袋子中取得的最高。


## EM算法(Expectation–maximization Algorithm)

EM算法是一种迭代算法，用于含有隐变量的概率模型参数的极大似然估计，或极大后验概率估计。EM算法的每次迭代由两步组成：E步：求期望(Expectation)；M步求极大(Maximization)。



### 算法步骤

输入：观测变量数据![](./img/57cec4137b614c87cb4e24a3d003a3e0 (5).svg)，隐变量数据![](./img/21c2e59531c8710156d34a3c30ac81d5 (1).svg)，联合分布![](./img/426f408db1cbeb843b2386f9678b1f86.svg)，条件概率分布![](./img/6dab5a7a4dd564825d5d429fd43482d7.svg)

输出：模型参数![](./img/2554a2bb846cffd697389e5dc8912759 (7).svg)

1. 选择参数的初值![](./img/9e16385381263051cc56f48b69bf81b6.svg)，开始迭代

2. E步：记![](./img/23bfcef55432741768ebc8b49e07fa61.svg)为第![](./img/865c0c0b4ab0e063e5caa3387c1a8741.svg)次迭代参数![](./img/2554a2bb846cffd697389e5dc8912759 (8).svg)的估计值，在第![](./img/865c0c0b4ab0e063e5caa3387c1a8741.svg)次迭代的E步，计算

![](./img/01d3ff3266863533fd6eb58153f20472.svg)

这里，![](./img/285a7983a5fd408e31adb93e94285d63.svg)是在给定观测数据![](./img/57cec4137b614c87cb4e24a3d003a3e0 (6).svg)和当前的参数估计![](./img/23bfcef55432741768ebc8b49e07fa61 (1).svg)下隐变量数据![](./img/21c2e59531c8710156d34a3c30ac81d5 (2).svg)的条件概率分布。

3. M步：求使![](./img/14bb6001aaec0f34ecbf250fce229b40.svg)极大化的![](./img/2554a2bb846cffd697389e5dc8912759 (9).svg)，确定第![](./img/15ab2d2b0b92c13f328635e5c4bdbe64.svg)次迭代的参数的估计值![](./img/e1ef049bc7854a2c5f3074842e6a293e.svg)

![](./img/1df52a6048dd4a921b9f17825679db25.svg)

4. 重复第（2）步和第（3）步，直至收敛


### 步骤说明

1. 参数的初值可以任意选择，但需注意EM算法对初值是敏感的

2. E步求![](./img/4f7aa5cdfa60b6de8d79425d849a72ed.svg)。![](./img/f09564c9ca56850d4cd6b3319e541aee.svg)函数式中![](./img/21c2e59531c8710156d34a3c30ac81d5 (3).svg)是未观测数据，![](./img/57cec4137b614c87cb4e24a3d003a3e0 (7).svg)是观测数据。注意![](./img/4f7aa5cdfa60b6de8d79425d849a72ed (1).svg)的第1个变元表示要极大化的参数，第2个变元表示参数的当前估计值。每次迭代实际在求![](./img/f09564c9ca56850d4cd6b3319e541aee.svg)函数及其极大。

3. M步求![](./img/4f7aa5cdfa60b6de8d79425d849a72ed.svg)极大化，得到![](./img/e1ef049bc7854a2c5f3074842e6a293e (1).svg)，完成一次迭代![](./img/d941b63fb9a99e167ef11ba6630473e8.svg)，每次迭代使似然函数增大或达到局部极值

4. 给出停止迭代的条件，一般是对较小的正数![](./img/b410b99c54b33afe231f77199fc4a845.svg)，满足![](./img/f0e5ca087181bb0689c7a39c770026ec.svg)或![](./img/29bf597384dc8f464342be9095ca3aba.svg)则停止迭代。


### Q函数

E步中的![](./img/4f7aa5cdfa60b6de8d79425d849a72ed.svg)是EM算法的核心，称为![](./img/f09564c9ca56850d4cd6b3319e541aee.svg)函数。完全数据的对数似然函数![](./img/da52cbda900dbd1fb044d676fa5a7a0b.svg)关于在给定观测数据![](./img/57cec4137b614c87cb4e24a3d003a3e0 (8).svg)和当前参数![](./img/23bfcef55432741768ebc8b49e07fa61 (2).svg)下对未观测数据![](./img/21c2e59531c8710156d34a3c30ac81d5 (4).svg)的条件概率分布![](./img/285a7983a5fd408e31adb93e94285d63 (1).svg)的期望称为![](./img/f09564c9ca56850d4cd6b3319e541aee.svg)函数

![](./img/01d3ff3266863533fd6eb58153f20472 (1).svg)


### Example

​	假设有3枚硬币，分别记作![](./img/ce04be1226e56f48da55b6c130d45b94.svg)。这些硬币正面出现的概率分别是![](./img/4d529df141f50f844df2c201e9549cc0.svg)。进行如下掷硬币试验：先掷硬币![](./img/7fc56270e7a70fa81a5935b72eacbe29 (4).svg)，根据其结果选出硬币![](./img/9d5ed678fe57bcca610140957afab571 (2).svg)或硬币![](./img/0d61f8370cad1d412f80b84d143e1257 (2).svg)，正面选硬币![](./img/9d5ed678fe57bcca610140957afab571 (2).svg)，反面选![](./img/0d61f8370cad1d412f80b84d143e1257 (2).svg)；然后掷选出的硬币，掷硬币的结果，出现正面记作1，出现反面记作0；独立地重复n次试验（本例n = 10)，结果如下

| n | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 | 10 |
| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |
| 结果 | 1 | 1 | 0 | 1 | 0 | 0 | 1 | 0 | 1 | 1 |

只能观测到掷硬币的结果，不能观测掷硬币过程。问如何估计三枚硬币正面出现的概率，即三硬币模型的参数。



​	三硬币模型可以写作：

![](./img/b35d942b1ffb8fe35b9e7f4048647d91.svg)

                                                    ![](./img/254496b0e52d8aa0acdb554f08a77e55.svg)

这里，随机变量![](./img/415290769594460e2e485922904f345d (2).svg)是观测变量，表示一次试验观测的结果是1或0；随机变量![](./img/fbade9e36a3f36d3d676c1b808451dd7.svg)是隐变量，表示未观测到的掷硬币![](./img/7fc56270e7a70fa81a5935b72eacbe29 (5).svg)的结果；![](./img/9562e6ae0b5451f0214fc2908e114d02.svg)是模型参数。这一模型是以上数据的生成模型。注意，随机变量![](./img/415290769594460e2e485922904f345d (3).svg)的数据可以观测，随机变量![](./img/fbade9e36a3f36d3d676c1b808451dd7.svg)的数据不可观测。将观测数据表示为![](./img/f44caefca8612223c7b79ea4088c7b91.svg)，未观测数据表示为![](./img/3d34d8970b6fc50b197af3ff58f1c3f4.svg)，则观测数据的似然函数为

![](./img/9b72961c91cfb1503552281ad13515ff.svg)

即

![](./img/029d73c3069d048b7b10067037237319.svg)

考虑求模型参数![](./img/9562e6ae0b5451f0214fc2908e114d02 (1).svg)的极大似然估计，即

![](./img/64c5fc73374e8711e8e8e26a8fabfa5e.svg)

这个问题没有解析解，只有通过迭代的方法求解。EM算法就是可以用于求解这个问题的一种迭代算法。

EM算法首先选取参数的初值，记作![](./img/74a848a0fcc0f918da72f92f6156c0fd.svg)，然后通过下面的迭代计算参数的估计值，直到收敛为止。第![](./img/865c0c0b4ab0e063e5caa3387c1a8741 (1).svg)次迭代参数的估计值为![](./img/b3cd8821358c1f95393d45eddc1641f4.svg)。EM算法的第![](./img/15ab2d2b0b92c13f328635e5c4bdbe64 (1).svg)次迭代如下

E步：计算在模型参数![](./img/2ff7e5a2e18174e83a52b940ea86ac4c.svg)下观测数据![](./img/8d62e469fb30ed435a668eb5c035b1f6.svg)来自掷硬币![](./img/9d5ed678fe57bcca610140957afab571 (3).svg)的概率

![](./img/70e2de853b3d26590db57360b62b3143.svg)

M步：计算模型参数的新估计值

![](./img/9190c937e2898b34dfbdd87e8f8da3b6.svg)

进行数字计算。假设模型参数的初值取为

![](./img/fae6ad5546170203a39047d4fa111a54.svg)

由E步得到对于![](./img/dc8a4e81aafc37e37f34d2ed59cf110d.svg)与![](./img/a2912d534181268b5665515673698dad.svg)均有![](./img/fca873df1fe3a61a4e84d1cbe5ca4114.svg)

由M步迭代公式得到

![](./img/4f765eef0c80d9b70c0c28cdf6200799.svg)

再回到E步，得到![](./img/8defeac4f75387e12b6871c2d28c9984.svg)

继续M步，得

![](./img/8defeac4f75387e12b6871c2d28c9984 (1).svg)

收敛，于是得到模型的参数![](./img/2554a2bb846cffd697389e5dc8912759 (10).svg)的极大似然估计

![](./img/842819ef539013dbbe86bd2d32f0d9e1.svg)

如果取初值![](./img/ee7da04ed679edcc2fc4791def4c2665.svg)，那么得到的模型参数的极大似然估计是![](./img/7677bbf3efe9b6198325bf023bd7927f.svg)。这就是说，EM算法与初值的选取有关，选择不同的初值可能得到不同的参数估计值。



### [Code实现](https://github.com/fengdu78/lihang-code/blob/master/code/%E7%AC%AC9%E7%AB%A0%20EM%E7%AE%97%E6%B3%95%E5%8F%8A%E5%85%B6%E6%8E%A8%E5%B9%BF%28EM%29/em.ipynb)
E-step![7528abf42d0675c3ecce96e38c6a4102](./img/7528abf42d0675c3ecce96e38c6a4102.svg)

```python
import numpy as np
import math

pro_A, pro_B, por_C = 0.5, 0.5, 0.5

def pmf(i, pro_A, pro_B, por_C):
    pro_1 = pro_A * math.pow(pro_B, data[i]) * math.pow((1-pro_B), 1-data[i])
    pro_2 = pro_A * math.pow(pro_C, data[i]) * math.pow((1-pro_C), 1-data[i])
    return pro_1 / (pro_1 + pro_2)
```

M-step：![](./img/5069f50bdb550fe85bc3f78908b8f97a.svg)

```python
class EM:
    def __init__(self, prob):
        self.pro_A, self.pro_B, self.pro_C = prob
        
    # e_step
    def pmf(self, i):
        pro_1 = self.pro_A * math.pow(self.pro_B, data[i]) * math.pow((1-self.pro_B), 1-data[i])
        pro_2 = (1 - self.pro_A) * math.pow(self.pro_C, data[i]) * math.pow((1-self.pro_C), 1-data[i])
        return pro_1 / (pro_1 + pro_2)
    
    # m_step
    def fit(self, data):
        count = len(data)
        print('init prob:{}, {}, {}'.format(self.pro_A, self.pro_B, self.pro_C))
        for d in range(count):
            _ = yield
            _pmf = [self.pmf(k) for k in range(count)]
            pro_A = 1/ count * sum(_pmf)
            pro_B = sum([_pmf[k]*data[k] for k in range(count)]) / sum([_pmf[k] for k in range(count)])
            pro_C = sum([(1-_pmf[k])*data[k] for k in range(count)]) / sum([(1-_pmf[k]) for k in range(count)])
            print('{}/{}  pro_a:{:.3f}, pro_b:{:.3f}, pro_c:{:.3f}'.format(d+1, count, pro_A, pro_B, pro_C))
            self.pro_A = pro_A
            self.pro_B = pro_B
            self.pro_C = pro_C
```

测试

```python
data=[1,1,0,1,0,0,1,0,1,1]

em = EM(prob=[0.5, 0.5, 0.5])
f = em.fit(data)
next(f)

# 第一次迭代
f.send(1)

# 第二次
f.send(2)

em = EM(prob=[0.4, 0.6, 0.7])
f2 = em.fit(data)
next(f2)

f2.send(1)

f2.send(2)
```
