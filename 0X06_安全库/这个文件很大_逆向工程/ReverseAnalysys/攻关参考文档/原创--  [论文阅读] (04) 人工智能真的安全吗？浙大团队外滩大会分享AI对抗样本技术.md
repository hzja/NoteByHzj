# 原创
：  [论文阅读] (04) 人工智能真的安全吗？浙大团队外滩大会分享AI对抗样本技术

# [论文阅读] (04) 人工智能真的安全吗？浙大团队外滩大会分享AI对抗样本技术

> 
《秀璋带你读论文》系列主要是督促自己阅读优秀论文及听取学术讲座，并分享给大家，希望您喜欢。由于作者的英文水平和学术能力不高，需要不断提升，所以还请大家批评指正，非常欢迎大家给我留言评论，学术路上期待与您前行，加油~


AI技术蓬勃发展，无论是金融服务、线下生活、还是医疗健康都有AI的影子，那保护好这些AI系统的安全是非常必要也是非常重要的。目前，AI安全是一个非常新的领域，是学界、业界都共同关注的热门话题，本论坛将邀请AI安全方面的专家，分享交流智能时代的功守道，推动和引领业界在AI安全领域的发展。

本次论坛的题目为“AI安全-智能时代的攻守道”，其中武汉大学王骞院长分享了语音系统的对抗性攻防，浙江大学纪守领研究员分享了NLP中的安全，浙江大学秦湛研究员分享了深度学习中的数据安全新型攻防，来自蚂蚁集团的宗志远老师分享了AI安全对抗防御体系，任奎院长分享了AI安全白皮书。<font color="red">**本文主要讲解NLP中的AI安全和白皮书相关知识，希望对您有所帮助。这些大佬是真的值得我们去学习，献上小弟的膝盖~fighting！**</font>

<mark>**PS：顺便问一句，你们喜欢这种会议讲座方式的分享吗？<br/> 担心效果不好，如果不好我就不分享和总结类似的会议知识了，欢迎评论给我留言。**</mark>

#### 文章目录

**前文推荐：**<br/> [[秀璋带你读论文] (01) 拿什么来拯救我的拖延症？初学者如何提升编程兴趣及LATEX入门详解](https://blog.csdn.net/Eastmount/article/details/106886194)<br/> [[娜璋带你读论文] (02) SP2019-Neural Cleanse: Identifying and Mitigating Backdoor Attacks in DNN](https://blog.csdn.net/Eastmount/article/details/107283275)<br/> [[娜璋带你读论文] (03) 清华张超老师 - GreyOne: Discover Vulnerabilities with Data Flow Sensitive Fuzzing](https://blog.csdn.net/Eastmount/article/details/107825286)<br/> [[娜璋带你读论文] (04) 人工智能真的安全吗？浙大团队外滩大会分享AI对抗样本技术]()<br/> [基于机器学习的恶意代码检测技术详解](https://blog.csdn.net/Eastmount/article/details/107420755)

---


## 一.AI安全白皮书

随着人工智能日益发展，自动驾驶、人脸识别、语音识别等技术被广泛应用，同时带来的是严峻的AI安全问题。常见的安全问题包括：

当今的AI安全非常重视四种性能，包括：

针对这四种性能的AI攻击层出不穷，比如推断攻击、对抗样本、投毒攻击、模型窃取等。

<mark>因此，任奎院长带来了《AI安全白皮书》的分享。</mark>

浙江大学和蚂蚁集团合作，他们调研了近年来发表在安全、人工智能等领域国际会议与期刊上的300余篇攻防技术研究成果，聚焦模型、数据、承载三个维度的安全威胁与挑战，梳理了AI安全的攻击与防御技术。根据真实场景中AI技术面临的安全问题，总结提出AI应用系统的一站式安全解决方案（AISDL），并共同推出了《AI安全白皮书》。整个框架如下图所示：

他们经过梳理，将AI技术面临的威胁归为三大类，分别是：

<mark>**在介绍三种安全问题之前，作者首先给大家普及下什么是对抗样本？**</mark><br/> 对抗样本指的是一个经过微小调整就可以让机器学习算法输出错误结果的输入样本。在图像识别中，可以理解为原来被一个卷积神经网络（CNN）分类为一个类（比如“熊猫”）的图片，经过非常细微甚至人眼无法察觉的改动后，突然被误分成另一个类（比如“长臂猿”）。再比如无人驾驶的模型如果被攻击，Stop标志可能被汽车识别为直行、转弯。

<mark>**对抗样本的经典流程如下图所示——GU等人提出的BadNets。**</mark><br/> 它通过恶意（poisoning）训练数据集来注入后门，具体如下：

由于攻击者可以完全访问训练过程，所以攻击者可以改变训练的结构，例如，学习速率、修改图像的比率等，从而使被后门攻击的DNN在干净和对抗性的输入上都有良好的表现。BadNets显示了超过99%的攻击成功率（对抗性输入被错误分类的百分比），而且不影响MNIST中的模型性能。下图右下角的触发器（后门）导致了神经网络训练学习错误地类别，将Label5和Label7预测为Label4。

> 
<font color="red">**PS：在下一篇文章中我们会详细讲解AI数据安全和AI语音安全论文，这篇文章主要针对NLP文本的对抗样本分享，望您喜欢！**</font>


---


### 1.AI模型安全问题

**(1) 模型完整性威胁=&gt;数据投毒攻击**<br/> 攻击者在正常训练集中加入少量的毒化数据，破坏模型完整性，操纵AI判断结果。模型偏移会使模型对好坏输入的分类发生偏移，降低模型的准确率。同时，后门攻击不影响模型的正常使用，只在攻击者设定的特殊场景使模型出现错误。

**(2) 模型鲁棒性威胁=&gt;对抗性样本攻击**<br/> 攻击者在模型测试阶段，向输入样本加入对抗扰动，破坏模型鲁棒性，操纵AI判断结果。

深度学习模型通常都存在模型鲁棒性缺乏的问题，一方面由于**环境因素多变**，包括AI模型在真实使用过程中表现不够稳定，受光照强度、视角角度距离、图像仿射变换、图像分辨率等影响，从而导致训练数据难以覆盖现实场景的全部情况。另一方面模型的**可解释性不足**，深度学习模型是一个黑箱，模型参数数量巨大、结构复杂，没有恶意攻击的情况下，可能出现预期之外的安全隐患，阻碍AI技术在医疗、交通等安全敏感性高的场景下使用。

任老师他们团队的相关工作包括分布式对抗攻击和面向三维点云的对抗攻击等。

---


### 2.AI数据安全问题

AI数据安全简单来说就是通过构造特定数据集，结合模型预测的结果来获取深度学习模型的参数或数据。如下图所示，通过模型逆向攻击重建图像，深度学习模型泄露了训练数据中的敏感信息。

AI数据安全包括模型参数泄露和训练数据泄露，具体如下图所示。模型参数泄露攻击方法包括方程求解攻击、基于Meta-model的模型窃取、模型替代攻击；训练数据泄露包括输出向量泄露和梯度更新泄露，方法包括成员推断攻击、模型逆向攻击、分布式模型梯度攻击。

任老师他们做的相关工作包括：

---


### 3.AI承载系统安全问题

**(1) 硬件设备安全问题**

**(2) 系统与软件安全问题**

---


### 4.防御方法

**(1) 模型安全性增强**<br/> 面向模型完整性威胁的防御

面向模型鲁棒性威胁的防御

<img alt="在这里插入图片描述" height="300" src="https://img-blog.csdnimg.cn/20201018140307220.png#pic_center" width="600"/><br/> <br/>

**(2) 模型安全性增强**

**(3) 系统安全性防御**<br/> 硬件安全保护

软件安全保护

最后他们和蚂蚁集团提出一种AI模型安全开发声生命周期——AI SDL，分阶段引入安全和隐私保护原则，实现有安全保证的AI开发过程。

**最后总结：**

---


## 二.从NLP视角看机器学习模型安全

在图像领域和语音领域都存在很多对抗样本攻击（Adversarial Attack），比如一段“How are you”的语音增加噪声被识别成“Open the door”，再如智能音响中增加噪声发起语音攻击等等。

**那么，在文本领域也存在对抗样本攻击吗？自然语言处理（Natural Language Processing，NLP）的机器学习服务（MLaaS）是否也容易受到对抗样本攻击呢？**

首先，给大家普及下自然语言处理。常见的应用包括：

本篇博客主要介绍针对情感分类的对抗文本，所以介绍下情感分类的基础。深度学习在处理文本时，NLP通常要将文本进行分词、数据清洗、词频计算，然后转换成对应的词向量或TF-IDF矩阵，再进行相似度计算或文本分类，当某种情感（积极\消极）的特征词出现较多，则预测为该类情感。那么，能否让深度学习模型总是预测错误呢？

NLP的对抗样本攻击和图像或语音的对抗样本存在很大的差异性，具体区别如下：

由于图片和文本数据内在的不同，用于图像的对抗攻击方法无法直接应用与文本数据上。首先，图像数据（例如像素值）是连续的，但文本数据是离散的。其次，仅仅对像素值进行微小的改变就可以造成图像数据的扰动，而且这种扰动是很难被人眼察觉的。但是对于文本的对抗攻击中，小的扰动很容易被察觉，但人类同样能「猜出」本来表达的意义。因此 NLP 模型需要对可辨识的特征鲁棒，而不像视觉只需要对「不太重要」的特征鲁棒。

<mark>**DeepWordBug**</mark><br/> 下图是DeepWordBug的深度网络攻击示例（选自 arXiv：1902.07285），展示了文本对抗样本的基本流程。正常深度学习预测的情感为positive，但修改某些关键词后（place<br/> heart），它的情感分类结果为negative。

与图像领域一样，有进攻就会有防御，目前也有很多研究尝试构建更鲁棒的自然语言处理模型。推荐大家阅读CMU的一篇对抗性拼写错误论文（arXiv：1905.11268）中，研究者通过移除、添加或调序单词内部的字符，以构建更稳健的文本分类模型。这些增减或调序都是一种扰动，就像人类也很可能出现这些笔误一样。通过这些扰动，模型能学会如何处理错别字，从而不至于对分类结果产生影响。

下面开始介绍纪老师他们开展的工作。

---


## 三.对抗文本TextBugger

<mark>**TextBugger: Generating Adversarial Text Against Real-world Applications**</mark><br/> 这篇论文发表在NDSS 2019，主要提出了生成文本对抗样本的模型TextBugger，用于生成文本对抗样本。其优势如下：

原文地址：

### 1.论文贡献

文本对抗在应用中越来越重要，而图像对抗中的方法不能直接用于文本。之前的对抗样本生成模型有着下述的缺点：

本文提出了一个新框架TextBugger，可生成黑箱和白箱场景下的保持样本原意的对抗样本。在白箱场景下，可以通过计算雅各比矩阵来找到句子中的关键词；在黑箱场景下，可以先找到最重要的句子，再使用一个评分函数来寻找句子中的关键词。在真实世界的分类器中使用了对抗样本，取得了不错的效果。具体贡献包括：

具体实验环境如下图所示，数据集为IMDB和Rotten Tomatoes Movie Reviews数据集，都是对影评数据进行情感分析的数据集。目标模型为：

基线算法为：

PS：该部分参考“人帅也要多读书”老师的理解。

> 
**对抗攻击分类**<br/> 对抗攻击的分类有很多种，从攻击环境来说，可以分为黑盒攻击、白盒攻击或灰盒攻击.

从攻击的目的来说，可以分为有目标攻击和无目标攻击。



---


### 2.白盒攻击

白盒攻击：通过雅各比矩阵找到最重要的单词，再生成五种类型的bug，根据置信度找到最佳的那一个。TextBugger整个框架如下图所示。

白盒攻击通过雅可比矩阵找到最重要的单词，算法流程如下：

作者发现在一些词嵌入模型中（如word2vec），“worst”和“better”等语义相反的词在文本中具有高度的句法相似性，因此“better”被认为是“worst”的最近邻。以上显然是不合理的，很容易被人察觉。因此使用了**语义保留技术**，即将该单词替换为上下文感知的单词向量空间中的top-k近邻。使用斯坦福提供的预先训练好的 **GloVe模型** 进行单词嵌入，并设置topk为5，从而保证邻居在语义上与原来的邻居相似。

TextBugger提出了五种对抗样本生成方法，如下图所示：

将使用候选词生成的对抗样本输入模型，得到对应类别的置信度，选取让置信度下降最大的词。如果替换掉单词后的对抗样本与原样本的语义相似度大于阈值，对抗样本生成成功。如果未大于阈值，则选取下一个单词进行修改。

---


### 3.黑盒攻击

在黑盒场景下，没有梯度的指示，所以首先找最重要的句子，然后通过打分函数找到最重要的单词。具体攻击分为三个步骤：

---


### 4.实验评估

主要使用编辑距离、杰卡德相似系数、欧氏距离和语义相似度进行评估。下表展示了论文中方法在白箱环境和黑箱环境下的表现，可以看出与之前的方法相比有很大的优势。

下图展示了对抗文本中的重要单词。根据算法攻击单词的频率，就可以知道对于某一类别影响最大的单词，比如“bad”, “awful”, “stupid”, “worst”, “terrible”这些词就是消极类别中的关键词。

下图是论文算法产生的对抗样本实例，通过简单的单词级别的攻击对分类关键词进行了处理，进而达到了攻击的效果，可以看到目标类别和攻击后的类别差别很大。具体修改比如：

实验数据表明，文档的长度对于攻击成功率影响不大，但更长的文本对于错误分类的置信度会下降。文档长度越长，攻击所需时长也就更长，这在直观上较好理解。

<mark>**总结**</mark><br/> 本论文算法的特点总结如下：首先，算法同时使用了字母级别和单词级别的扰动；其次，论文评估了算法的效率；最后，论文使用算法在众多在线平台上进行了实验，证明了算法的普适性和鲁棒性。同时，现存的防御方法只集中在的图像领域，而在文本领域比较少，对抗训练的方法也只应用于提高分类器的准确性而非防御对抗样本。

---


## 四.中文对抗文本

目前看到的很多论文都是介绍英文的对抗文本攻击，但是中文同样存在，并且由于中文语义和分词，其攻击和防御难度更大，接下来纪老师他们分享了正在开展的一个工作。**但由于这部分介绍很快，这里仅放出当时拍摄的相关PPT，请大家下来进行研究，我感觉word2vec语义知识能做一些事情。**

随着对抗样本发展，火星文字越来越多，它们一定程度上能够绕过我们新闻平台、社交网络、情感模型，比如“微信”修改为“薇心”、“玥发叁仟”等词语。中文的对抗文本某种程度上难度更高，那么怎么解决呢？

纪老师他们团队提出了CTbugger（Adversarial Chinese Text），其框架如下图所示，通过对深度学习模型进行恶意文本攻击从而生成对应的中文对抗文本。

另一块工作是TextShield，其框架如下图所示：

---


## 五.总结

最后给出总结的相关文献，大家可以去了解学习。真的非常感谢所有老师的分享，学到很多知识，也意识到自己的不足。我自己也需要思考一些问题：

<font color="red">**学术或许是需要天赋的，这些大佬真值得我们学习，顶会论文要坚持看，科研实验不能间断。同时自己会继续努力，争取靠后天努力来弥补这些鸿沟，更重要的是享受这种奋斗的过程，加油！最后感谢老师给予的机会，虽然自己的技术和科研都很菜，安全也非常难，但还是得苦心智，劳筋骨，饿体肤。感恩亲人的支持，也享受这个奋斗的过程。月是故乡圆，佳节倍思亲。**</font>

最后给出“山竹小果”老师归纳的对抗样本相关论文：<br/> **(1) 文本攻击与防御的论文概述**

**(2) 黑盒攻击**

**(3) 白盒攻击**

**(4) 同时探讨黑盒和白盒攻击**

**(5) 对抗防御**

**(6) 对文本攻击和防御研究提出新的评价方法**

---


**参考文献：**<br/> 感谢这些大佬和老师们的分享和总结，秀璋受益匪浅，再次感激。<br/> [1] [AI安全 - 智能时代的攻守道](https://www.inclusionconf.com/schedule/forums/7000)<br/> [2] [https://arxiv.org/abs/1812.05271](https://arxiv.org/abs/1812.05271)<br/> [3] [(强烈推荐)NLP中的对抗样本 - 山竹小果](https://www.cnblogs.com/shona/p/11298215.html)<br/> [4] [TextBugger：针对真实应用生成对抗文本 - 人帅也要多读书](https://zhuanlan.zhihu.com/p/114885650)<br/> [5] [论文阅读 | TextBugger: Generating Adversarial Text Against Real-world Applications](https://www.cnblogs.com/shona/p/11809257.html)<br/> [6] [对抗攻击概念介绍 - 机器学习安全小白](https://www.cnblogs.com/tangweijqxx/p/10614071.html)<br/> [7] Li J, Ji S, Du T, et al. TextBugger: Generating Adversarial Text Against Real-world Applications[J]. arXiv: Cryptography and Security, 2018.

(By:Eastmount 2020-10-18 晚上10点 http://blog.csdn.net/eastmount/ )
