# 集群

## 主从模式

首先slave连接到master，master会启动一个线程，并把最新的**RDB**文件发送给slave的，slave拿到之后做的第一件事情就是写进本地的磁盘，然后加载进内存。这个过程可能有新的写请求，master会通过**AOF**增量日志的方式将新数据同步给slave，就像**MySQL**的**Binlog**一样，把日志增量同步给slave服务



## 哨兵模式

哨兵可以同时监视多个主从服务器，并且在被监视的master下线时，自动将某个slave提升为master，然后由新的master继续接收命令，基于主从方案的缺点还是很明显的，无法保证高可用，master挂了，整个服务挂了。而哨兵可以同时监视多个主从服务器，假如master挂了，可以Raft协议做选举将某个slave提升为master，然后由新的master继续接收命令



## 集群模式

依靠哨兵可以实现Redis的高可用，如果还想在支容纳海量的数据，那就需要Redis集群

1. 无中心架构（不存在哪个节点影响性能瓶颈）
2. 数据按照哈希槽（hash slot）分布存储在多个节点， 一个Redis集群包含16384个哈希槽
3. 集群中的每个节点负责维护一部分哈希槽。 比如一个集群可以有三个节点：

- 节点 A 负责处理 0 号至 5500 号哈希槽
- 节点 B 负责处理 5501 号至 11000 号哈希槽
- 节点 C 负责处理 11001 号至 16384 号哈希槽

4. 如果节点A向节点B发送ping消息，节点B没有在规定的时间内响应，那么节点A会标记节点B为疑似下线状态，同时把B的状态通过消息的方式发送给其他节点，如果超过半数以上的节点都标记B为疑似下线状态，B就会被标踢出集群，然后将会发生故障转移，优先从复制数据较多的从节点选择一个成为主节点，整个过程和哨兵非常类似，都是基于Raft协议做选举。



# 主从模式

## 主从原理

![v2-72d216bf617a3b037f3823f9a57d6bf6_720w](D:\Note\python\数据库交互\图片\v2-72d216bf617a3b037f3823f9a57d6bf6_720w.jpg)

首先slave连接到master，master会启动一个线程，并把最新的**RDB**文件发送给slave的，slave拿到之后做的第一件事情就是写进本地的磁盘，然后加载进内存。这个过程可能有新的写请求，master会通过**AOF**增量日志的方式将新数据同步给slave，就像**MySQL**的**Binlog**一样，把日志增量同步给slave服务



## 部署

### master

~~~ python
# 修改redis.conf
vim redis.conf
bind 0.0.0.0
port 6379
logfile "6379.log"
dbfilename "dump-6379.rdb"
daemonize yes
rdbcompression yes

# 启动服务
[root@redis1 bin]# ./redis-server redis.conf
~~~



### slave

~~~ python
# 修改redis.conf
vim redis.conf
bind 0.0.0.0
port 6380
logfile "6380.log"
dbfilename "dump-6380.rdb"
daemonize yes
rdbcompression yes
slaveof 192.168.0.12 6379 

# 启动服务
[root@redis2 bin]# ./redis-server redis.conf

# 从机连接6380的客户端，查看同步数据
[root@redis2 bin]# ./redis-cli -p 6380
127.0.0.1:6380> get nu
"4"
~~~



> **注意：**
>
> 1. 主机一旦发生增删改操作，那么从机会将数据同步到从机中
> 2. 从机不能执行写操作，会报错



~~~ python
[root@redis2 bin]# ./redis-cli -p 6380
127.0.0.1:6380> set test 123
(error) READONLY You can't write against a read only slave.
~~~



# 哨兵模式

哨兵可以同时监视多个主从服务器，并且在被监视的master下线时，自动将某个slave提升为master，然后由新的master继续接收命令，基于主从方案的缺点还是很明显的，无法保证高可用，master挂了，整个服务挂了。而哨兵可以同时监视多个主从服务器，假如master挂了，可以自动将某个slave提升为master，然后由新的master继续接收命令



## 原理

哨兵可以同时监视多个主从服务器，并且在被监视的master下线时，自动将某个slave提升为master，然后由新的master继续接收命令。整个过程如下：

1. 初始化sentinel，将普通的redis代码替换成sentinel专用代码
2. 初始化masters字典和服务器信息，服务器信息主要保存ip:port，并记录实例的地址和ID
3. 创建和master的两个连接，命令连接和订阅连接，并且订阅sentinel:hello频道
4. 每隔10秒向master发送info命令，获取master和它下面所有slave的当前信息
5. 当发现master有新的slave之后，sentinel和新的slave同样建立两个连接，同时每个10秒发送info命令，更新master信息
6. sentinel每隔1秒向所有服务器发送ping命令，如果某台服务器在配置的响应时间内连续返回无效回复，将会被标记为下线状态
7. 选举出领头sentinel，领头sentinel需要半数以上的sentinel同意
8. 领头sentinel从已下线的的master所有slave中挑选一个，将其转换为master
9. 让所有的slave改为从新的master复制数据
10. 将原来的master设置为新的master的从服务器，当原来master重新回复连接时，就变成了新master的从服务器

sentinel会每隔1秒向所有实例（包括主从服务器和其他sentinel）发送ping命令，并且根据回复判断是否已经下线，这种方式叫做主观下线。当判断为主观下线时，就会向其他监视的sentinel询问，如果超过半数的投票认为已经是下线状态，则会标记为客观下线状态，同时触发故障转移。



**正常状态**

![1586503882871-b25fffbc-6749-41d6-9017-49cb358e4e98](D:\Note\python\数据库交互\图片\1586503882871-b25fffbc-6749-41d6-9017-49cb358e4e98.png)



**server1 掉线后**

![1586503883426-4ef69993-8ea3-428a-9735-4f2e915b11e9](D:\Note\python\数据库交互\图片\1586503883426-4ef69993-8ea3-428a-9735-4f2e915b11e9.png)



**将server2升级为master**

![1586503884052-72b50a04-dc44-4690-bd3b-44e9d04f1d5a](D:\Note\python\数据库交互\图片\1586503884052-72b50a04-dc44-4690-bd3b-44e9d04f1d5a.png)



## 部署

~~~ tex
192.168.6.201 server1  master
192.168.6.202 server2  slave1
192.168.6.203 server3  slave2
192.168.6.204 server4  slave3
~~~



### master

~~~ shell
[root@master ~]# vim /usr/local/redis/bin/redis.conf 
bind 127.0.0.1 192.168.6.201            都是本机IP
daemonize yes
port 6379
timeout 30
logfile "/usr/local/redis/redis.log"
dir /usr/local/redis
appendonly yes
appendfilename "appendonly.aof"
appendfsync always
~~~



### slave

~~~ shell
[root@server1 ~]# vim /usr/local/redis/bin/redis.conf 
bind 127.0.0.1 192.168.6.202
port 6379
timeout 30
daemonize yes
logfile "/usr/local/redis/redis.log"
dir /usr/local/redis
slaveof 192.168.6.201 6379                  指向master
slave-serve-stale-data no
appendonly yes
appendfilename "appendonly.aof"
appendfsync always

[root@server2 ]# cp /root/redis-stable/sentinel.conf/   /usr/local/redis/bin/
[root@server2 ]# vim /usr/local/redis/bin/sentinel.conf
protected-mode no
port 26379
dir /usr/local/redis
sentinel monitor mymaster 192.168.6.201 6379 2  (哨兵进程总数/2+1)
sentinel down-after-milliseconds mymaster 5000
sentinel parallel-syncs mymaster 3
sentinel failover-timeout mymaster 60000

[root@server2 ]# ./redis-server redis.conf
启动哨兵
./redis-server sentinel.conf --sentinel &                  启动后会出现26379端口
或
./redis-sentinel /usr/local/redis/bin/sentinel.conf
~~~



## 测试

### master

~~~ shell
[root@server1 bin]# ./redis-cli 
127.0.0.1:6379> info replication
# Replication
role:master
connected_slaves:3
slave0:ip=192.168.122.202,port=6379,state=online,offset=26511,lag=0
slave1:ip=192.168.122.203,port=6379,state=online,offset=26511,lag=0
slave2:ip=192.168.122.204,port=6379,state=online,offset=26366,lag=0
master_repl_offset:26511
repl_backlog_active:1
repl_backlog_size:1048576
repl_backlog_first_byte_offset:2
repl_backlog_histlen:26510
~~~



### slave查询

~~~ shell
[root@server2 bin]# ./redis-cli 
127.0.0.1:6379> info replication
role:slave
master_host:192.168.122.201
master_port:6379
master_link_status:up
master_last_io_seconds_ago:0
master_sync_in_progress:0
slave_repl_offset:38630
slave_priority:100
slave_read_only:1
connected_slaves:0
master_repl_offset:0
repl_backlog_active:
repl_backlog_size:1048576
repl_backlog_first_byte_offset:0
repl_backlog_histlen:0
~~~

kill -9 redis.pid

剩下三台会选一个master出来，不支持王者归来



# 集群模式

![1586504600354-818aad51-2005-4ac0-aec9-f5e7d2f34f6a](D:\Note\python\数据库交互\图片\1586504600354-818aad51-2005-4ac0-aec9-f5e7d2f34f6a.png)

依靠哨兵可以实现Redis的高可用，如果还想在支容纳海量的数据，那就需要Redis集群

1. 无中心架构（不存在哪个节点影响性能瓶颈）
2. 数据按照哈希槽（hash slot）分布存储在多个节点， 一个Redis集群包含16384个哈希槽
3. 集群中的每个节点负责维护一部分哈希槽。 比如一个集群可以有三个节点：

- 节点 A 负责处理 0 号至 5500 号哈希槽
- 节点 B 负责处理 5501 号至 11000 号哈希槽
- 节点 C 负责处理 11001 号至 16384 号哈希槽

4. 如果节点A向节点B发送ping消息，节点B没有在规定的时间内响应，那么节点A会标记节点B为疑似下线状态，同时把B的状态通过消息的方式发送给其他节点，如果超过半数以上的节点都标记B为疑似下线状态，B就会被标踢出集群，然后将会发生故障转移，优先从复制数据较多的从节点选择一个成为主节