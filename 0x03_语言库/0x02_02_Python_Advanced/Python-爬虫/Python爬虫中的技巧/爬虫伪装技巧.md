Python 爬虫
<a name="QdhUE"></a>
### 1、浏览器伪装
因为网站服务器能够很轻易的识别出访问的来源浏览器，以`requests`请求为例，默认`headers`头数据中没有浏览器信息，在与浏览器交互时简直就是“裸奔”，所以可以加入“User-Agent”信息伪装成真实浏览器，代码如下：
```python
import requests 
headers={'User-Agent':'Mozilla/5.0 (Windows NT 6.1; Win64; x64; rv:88.0) Gecko/20100101 Firefox/88.0'}  #模拟成火狐浏览器
response = requests.get("http://www.baidu.com",headers=headers)  #模拟请求url
```
<a name="PqyMc"></a>
### 2、访问地址伪装
访问地址指的是`headers`头部中的`reffer`信息，那么它有什么作用呢？举个例子解释一下：<br />在https://bj.meituan.com/里有一个https://waimai.meituan.com/链接，那么点击这个https://waimai.meituan.com/，它的`headers`信息里就有：Referer=https://bj.meituan.com/ <br />那么可以利用这个来防止盗链，比如只允许自己的网站访问自己的图片服务器<br />可以加入“reffer”信息伪装访问地址，代码如下：
```python
import requests 
headers={
    'User-Agent':'Mozilla/5.0 (Windows NT 6.1; Win64; x64; rv:88.0) Gecko/20100101 Firefox/88.0',
    'reffer':'https://bj.meituan.com/'}
response = requests.get("https://waimai.meituan.com/",headers=headers)  #模拟请求url
```
<a name="y0j9N"></a>
### 3、ip地址伪装
对于网络中的反爬虫策略来说，大多数都是根据单个IP的行为来判断是不是网络爬虫的，例如，反爬虫检测到某个IP的访问次数很多，或者是访问的频率很快，就会封禁这个IP。这时就要选择代理IP来突破反爬虫的机制，更稳定的及逆行数据的爬取。Python添加代理IP的代码如下：
```python
import requests 
proxies={'https':'101.236.54.97:8866'} 
headers={
    'User-Agent':'Mozilla/5.0 (Windows NT 6.1; Win64; x64; rv:88.0) Gecko/20100101 Firefox/88.0',
    'reffer':'https://bj.meituan.com/'}
response = requests.get("https://waimai.meituan.com/",headers=headers，proxies=proxies)  #模拟请求url
```
代理IP可以自己去网上找免费的，但不太稳定，也可去花钱买一些比较稳定的。
<a name="LVfE8"></a>
### 4、伪装访问速率
真实用户的访问次数以及访问规律是很稳定的，并不会多次的访问，所以要伪装成真实的用户来爬取数据，这样反爬虫机制就不会察觉，可以采用控制访问频率的方式，主要是随机设置访问时间，代码如下：
```python
import requests 
import time,random
proxies={'https':'101.236.54.97:8866'} 
headers={
    'User-Agent':'Mozilla/5.0 (Windows NT 6.1; Win64; x64; rv:88.0) Gecko/20100101 Firefox/88.0',
    'reffer':'https://bj.meituan.com/'}
for i in range(10):
    response = requests.get("https://waimai.meituan.com/",headers=headers，proxies=proxies)  #模拟请求url
    time.sleep(random.uniform(1.1,5.4))
```
<a name="aFCsJ"></a>
### 5、伪装用户真实信息
有些网页是需要登录后才会显示数据，而cookie值会携带个人的登录信息，在爬虫中加入`cookie`值就能避免登录的麻烦，例如知乎、京东等网站，加入方法如下：
```python
import requests 
proxies={'https':'101.236.54.97:8866'} 
headers={
    'User-Agent':'Mozilla/5.0 (Windows NT 6.1; Win64; x64; rv:88.0) Gecko/20100101 Firefox/88.0',
    'reffer':'https://bj.meituan.com/'}
cookies=''
response = requests.get("https://waimai.meituan.com/",headers=headers，proxies=proxies,,cookies=cookies)  #模拟请求url
```


