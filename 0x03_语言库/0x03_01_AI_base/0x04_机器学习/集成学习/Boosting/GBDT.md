提升方法实际采用加法模型（即基函数的线性组合）与前向分布算法。以决策树为基函数的提升方法称为提升树。提升树模型可以表示为决策树的加法模型：

![](./img/6142625c2eb250be4a9ba52d0cb8a5b7.svg)

其中，![](./img/0fbe3a5057651185552d7b3c3fa608bd.svg)表示决策树；![](./img/8f11ec2f6ca5745136a358f86e667a35.svg)为决策树的参数；![](./img/69691c7bdcc3ce6d5d8a1361f22d04ac.svg)为树的个数。

提升树算法采用前向分步算法。首先确定初始提升树![](./img/e2e61e94c99ec37fa848bdda31126f36.svg)，第![](./img/6f8f57715090da2632453988d9a1501b.svg)步的模型是

![](./img/5a69101dc229658c50a8e37eb73a4fa9.svg)

其中，![](./img/e073d077293ea8cad3c73a239128b222.svg)为当前模型，通过经验风险极小化确定下一棵决策树的参数![](./img/8f11ec2f6ca5745136a358f86e667a35.svg)，

![](./img/474c88661e4d532c68b3becd6812d2b9.svg)

下面讨论针对不同问题的提升树学习算法，其主要区别在于使用的损失函数不同。包括平方误差损失函数的回归问题，用指数函数的分类问题，以及用一般损失函数的一般决策问题。

<a name="521be0ba"></a>
## 分类问题

将GBDT应用于回归问题，相对来说比较容易理解。因为回归问题的损失函数一般为平方差损失函数，这时的残差，恰好等于预测值与实际值之间的差值。每次拿一棵决策树去拟合这个差值，使得残差越来越小，这个过程还是比较intuitive的。而将GBDT用于分类问题，则显得不那么显而易见。

在说明分类之前，我们先介绍一种损失函数。与常见的直接求预测与真实值的偏差不同，这种损失函数的目的是最大化预测值为真实值的概率。这种损失函数叫做对数损失函数（Log-Likehood Loss），定义如下

![](./img/26bc26a6a56f052608cd9fbb7853d81a.svg)

对于二项分布，![](./img/2ccf20565f39f492a657ce6fd89a80c8.svg)，我们定义预测概率为![](./img/5674564ca817b77c845bac69dbade4f5.svg)，即二项分布的概率，可得

![](./img/957f46c5ec4ff471e9e21cd85526116c.svg)

即，可以合并写成

![](./img/9d5d2e48007ea73d9188b05ec221ad8c.svg)

即

![](./img/dddb1082919ed0a19a8b8c845fbfccc5.svg)

对于![](./img/4130c89f2d12c3ac81aba3adbff28685.svg)与![](./img/d76f2c4d6bdf142af5106c3f36e9e970.svg)的关系，我们定义为

![](./img/ae8ce8053e8786ab05760d10bbddb81f.svg)

<a name="a314d39b"></a>
### 二分类

类似于逻辑回归、FM模型用于分类问题，其实是在用一个线性模型或者包含交叉项的非线性模型，去拟合所谓的对数几率![](./img/aaabce23a377a7692b832995f393ee37.svg)。而GBDT也是一样，只是用一系列的梯度提升树去拟合这个对数几率，实际上最终得到的是一系列CART回归树。其分类模型可以表达为：

![](./img/1d8b549384a5bf6583632e193a251278.svg)

其中，![](./img/23cee294c31cecb7985d655f5625a675.svg)就是学习到的决策树。

清楚了这一点之后，我们便可以参考逻辑回归，单样本![](./img/e1b22a94bd6fb5e430efa18091a9b75d.svg)的损失函数可以表达为交叉熵：

![](./img/32403e4c1b910c5ac259ae7c467ca23b.svg)

假设第![](./img/8ce4b16b22b58894aa86c421e8759df3.svg)步迭代之后当前学习器为![](./img/f4f1ac2ea5a12d1b0ec3ec0130ff4efa.svg)，将![](./img/c9e53cbdffe795c0913cd927f13ffb9b.svg)的表达式带入之后，损失函数写为：

![](./img/936eeef65d44b53021cc964042785951.svg)

可以求得损失函数相对于当前学习器的负梯度为：

![](./img/063d2c27185d14567a6c7e833bbc2ca8.svg)

可以看到，同回归问题很类似，下一棵决策树的训练样本为：![](./img/5268e81a5fe7f444b25269fdbc9964c4.svg)，其所需要拟合的残差为真实标签与预测概率之差。于是便有下面GBDT应用于二分类的算法：

- ![](./img/18d41e4563b836ad9665ab689dcec50c.svg)，其中![](./img/03b632315ee5bee654b60a6bd902a249.svg)是训练样本中![](./img/6a267007228f9f654a0d28dec6932c31.svg)的比例，利用先验信息来初始化学习器
- ![](./img/bc6233f9bd532fc2e255464dbb4cdaed.svg)：
   - 计算![](./img/e478c9ac6fdcbfb61348c20be19da84b.svg)，并使用训练集![](./img/a0eb316377cb8fb4fe650d3035694aa9.svg)训练一棵回归树![](./img/76c0805f14b8f3a980cf28a134214ccd.svg)，其中![](./img/132a71fa58cbf776abd0f7027f3b653b.svg)
   - 通过最小化损失函数找树的最优权重：![](./img/f33bbe515584919df1510fd04ad5668a.svg)
   - 考虑shrinkage，可得这一轮迭代之后的学习器![](./img/482220a0864d5b5a8b0d1c8a65765bee.svg)，![](./img/7b7f9dbfea05c83784f8b85149852f08.svg)为学习率
- 得到最终学习器：![](./img/c5224d7ee3bf232eabb66fb81947ba2e.svg)

<a name="60d77de2"></a>
### 多分类

模仿上面两类分类的损失函数，我们能够将![](./img/a5f3c6a11b03839d46af9fb43c97c188.svg)类分类的损失函数定义为

![](./img/aa57380c1c3eb899a0f14f3d925b5a4a.svg)

其中，![](./img/e1879e64251d0585baa973732e48911e.svg)，且将![](./img/a8b9bd5204599937f7a3724f3f2933b3.svg)与![](./img/7a004872291ee1562ebb0c64dcc2eed1.svg)关系定义为

![](./img/7d7b6e3ae14e1491eff1ed86c4c61ea4.svg)

或者，换一种表达方式

![](./img/66715d0e373ddf5d13c8e9e070b9029c.svg)

即，对于多分类问题，则需要考虑以下softmax模型：

![](./img/69c1f36e8a09bdd80e17cbc3c6775761.svg)

其中![](./img/af7ae8c8eacd745b863dc682d5d3db39.svg)是![](./img/8ce4b16b22b58894aa86c421e8759df3.svg)个不同的tree ensemble。每一轮的训练实际上是训练了![](./img/8ce4b16b22b58894aa86c421e8759df3.svg)棵树去拟合softmax的每一个分支模型的负梯度。可见，这![](./img/8ce4b16b22b58894aa86c421e8759df3.svg)棵树同样是拟合了样本的真实标签与预测概率之差，与二分类的过程非常类似。

<a name="e80517ad"></a>
## 回归问题

已知一个训练数据集![](./img/ae9f8365ee00381a4ee90ee61d2b0af2.svg)，![](./img/bd60d710ce19420dade12257b132cbda.svg)为输入空间，![](./img/0c0f990b232052e2e8644bfd6652a516.svg)，![](./img/12f27527bbcbebfdebc4ffda139ff725.svg)为输出空间。如果将输入空间![](./img/bd60d710ce19420dade12257b132cbda.svg)划分为![](./img/ff44570aca8241914870afbc310cdb85.svg)个互不相交的区域![](./img/fb66f5414a01e7759a9155c27900f161.svg)，并且在每个区域上确定输出的常量![](./img/4bf5e6e7f38061ce5bb452465c966eed.svg)，那么树可表示为

![](./img/c2375efbe9bd6357613a1927b9f1ebb2.svg)

其中，参数![](./img/6b6dd811ca52c94e69e13a68febe8bdd.svg)表示树的区域划分和各区域上的常数。![](./img/ff44570aca8241914870afbc310cdb85.svg)是回归树的复杂度即叶结点个数。

回归问题提升树使用以下前向分布算法：

- ![](./img/e2e61e94c99ec37fa848bdda31126f36.svg)
- ![](./img/90fcc74024e23dbb5de54a89be31740a.svg)
- ![](./img/475fd7698f84f8a3480b00065de7ce14.svg)

在前向分步算法的第![](./img/6f8f57715090da2632453988d9a1501b.svg)步，给定当前模型![](./img/e073d077293ea8cad3c73a239128b222.svg)，需求解

![](./img/1bef96b7540b4d46597165da71b72648.svg)

得到![](./img/77fce9cad2e053d911b5f6293d3263ca.svg)，即第![](./img/6f8f57715090da2632453988d9a1501b.svg)颗树的参数。当采用平方误差损失函数时，

![](./img/7bc1682766104ce930d5814ed9725b4f.svg)

其损失变为

![](./img/247e508192785d2003a76f6d06eee053.svg)

这里，![](./img/df848a8cc068ed6c3b912dad695efeb1.svg)即当前模型拟合数据的残差。所以，对回归问题的提升树算法来说，只需简单地拟合当前模型的残差。

<a name="q2XZk"></a>
### 回归问题的提升树算法

输入：训练数据集![](./img/7ae05f9ecadae501addeb24734a2886a.svg)

输出：提升树![](./img/a919212ff67cf5e76f59a94e0ebf6902.svg)

（1）初始化![](./img/e2e61e94c99ec37fa848bdda31126f36.svg)

（2）对![](./img/71b9ad04aff480dd7485124a1a8feff3.svg)

- （a）按![](./img/2358e9d5883585d04f77037d32b6e364.svg)计算残差
- （b）拟合残差![](./img/7657683495075760cd15e2f50f2de376.svg)学习一个回归树，得到![](./img/0fbe3a5057651185552d7b3c3fa608bd.svg)
- （c）更新![](./img/5a69101dc229658c50a8e37eb73a4fa9.svg)

（3）得到回归提升树：![](./img/475fd7698f84f8a3480b00065de7ce14.svg)

<a name="0oenX"></a>
### 举例如下
| x | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 | 10 |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
| y | 5.56 | 5.70 | 5.91 | 6.40 | 6.80 | 7.05 | 8.90 | 8.70 | 9.00 | 9.05 |


对于![](./img/f3fa7ed3e0e30cc639e9ce8cb1dff9ca.svg)：

求![](./img/ae06113ce0d044d277f917b8c6eef7bd.svg)即回归树![](./img/3ff2960e625af6d70de019453806d306.svg)，首先通过以下优化问题：

![](./img/bfcbe114c2f1c0f71c0fb9e717ce397c.svg)

求解训练数据的切分点![](./img/03c7c0ace395d80182db07ae2c30f034.svg)：

![](./img/faf1b57c4a9304e222eb04a6de4c5b7b.svg)

容易求得在![](./img/b7000dd9eb57a731c41f8c85a55ff36a.svg)，![](./img/dcaf993833e8825e80c88a3a9eef9ed3.svg)内部使平方损失误差达到最小值的![](./img/576f1dacd615219d9f8bea06b26d5fdc.svg)，![](./img/71f0427a673c14326195285a092cc63a.svg)为

![](./img/4b8a6a6d2126e3a71b6d2a22fd7dbf80.svg)

这里![](./img/865adb153c19640c27383f3fc1ee7e8a.svg)，![](./img/0bf75941d6426e5efc12c0a20858452b.svg)是![](./img/b7000dd9eb57a731c41f8c85a55ff36a.svg)，![](./img/dcaf993833e8825e80c88a3a9eef9ed3.svg)的样本点数

求训练数据的切分点。根据所给数据，考虑如下切分点：1.5，2.5，3.5，4.5，6.5，7.5，8.5，9.5

对各切分点，求相应的![](./img/2e319f9b1a490804c9a88dd82bc334bd.svg)及![](./img/dd2be0a13474f3f904b95bb6617ef5a3.svg)

例如，当![](./img/1aa3bdbc0ce830f2243d1c97371967c4.svg)时，![](./img/0b8a10969943dfdd5610ab6059fa043d.svg)，![](./img/e00d4bbae37d9f8ffdb7ffbea85f5515.svg)，![](./img/baaf492105d4e80c6bf52c24077a2a07.svg)，![](./img/5a7f747df43c88307f688bc1e12f4747.svg)，

![](./img/0c85a277005194f972234d059825c5d1.svg)

现将![](./img/03c7c0ace395d80182db07ae2c30f034.svg)及![](./img/e643b3d0d33959f3addd349c9bb5c295.svg)的计算结果列表如下

| s | 1.5 | 2.5 | 3.5 | 4.5 | 5.5 | 6.5 | 7.5 | 8.5 | 9.5 |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
| m(s) | 15.72 | 12.07 | 8.36 | 5.78 | 3.91 | 1.93 | 8.01 | 11.73 | 15.74 |


由上表可知，当![](./img/b9e8b163e90025c95ad07fa15bdf3f75.svg)时![](./img/e643b3d0d33959f3addd349c9bb5c295.svg)达到最小值，此时![](./img/92b21420b513e1c26d4001fefb39a7f0.svg)，![](./img/c0771651962cd14a504de45bfb2c0cc5.svg)，![](./img/10f22fc4c8a1352f57da3398709360aa.svg)，![](./img/4a44d0554fd95aa9ea4fbc72ac2fca8d.svg)，所以回归树![](./img/3ff2960e625af6d70de019453806d306.svg)为

![](./img/8f24c4b2a5bfc66c6ae2508b8e609ed8.svg)

![](./img/8c672624cb8f8a48c5fde6a700243bf0.svg)

用![](./img/ae06113ce0d044d277f917b8c6eef7bd.svg)拟合训练数据的残差![](./img/f39d1ae185d1685606af369dce840b7c.svg)

| x | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 | 10 |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
| r_{2i} | -0.68 | -0.54 | -0.33 | 0.16 | 0.56 | 0.81 | -0.01 | -0.21 | 0.09 | 0.14 |


用![](./img/ae06113ce0d044d277f917b8c6eef7bd.svg)拟合训练数据的平均损失误差：![](./img/b7346890c61ff64d7e4d65f5fc67eee9.svg)

对于![](./img/4ae12f01f3c5aeee8f675424b1a9f29a.svg)：

求![](./img/6ff0519b35df9a12f482a29a5128793b.svg)，方法与求![](./img/3ff2960e625af6d70de019453806d306.svg)一样，只是拟合的数据为上一步的残差。可以得到

![](./img/59a9ecfba3ee9cb3355a6eba562d65eb.svg)

![](./img/9c6cfd27475e06fd509a2363e1b0d4ff.svg)

用![](./img/b95f1bf9e6c7b854f80f4eb96d563c87.svg)拟合训练数据的平方损失误差是：![](./img/d71bc8656aaf7dc4106080bf8bb6cc2b.svg)

对于![](./img/3bc551c5a3b53184a8d75c842c5ca900.svg)：

继续按上述方法计算，直至求得拟合训练数据的平方误差到可接受范围。

<a name="5e8754cc"></a>
## 梯度提升

提升树利用加法模型与前向分步算法实现学习的优化过程。当损失函数是平方损失和指数损失函数时，每一步优化是很简单的。但对一般损失函数而言，往往每一步优化并不那么容易。针对这一问题，梯度提升算法利用最速下降法的近似方法，其核心是利用损失函数的[负梯度](https://zhuanlan.zhihu.com/p/33260455)在当前模型的值

![](./img/b5402aaa4a740b9b7584042fc4ee44f8.svg)

作为回归问题提升树算法中的残差的近似值，拟合一个回归树。

输入：训练数据集![](./img/6581e4ad3025fa111b541a0ab103449c.svg)，损失函数![](./img/7a18e25616dc46fbac8138a106b4a5cf.svg)

输出：回归树![](./img/8fa889721e58a1f42232c0a3608524c0.svg)

（1）初始化：![](./img/3074fa12e23e9c0cf58785858c756e5a.svg)

（2）对![](./img/71b9ad04aff480dd7485124a1a8feff3.svg)

- （a）对![](./img/278a3fdadaae5c80e85ba69fb4f45874.svg)，计算
- ![](./img/0ffe028a0804c371c4a95df10a01e857.svg)
- （b）对![](./img/7657683495075760cd15e2f50f2de376.svg)拟合一个回归树，得到第![](./img/6f8f57715090da2632453988d9a1501b.svg)棵树的叶结点区域![](./img/a9b8f1751a64792c93d168807f07377e.svg)
- （c）对![](./img/aa9843cf68af6be9d112d42af0dd9840.svg)，计算
- ![](./img/f5d12dd251a2a7469c2f1cd7cd8246f1.svg)
- （d）更新![](./img/a589a1c2c54f78b3bfa712e23d28284f.svg)

（3）得到回归树：![](./img/60719b81a3c2813166329e55945c7607.svg)

算法第（1）步初始化，估计使损失函数极小化的常数值，它是只有一个根节点的数。

第（2a）步计算损失函数的负梯度在当前模型的值，将它作为残差的估计。对于平方损失函数，它就是通常所说的残差；对于一般损失函数，它就是残差的近似值。

第（2b）步估计回归树叶结点区域，以拟合残差的近似值。

第（2c）步利用线性搜索估计叶结点区域的值，是损失函数极小化。

第（2d）步更新回归树

第（3）步得到输出的最终模型。

<a name="Source"></a>
## Source

[https://zhuanlan.zhihu.com/p/46445201](https://zhuanlan.zhihu.com/p/46445201)<br />[https://zhuanlan.zhihu.com/p/25257856](https://zhuanlan.zhihu.com/p/25257856)
