## 梯度下降法(Gradient descent)

梯度下降法(Gradient descent)或最速下降法(Steepest descent)是求解无约束最优化问题的一种最常见的方法，有实现简单的有点。梯度下降法是迭代算法，每一步需要求解目标函数的梯度向量。

假设![](./img/50bbd36e1fd2333108437a2ca378be62.svg)是![](./img/046695bceb2371d496d2278caff5dad9.svg)是具有一阶连续偏导数的函数，要求解的无约束最优化问题是：

![](./img/93eb2b071a00d5425a8bb225e6fc4aaa.svg)

梯度下降法是一种迭代算法，选取适当的初值![](./img/c9334e5292986f3a604733dabe18bbc3.svg)，不断迭代，更新![](./img/9dd4e461268c8034f5c8564e155c67a6.svg)的值，进行目标函数的极小化，直到收敛。由于[负梯度方向是使函数值下降最快的方向](https://zhuanlan.zhihu.com/p/33260455)，在迭代的每一步，以负梯度方向更新![](./img/9dd4e461268c8034f5c8564e155c67a6.svg)的值，从而达到减少函数值的目的。

由于![](./img/50bbd36e1fd2333108437a2ca378be62.svg)具有一阶连续偏导数，若第![](./img/8ce4b16b22b58894aa86c421e8759df3.svg)次迭代值为![](./img/574f0e7c24b78246e9227c5e235853e0.svg)，则可将![](./img/50bbd36e1fd2333108437a2ca378be62.svg)在![](./img/574f0e7c24b78246e9227c5e235853e0.svg)附近进行一阶泰勒展开

![](./img/59ea730540e316665c56bb175b9d6964.svg)

这里，![](./img/63842683622db4e69fc81a22d2e2f351.svg)为![](./img/50bbd36e1fd2333108437a2ca378be62.svg)在![](./img/574f0e7c24b78246e9227c5e235853e0.svg)的梯度，求出第![](./img/a31a860e7a59c7616c1515ec3ae652a6.svg)次迭代值![](./img/4d896b2001b80734a36e343c1e382c6b.svg)

![](./img/788e8e8fa023785c05007aa667c09fa9.svg)

其中，![](./img/37776ef04e8a6a01d5ffe1dcf1322214.svg)是搜索方向，取负梯度方向![](./img/e83a18644d80732f4aa89ea3d1f975a6.svg)，![](./img/8ff9c1b69b4201fec1b23780372d5cdf.svg)是步长，由一维搜索确定，即![](./img/8ff9c1b69b4201fec1b23780372d5cdf.svg)使得

![](./img/7bfe17740d3e71e53daf8fdee0e6934d.svg)


### 具体算法

输入：目标函数![](./img/50bbd36e1fd2333108437a2ca378be62.svg)，梯度函数![](./img/92d1ada63bd656e9ed9eb2a1478a8a7a.svg)，计算精度![](./img/f8b1c5a729a09649c275fca88976d8dd.svg)；输出：![](./img/50bbd36e1fd2333108437a2ca378be62.svg)的极小点![](./img/5d9075efa0ec4b2e8228505844f11742.svg)

1. 取初始值![](./img/19f16338d210d9502fe36ab6cb3bedd6.svg)，置![](./img/88ca2849b6983bdef7879ade5527dd3d.svg)
2. 计算![](./img/fcc3ea535bfd666f796a8f33975187ef.svg)
3. 计算梯度![](./img/0f28f8356cbf417f5afb8306697337ad.svg)：
   1. 当![](./img/81369c23ffef0cd38334a178da44dbda.svg)时，停止迭代，令![](./img/92461b9e236faf35d0e5519f14c57ea8.svg)
   2. 否则，令![](./img/a2fa14b88c2c746d8bd0293aa533420c.svg)，求![](./img/8ff9c1b69b4201fec1b23780372d5cdf.svg)，使![](./img/21c0db52e7759faf9040a05f44a9ddfc.svg)
4. 置![](./img/7f46220be673bdeb402d35c9fb159bd6.svg)，计算![](./img/348b1694699c20231e90f9518ed67508.svg)
   1. 当![](./img/2a9bd70d4076c8f5079d8a5e8de09ef4.svg)或![](./img/3e1afbfed3e675f641b75ff05949e877.svg)时，停止迭代，令![](./img/4c8fe3b2fc6373580e9fafaa10626f75.svg)
5. 否则，置![](./img/2faa1c805fc4669b2a430017b16b6e14.svg)，转3.

当目标函数是凸函数时，梯度下降法的解是全局最优解。一般情况下，其解不保证是全局最优解，梯度下降法的收敛速度也未必是很快的。


## [牛顿法(Newton's method)](https://blog.csdn.net/itplus/article/details/21896453)

优化问题的最优解一般出现在极值点上，也就![](./img/4a3fc52d7e6d57bcf9bad4287f7c9ad0.svg)的解，所以，牛顿法求解优化问题即求![](./img/74caf4d1ec90d3a36ea7c7bbfe65b516.svg)的零点。

首先，随机选择初始值![](./img/3e0d691f3a530e6c7e079636f20c111b.svg)，对于函数![](./img/50bbd36e1fd2333108437a2ca378be62.svg)，计算相应的![](./img/c58b04667bcf6700fa38285f33640500.svg)和切线斜率![](./img/f73099bf155ddee8ab8a19de9b9e71ea.svg)(这里![](./img/6bda8af54c40bc23ed858e9e9f5c11d2.svg)即表示![](./img/8fa14cdd754f91cc6554c9e71929cce7.svg)的导数)。然后我们计算穿过点![](./img/95a184f9dbcc23c56cd3c2743062a7e3.svg)并且斜率为![](./img/f73099bf155ddee8ab8a19de9b9e71ea.svg)的直线和![](./img/9dd4e461268c8034f5c8564e155c67a6.svg)轴的交点的![](./img/9dd4e461268c8034f5c8564e155c67a6.svg)坐标，也就是

![](./img/fd4b5b813352baf7c65a0907ad13b740.svg)

我们将新求得点的![](./img/9dd4e461268c8034f5c8564e155c67a6.svg)坐标命名为![](./img/aa687da0086c1ea060a8838e24611319.svg)，通常![](./img/aa687da0086c1ea060a8838e24611319.svg)会比![](./img/3e0d691f3a530e6c7e079636f20c111b.svg)更接近方程![](./img/fd05d8d90456c441c8f10641bd8576bc.svg)的解，![](./img/9bc54fc2f0ec42a820e56728f659156e.svg)处其实并不是完全相等的，而是近似相等，因为只做了一阶泰勒展开，当进行了无限阶泰勒展开全加起来才是等式。由于相对而言一阶变换量是最大的，所以我们先只考虑一阶展开）。因此我们可以利用![](./img/aa687da0086c1ea060a8838e24611319.svg)开始下一轮迭代。迭代公式可简化为下式：

![](./img/fd4b5b813352baf7c65a0907ad13b740.svg)

                                                  ![](./img/bc06f311584515f936cc44be1bf024c1.svg)

                                                   ![](./img/b72271f4a22fb0ab9dbfbd7834bf18d9.svg)

                                                   ![](./img/f02f9bdd4c0dbf0f66c2af8024c47074.svg)

                                                   ![](./img/344f3804faa603184d39ebcf161f595f.svg)

通过迭代，这个式子必在![](./img/ffd4da749f2d1c0cb32433dfacd740bf.svg)的时候收敛，具体过程如下图。

![NewtonIteration_Ani.gif](./img/1592214216864-91f5e79e-704e-41d4-b0c2-d2905390d653.gif)

在优化问题中，其任务就是优化一个目标函数![](./img/8fa14cdd754f91cc6554c9e71929cce7.svg)，求这个函数![](./img/8fa14cdd754f91cc6554c9e71929cce7.svg)的极大极小问题，可以转化为求解函数的导数![](./img/eb447b2857ee983a0cf17da268b3cb92.svg)的问题了。这就和上面说的牛顿求解很相似了。

为了求导数![](./img/eb447b2857ee983a0cf17da268b3cb92.svg)的根，我们要把![](./img/50bbd36e1fd2333108437a2ca378be62.svg)在探索点![](./img/5bef92b1854f9c388d11bfbb1720c05d.svg)处泰勒展开，展开到二阶泰勒近似，即要考虑导数的导数：

![](./img/de17931470abea21d23c55d0ec36fff2.svg)

我们考虑二阶导，即导数的导数：

![](./img/977192e5cc09dda5bf88b2faf1d270c7.svg)

求得迭代公式：

![](./img/9ecbe8e1cb6ded4e5b3d0e620a07a1f1.svg)

上面解释了牛顿法的优化，都是以一维举例的（就一个变量![](./img/9dd4e461268c8034f5c8564e155c67a6.svg)）。从一维问题的迭代推广至多维问题只需将导数替换为梯度![](./img/d74105b64457f80d3fdf9000bc084f89.svg)，并将二阶导数的倒数替换为Hessian矩阵的逆矩阵![](./img/82cbd43ec4128721c22694bc538d0272.svg)，即：

![](./img/ab159c88a84b5de3126d3a6024e97e78.svg)

通常，使用牛顿法时会加入一个步长变量![](./img/0076b144c37c810860be29b8672c5054.svg)作微调，即：

![](./img/5169ff87c34dab18faddc38279ecf7c6.svg)

这个方法即被称为无约束牛顿法，通常用于第一步之后的迭代。





### 统计学习方法（李航）牛顿法详解

考虑无约束最优化问题：![](./img/93eb2b071a00d5425a8bb225e6fc4aaa.svg)

假设![](./img/50bbd36e1fd2333108437a2ca378be62.svg)具有二阶连续偏导数，若第![](./img/8ce4b16b22b58894aa86c421e8759df3.svg)次迭代值为![](./img/574f0e7c24b78246e9227c5e235853e0.svg)，则可将![](./img/50bbd36e1fd2333108437a2ca378be62.svg)在![](./img/574f0e7c24b78246e9227c5e235853e0.svg)附近进行二阶泰勒展开

![](./img/d1b4a946cdc75146ee2b37091b2a5a33.svg)

这里，![](./img/108b4c8106b38e872b58b935a099af41.svg)是![](./img/50bbd36e1fd2333108437a2ca378be62.svg)的梯度向量在点![](./img/574f0e7c24b78246e9227c5e235853e0.svg)的值，![](./img/8934fb331476c23414da3d0c9ea50d05.svg)是![](./img/50bbd36e1fd2333108437a2ca378be62.svg)的Hessian矩阵

![](./img/94d71a490d31a5356ed4571e956f3860.svg)

函数![](./img/50bbd36e1fd2333108437a2ca378be62.svg)有极值的必要条件是在极值点处一阶导数为0，即梯度向量为0特别是当![](./img/8934fb331476c23414da3d0c9ea50d05.svg)是[正定矩阵](https://zh.wikipedia.org/wiki/%E6%AD%A3%E5%AE%9A%E7%9F%A9%E9%98%B5)时，函数![](./img/50bbd36e1fd2333108437a2ca378be62.svg)的极值为极小值。牛顿法利用极小值的必要条件![](./img/c67c45718731ea3fadbacfc7e22b0f84.svg)，每次迭代从点![](./img/574f0e7c24b78246e9227c5e235853e0.svg)开始，求目标函数的极小点，作为第![](./img/a31a860e7a59c7616c1515ec3ae652a6.svg)次迭代值![](./img/4d896b2001b80734a36e343c1e382c6b.svg)，具体地，假设![](./img/4d896b2001b80734a36e343c1e382c6b.svg)满足![](./img/984587cd27bad6faf7c4922365228dac.svg)结合泰勒展开则有

![](./img/f229dcee0717dcc2a957ebb1dfec2534.svg)

其中![](./img/4190472dced9f6b24336713ab0b04070.svg)，这样

![](./img/755d51af89595ddca1cebdfe482f4974.svg)

因此，

![](./img/746c3753562984482872c2e9b2372158.svg)

将上式作为迭代公式的算法就是牛顿法。


### 具体算法

输入：目标函数![](./img/50bbd36e1fd2333108437a2ca378be62.svg)，梯度![](./img/92d1ada63bd656e9ed9eb2a1478a8a7a.svg)，海塞矩阵![](./img/355b078226649f1cf244ee0f59eeaf3a.svg)，精度要求![](./img/f8b1c5a729a09649c275fca88976d8dd.svg)

输出：![](./img/50bbd36e1fd2333108437a2ca378be62.svg)的极小点![](./img/5d9075efa0ec4b2e8228505844f11742.svg)

1. 取初始值![](./img/c9334e5292986f3a604733dabe18bbc3.svg)，置![](./img/88ca2849b6983bdef7879ade5527dd3d.svg)
2. 计算![](./img/0f28f8356cbf417f5afb8306697337ad.svg)
3. 若![](./img/81369c23ffef0cd38334a178da44dbda.svg)，则停止计算，得近似解![](./img/d9f94d5cf472f3951e42d4c87d7308eb.svg)
4. 计算![](./img/4190472dced9f6b24336713ab0b04070.svg)，并求![](./img/37776ef04e8a6a01d5ffe1dcf1322214.svg)：![](./img/49dceec9fd9e1cab5307fb1dfa7ec2a8.svg)
5. 置![](./img/8f97e6d1154932365a72f40971d09aa8.svg)
6. 置![](./img/2faa1c805fc4669b2a430017b16b6e14.svg)，转2.


### 应用举例

我们刷leetcode时有这样一道题，[求解平方根(只保留整数部分)](https://leetcode-cn.com/problems/sqrtx/)。牛顿迭代法是已知的实现求方根最快的方法之一，只需要迭代几次后就能得到相当精确的结果。设![](./img/9dd4e461268c8034f5c8564e155c67a6.svg)的![](./img/6f8f57715090da2632453988d9a1501b.svg)次方根为![](./img/0cc175b9c0f1b6a831c399e269772661.svg)，则推导公式如下：

![](./img/33a8c74e878df09f45cba221bf6d0647.svg)      ![](./img/b0e55c1d98e352e9de7c6bf45b8e1cab.svg)

代入牛顿法公式

![](./img/985d1d86e9ad0a8fa57e846a2071cf1a.svg)

           ![](./img/03a6556d6475d813fbca741cf0cc6510.svg)

代码如下(求平方根，即![](./img/4ae12f01f3c5aeee8f675424b1a9f29a.svg)，![](./img/90df05a54e021a01ef47d8937df21ce5.svg))：

```python
class Solution:
    def mySqrt(self, a):
        x = a
        while x > a / x:
            x = (x + a / x) // 2
        return int(r)
```





## 拟牛顿法(quasi-Newton method)

​	在牛顿法的迭代中，需要计算海赛矩阵的逆矩阵，这一计算比较复杂，考虑用一个![](./img/7b8b965ad4bca0e41ab51de7b31363a1.svg)阶矩阵![](./img/47182e4c0af9ad049aa2e24371574a22.svg)来近似替代![](./img/8ab0ac990a9a1d5b7d5ec1af43058c09.svg)。这就是拟牛顿法的基本想法。先看牛顿法迭代中海赛矩阵![](./img/1f59d0a44ed6e37998975a212e4c6055.svg)满足的条件。首先，![](./img/1f59d0a44ed6e37998975a212e4c6055.svg)满足以下关系。在![](./img/f229dcee0717dcc2a957ebb1dfec2534.svg)中取![](./img/7eb182d2676d6d8484d768fcc89982a1.svg)，得

![](./img/6cf561ca032500ad2d855b3bfe6fa4aa.svg)

​	记![](./img/077fc0ab1732b59e48f10620cef8550a.svg)，![](./img/413aa30d201cf1b7bbe435526cebaeed.svg)，则

​	![](./img/33212d384b9bfc8c19147a0e28b8e57f.svg)或![](./img/e4dd3d5d9a15d4ff3183a96b6c1a5604.svg)

​	上式即称为拟牛顿条件。如果![](./img/1f59d0a44ed6e37998975a212e4c6055.svg)是正定的(![](./img/735521714fd516c5e239867c2b9c251e.svg)也是正定的)，那么可以保证牛顿法搜索方向![](./img/37776ef04e8a6a01d5ffe1dcf1322214.svg)是下降方向。这是因为搜索方向是![](./img/db94bd394d2ac5ab5b5f3c78cf4b991b.svg)，结合牛顿法迭代式![](./img/746c3753562984482872c2e9b2372158.svg)有

![](./img/722a23ab6f0ef2302e04d830c59aea1c.svg)

所以![](./img/50bbd36e1fd2333108437a2ca378be62.svg)在![](./img/574f0e7c24b78246e9227c5e235853e0.svg)的泰勒展开式可以近似写成

![](./img/3891fb6dd3922f531cfac20f734064be.svg)

因![](./img/735521714fd516c5e239867c2b9c251e.svg)正定，故有![](./img/a0c9bff3165ac31a7fe7add1cde8dc10.svg)当![](./img/c6a6eb61fd9c6c913da73b3642ca147d.svg)为一个充分小的正数时，总有![](./img/c1dec310109be7156a9c056497e22909.svg)，也就是说![](./img/37776ef04e8a6a01d5ffe1dcf1322214.svg)是下降方向。

拟牛顿法将![](./img/c2ea3ff285c8537c2e3d9a7db7c554e5.svg)作为![](./img/8a1ed6fb9f9db1cee71fd105ae8ee087.svg)的近似，要求矩阵![](./img/c2ea3ff285c8537c2e3d9a7db7c554e5.svg)满足同样的条件。首先，每次迭代矩阵![](./img/c2ea3ff285c8537c2e3d9a7db7c554e5.svg)是正定的。同时，![](./img/c2ea3ff285c8537c2e3d9a7db7c554e5.svg)满足下面的拟牛顿条件：

![](./img/b889c1810b4f0bc4b4d9313988db4ed6.svg)

按照拟牛顿条件选择![](./img/c2ea3ff285c8537c2e3d9a7db7c554e5.svg)作为![](./img/735521714fd516c5e239867c2b9c251e.svg)的近似或选择![](./img/7bd1b4936c9b980674991d3f1633613a.svg)作为![](./img/1f59d0a44ed6e37998975a212e4c6055.svg)的近似的算法称为拟牛顿法。按照拟牛顿条件，在每次迭代中可以选择更新矩阵![](./img/a66f7de1c059bd6e88900e18f5f4e2b9.svg)：

![](./img/7ed8a1218e17029e7914563820d13b82.svg)


### DFP

DFP算法选择![](./img/a66f7de1c059bd6e88900e18f5f4e2b9.svg)的方法是，假设每一步迭代中矩阵![](./img/a66f7de1c059bd6e88900e18f5f4e2b9.svg)是由![](./img/c2ea3ff285c8537c2e3d9a7db7c554e5.svg)加上两个附加项构成的，即

![](./img/a649ab3d65467d0ca4b5a9a1f50e4992.svg)

其中![](./img/4d87fb6bc2dc8676fabdab15c468a633.svg)，![](./img/246c34c550a7977ad4c582df9b57f0b6.svg)是待定矩阵。这时，

![](./img/873d045f74e5e15afcacf4b11fd9a480.svg)

为使![](./img/a66f7de1c059bd6e88900e18f5f4e2b9.svg)满足拟牛顿条件，可使![](./img/4d87fb6bc2dc8676fabdab15c468a633.svg)和![](./img/246c34c550a7977ad4c582df9b57f0b6.svg)满足：

![](./img/6645629651de958d56dd2fcfe6f16b74.svg)    ![](./img/20cd6e6e4e03739e75294421adf301bf.svg)

事实上，不难找出这样的![](./img/4d87fb6bc2dc8676fabdab15c468a633.svg)和![](./img/246c34c550a7977ad4c582df9b57f0b6.svg)，例如取

![](./img/b1d7ae95cbe1e96629422dc82ad1ae93.svg)    ![](./img/78cad8583bc46a3956b2013b161102ba.svg)

这样就可得到矩阵![](./img/a66f7de1c059bd6e88900e18f5f4e2b9.svg)的迭代公式：

![](./img/55aa2b67606c982e5631391a36411e9b.svg)

称为DFP算法。可以证明，如果初始矩阵![](./img/b832012cc23cc15f9892c6d5349c7571.svg)是正定的，则迭代过程中的每个矩阵![](./img/c2ea3ff285c8537c2e3d9a7db7c554e5.svg)都是正定的。

输入：目标函数![](./img/50bbd36e1fd2333108437a2ca378be62.svg)，梯度![](./img/92d1ada63bd656e9ed9eb2a1478a8a7a.svg)，精度要求![](./img/f8b1c5a729a09649c275fca88976d8dd.svg)；输出：![](./img/50bbd36e1fd2333108437a2ca378be62.svg)的极小点![](./img/5d9075efa0ec4b2e8228505844f11742.svg)

1. 选定初始点![](./img/c9334e5292986f3a604733dabe18bbc3.svg)，取![](./img/b832012cc23cc15f9892c6d5349c7571.svg)为正定对称矩阵，置![](./img/88ca2849b6983bdef7879ade5527dd3d.svg)
2. 计算![](./img/0f28f8356cbf417f5afb8306697337ad.svg)若![](./img/81369c23ffef0cd38334a178da44dbda.svg)，则停止计算，得近似解![](./img/d9f94d5cf472f3951e42d4c87d7308eb.svg)
3. 置![](./img/662e06a71df55738ea7f1475515a4422.svg)
4. 一维搜索：求![](./img/8ff9c1b69b4201fec1b23780372d5cdf.svg)使得![](./img/21c0db52e7759faf9040a05f44a9ddfc.svg)
5. 置![](./img/7f46220be673bdeb402d35c9fb159bd6.svg)
6. 计算![](./img/2e8119c79ade89b4e5bc6eb10e1b6004.svg)
   1. 若![](./img/81369c23ffef0cd38334a178da44dbda.svg)，则停止计算，得近似解![](./img/4c8fe3b2fc6373580e9fafaa10626f75.svg)；
   2. 否则，按![](./img/0c43a96c88526b76be4a01bc7c51795d.svg)计算出![](./img/a66f7de1c059bd6e88900e18f5f4e2b9.svg)
7. 置![](./img/2faa1c805fc4669b2a430017b16b6e14.svg)，转3.



### BFGS

BFGS算法是最流行的拟牛顿算法。可以考虑用![](./img/c2ea3ff285c8537c2e3d9a7db7c554e5.svg)逼近海赛矩阵的逆矩阵![](./img/5c5ca72dc2cacdf240bf25622dc47f76.svg)，也可以考虑用![](./img/7bd1b4936c9b980674991d3f1633613a.svg)逼近海赛矩阵![](./img/c1d9f50f86825a1a2302ec2449c17196.svg)。这时，相应的拟牛顿条件是

![](./img/a0c996b62d5aa17bfe838c7993c690c5.svg)

可以用同样的方法得到另一迭代公式。首先令

![](./img/dfad77956c0cc0c0e27cd5615d71035e.svg)

![](./img/8548a30ca79601b928f93fa57ea55c58.svg)

考虑使![](./img/4d87fb6bc2dc8676fabdab15c468a633.svg)和![](./img/246c34c550a7977ad4c582df9b57f0b6.svg)满足：

![](./img/2185297169484402041323113ac6a7e8.svg)    ![](./img/2364d5f1fd871d1d1fb1684a343b248b.svg)

找出适合条件的![](./img/4d87fb6bc2dc8676fabdab15c468a633.svg)和![](./img/246c34c550a7977ad4c582df9b57f0b6.svg)，得到BFGS算法矩阵![](./img/76add380e06d5acfbb35a73a7fbb47f5.svg)的迭代公式：

![](./img/68334719a7fb236a71b50d07712d6d4e.svg)

可以证明，如果初始矩阵![](./img/c04ce5bbcf0b34afa4487b552a897d34.svg)是正定的，则迭代过程中的每个矩阵![](./img/7bd1b4936c9b980674991d3f1633613a.svg)都是正定的。

输入：目标函数![](./img/50bbd36e1fd2333108437a2ca378be62.svg)，梯度![](./img/92d1ada63bd656e9ed9eb2a1478a8a7a.svg)，精度要求![](./img/f8b1c5a729a09649c275fca88976d8dd.svg)；输出：![](./img/50bbd36e1fd2333108437a2ca378be62.svg)的极小点![](./img/5d9075efa0ec4b2e8228505844f11742.svg)

1. 选定初始点![](./img/c9334e5292986f3a604733dabe18bbc3.svg)，取![](./img/c04ce5bbcf0b34afa4487b552a897d34.svg)为正定对称矩阵，置![](./img/88ca2849b6983bdef7879ade5527dd3d.svg)
2. 计算![](./img/0f28f8356cbf417f5afb8306697337ad.svg)若![](./img/81369c23ffef0cd38334a178da44dbda.svg)，则停止计算，得近似解![](./img/d9f94d5cf472f3951e42d4c87d7308eb.svg)
3. 由![](./img/11d7c1138037502376382a69b2094812.svg)求出![](./img/37776ef04e8a6a01d5ffe1dcf1322214.svg)
4. 一维搜索：求![](./img/8ff9c1b69b4201fec1b23780372d5cdf.svg)使得![](./img/21c0db52e7759faf9040a05f44a9ddfc.svg)
5. 置![](./img/019db9b8808b8400575d8ff749d131d8.svg)
6. 计算![](./img/e0d8f5885fe9d818291d5e8f4327874d.svg)
   1. 若![](./img/21be24a05009248957bb86bc43fd1d20.svg)，则停止计算，得近似解![](./img/4c8fe3b2fc6373580e9fafaa10626f75.svg)
   2. 否则，按![](./img/a1281545b6eb127f1463d559258c809a.svg)计算出![](./img/76add380e06d5acfbb35a73a7fbb47f5.svg)
7. 置![](./img/2faa1c805fc4669b2a430017b16b6e14.svg)，转3.





## 坐标下降法(Coordinate descent)

坐标下降法是一种非梯度优化方法，它在每步迭代中沿一个坐标方向进行搜索，通过循环使用不用的坐标方向来达到目标函数的局部极小值。

假设我们求解![](./img/50bbd36e1fd2333108437a2ca378be62.svg)的极小值，其中![](./img/479f94e13f7ad2e683712fd7cd093e19.svg)是一个![](./img/8277e0910d750195b448797616e091ad.svg)维向量。从初始点![](./img/d59ac533525f1eee0a1d79683a9c95d5.svg)开始，坐标下降法通过迭代地构造序列![](./img/b9ba5fdc4cca6882216d0d4f4806c15d.svg)来解决问题，![](./img/22e1be9d07be1bbda0c8d22596ebffbd.svg)的第![](./img/865c0c0b4ab0e063e5caa3387c1a8741.svg)个分量![](./img/b71c3749e8d3aaaec0852d6e2b27dbd0.svg)构造为：

![](./img/f909484d61fa5847fe6a0b9d42dac503.svg)

通过执行此操作，显然有

![](./img/f5c0754c56aaaa7694cc4d184a5f18ab.svg)

与梯度下降法类似，通过迭代执行该过程，序列![](./img/b9ba5fdc4cca6882216d0d4f4806c15d.svg)能收敛到所期望的局部极小点或驻点。坐标下降法不需计算目标函数的梯度，在每次迭代中仅需求解一维搜索问题，对于某些复杂问题计算较为简便。但若目标函数不光滑，则坐标下降法有可能陷入非驻点。





## 最小二乘法(Least squares)

最小二乘法是无约束的数学优化方法，通过最小化误差的平方和寻找数据的最佳函数匹配。利用最小二乘法可以简便地求得未知的数据，并使得这些求得的数据与实际数据之间误差的平方和为最小，即

![](./img/152a6ea65f37cbec049dfb239d30bac8.svg)

最小值可以通过对上式参数分别求[偏导数](https://zh.wikipedia.org/wiki/%E5%81%8F%E5%AF%BC%E6%95%B0)，然后使它们等于零得到。





### 应用举例

某次实验得到了四个数据点![](./img/90cbc22edf225adf8a68974f51227f05.svg)：![](./img/b6237027a1725fbd14a8023bc3b7d995.svg)。我们希望找出一条和这四个点最匹配的直线![](./img/279955ea85405f2f81943ee8eded1cec.svg)，即找出在“最佳情况”下能够大致符合如下超定线性方程组的![](./img/b4ceec2c4656f5c1e7fc76c59c4f80f3.svg)和![](./img/d9f51e864a6151f57e727294da7ac28c.svg)：

![](./img/fe1773ca4a37ee5768a8e1a9e815509b.svg)    ![](./img/08b942a60afcfbcaf4f3caa59154267b.svg)    ![](./img/7c555d1d6e0fcbac1915bdc1cc262977.svg)    ![](./img/bb82a6d9bb76ca91e5974ad364d24baf.svg)

最小二乘法采用的手段是尽量使得等号两边的方差最小，也就是找出这个函数的最小值：

![](./img/c9d27c8cbb11efdaa794b4a5c2907a0c.svg)

最小值可以通过对![](./img/4d12d0186526704f57fd8ddd3732dca5.svg)分别求![](./img/b4ceec2c4656f5c1e7fc76c59c4f80f3.svg)和![](./img/d9f51e864a6151f57e727294da7ac28c.svg)的偏导数，然后使它们等于零得到。

![](./img/b6889b55a3f5671b02f853febf0336d1.svg)    ![](./img/237138d2e9bfba8ef7bbf32514519ed0.svg)

如此就得到了一个只有两个未知数的方程组，易得![](./img/ec7985f7704171ee0208d1db3e874357.svg)，![](./img/611a91383fa19f8a35b40326632d52fa.svg)，所以![](./img/e110ffe02b87e6c5abcdfecb9e9621fe.svg)最佳





## [置信域方法（Trust-region methods）](https://zh.wikipedia.org/wiki/%E7%BD%AE%E4%BF%A1%E5%9F%9F%E6%96%B9%E6%B3%95)

置信域方法（Trust-region methods）又称信赖域方法，它是一种最优化方法，能够保证最优化方法总体收敛。





### 思想框架

求![](./img/baab46696b8dd7dd5326e784a824d874.svg)，![](./img/50bbd36e1fd2333108437a2ca378be62.svg)是定义在![](./img/046695bceb2371d496d2278caff5dad9.svg)上的二阶连续可微函数。定义当前点邻域![](./img/834faffb9f4a28541d8ab484b7902a12.svg)。这里![](./img/5b3e00cfdbe2e93451396371e6f3474d.svg)称为置信域半径。假定在这个邻域中，二次模型是目标函数![](./img/50bbd36e1fd2333108437a2ca378be62.svg)的一个合适的近似，则在这个邻域（称为置信域）中极小化二次模型，得到近似极小点![](./img/2a237e54504442e3d483a39f75df7bfa.svg)，并取，其中![](./img/f87a2fcec84d682d94a2db4992e00ef5.svg)。

置信域方法的模型子问题是

![](./img/88576919773c5d943d58a9868f62c8bf.svg)

其中，![](./img/1c3d8357d519ceb53ec4ea66c45c50f9.svg)，![](./img/de519b698c527f859a8d52f2b90c3628.svg)，![](./img/7bd1b4936c9b980674991d3f1633613a.svg)是一个对称矩阵，它是Hessian矩阵![](./img/e7182486a7048528622cfbd3ecdf27cd.svg)或其近似，![](./img/34618f8e859ea56ffa11cae7acaadc8a.svg)为置信域半径，![](./img/2275b70d5254126abcdd3be81bbf5976.svg)为某一范数，通常我们采用L2范数。选择![](./img/5b3e00cfdbe2e93451396371e6f3474d.svg)的方法：根据模型函数![](./img/61295c816885a8483820548329d99eaf.svg)对目标函数![](./img/50bbd36e1fd2333108437a2ca378be62.svg)的拟合程度来调整置信域半径![](./img/5b3e00cfdbe2e93451396371e6f3474d.svg)。对于置信域方法的模型子问题的解![](./img/2a237e54504442e3d483a39f75df7bfa.svg)，设目标函数的下降量

![](./img/b73df5553a2ddde473a131a45e251f04.svg)

为实际下降量，设模型函数的下降量

![](./img/003f6d5ebc3fabfbda7cdd44945f99db.svg)

为预测下降量。定义比值

![](./img/26c3db91ab1cfd7a4e75e43cfcc08ad2.svg)

它用来衡量模型函数![](./img/713d5a64d230070b7f905b5cb6492a14.svg)与目标函数![](./img/8fa14cdd754f91cc6554c9e71929cce7.svg)的一致性程度。



### 具体算法

1. 给出初始点![](./img/3e0d691f3a530e6c7e079636f20c111b.svg)，置信域半径的上界![](./img/9d3cac90ed5b8759735e6440f0264098.svg)，![](./img/52eadb9237cae183d5a0c36f7f35bc87.svg)，![](./img/077cb4bb6159a833cf3a2c8bfd7c2114.svg)，![](./img/8647cccaf97d0673d43017382b8ed4b4.svg)，![](./img/db4eb5984ec0adc200589b4ad213f9ce.svg)，![](./img/4505ec5ffa9d72265d0346b896b539a1.svg)
2. 如果![](./img/f16b36bda0688b2376deb2c370e689a5.svg)，停止
3. (近似地)求解置信域方法的模型子问题，得到![](./img/2a237e54504442e3d483a39f75df7bfa.svg)
4. 计算![](./img/f9bdf0f69b0bc5a64851eee2427f1229.svg)和![](./img/1c06ba8c53212c805a523132ed7041d6.svg)。令
   1. ![](./img/7f032fdcd52f7e775e0082a3f13bc0e5.svg)
5. 校正置信域半径，令
   1. ![](./img/f9ea383585d4f32476330cd8284d8b5b.svg)
6. 产生![](./img/76add380e06d5acfbb35a73a7fbb47f5.svg)，校正![](./img/f0d67d848468c1e5da5111bd1238ca60.svg)，令![](./img/7d6a400bea39d277e8f04d98cbba98c0.svg)，转2.
