最优化求解问题可能是我们在工作中遇到的最多的一类问题了：从已有的数据中炼出最适合的模型参数，从而对未知的数据进行预测。当我们面对高维高数据量的场景时，常见的批量处理的方式已经显得力不从心，需要有在线处理的方法来解决此类问题。
<a name="t6qld"></a>
# 动机与目的
在实际工作中，无论是工程师、项目经理、产品同学都会经常讨论一类话题：“从线上对比的效果来看，某某特征或因素对某某产品的最终效果有很大的影响”。这类话题本质上说的是通过已有的数据反映出某些特定的因素对结果有很强的正（或负）相关性。而如何定量计算这种相关性？如何得到一套模型参数能够使得效果达到最优？这就是最优化计算要做的事情。

举一类典型点的例子：在推荐和广告计算中，我们经常会需要对某些值进行预测，例如在一条推荐或广告在曝光之前先预测用户是否会产生点击（CTR预估），或者是否会由此产生某些转化（RPM预估）。这类问题可以表示为：针对一个输入![](./img/cafb1bf9a89550f3bf474f8f72ae1f0a.svg)，通过某个函数![](./img/15d8420cf5d547366c0636eed59786d1.svg)计算（预测）输出![](./img/779aa012a4779de2a3f61c906d6f9043.svg)。根据![](./img/415290769594460e2e485922904f345d.svg)值为连续的还是离散的，预测问题被划分成回归问题和分类问题。而利用已有的样本数据![](./img/1e82833eacd88b81e96ed3e00493787a.svg)训练![](./img/15d8420cf5d547366c0636eed59786d1.svg)的过程往往转换成一个最优化求解的过程。

无论是线性回归（Linear Regression）、逻辑回归（Logistic Regression）、支持向量机（SVM）、深度学习（Deep Learning）中，最优化求解都是基本的步骤。常见的梯度下降、牛顿法、拟牛顿法等属于批量处理的方法（Batch），每次更新都需要对已经训练过的样本重新训练一遍。而当我们面对高维高数据量的时候，批量处理的方式就显得笨重和不够高效，因此需要有在线处理的方法（Online）来解决相同的问题。
<a name="4jfJi"></a>
# 从Batch到Online
我们面对的最优化问题都是无约束的最优化问题（有约束最优化问题可以利用拉格朗日乘数法或KKT条件转化成无约束最优化问题），因此我们通常可以将它们描述成：

![](./img/c7f769e18f028f382aec1f2f74e69a2c.svg)<br />![](./img/793245bd81e00da04f2eb27f1940bcb7.svg)<br />![](./img/c9be008e0974ce40917da0357c9c2d90.svg)<br />这里![](./img/21c2e59531c8710156d34a3c30ac81d5.svg)为观测样本集合（训练集）；![](./img/9ad099394c6c5cea2a84519998301c9b.svg)为第![](./img/363b122c528f54df4a0446b6bab05515.svg)条样本的特征向量；![](./img/c9be008e0974ce40917da0357c9c2d90.svg)为预测值；![](./img/0b00f782320de00f5814ce4e25f5027e.svg)为特征向量到预测值的映射函数；![](./img/50f0d238908487fd71d76319435159ba.svg)为最优化求解的目标函数，也称作损失函数，损失函数通常可以分解为各样本损失函数的累加，即![](./img/0df291162c49c40c249af07fb4c74db2.svg)；![](./img/61e9c06ea9a85a5088a499df6458d276.svg)为特征权重，也就是我们需要求解的参数。

在我们实际的数值计算中，通常的做法是随机给定一个初始的![](./img/76564c2545b2c372aebe7bd0b3fbafb9.svg)，通过迭代，在每次迭代中计算损失函数在当前![](./img/b96af25f9f72fbb3c84e9557f3d7df1d.svg)的下降方向，并更新![](./img/61e9c06ea9a85a5088a499df6458d276.svg)，直到损失函数稳定在最小的点。例如梯度下降法（GD，Gradient Descent）就是通过计算损失函数在当前![](./img/b96af25f9f72fbb3c84e9557f3d7df1d.svg)处的梯度，以梯度![](./img/4d75dd209e5c26aaeeecefd38c9dfc5e.svg)的反方向作为下降方向更新![](./img/61e9c06ea9a85a5088a499df6458d276.svg)，如果损失函数是一个非平滑的凸函数（Non-smooth convex），在不可导处用次梯度（Subgradient）方向的反方向作为下降方向。算法如下：

![](./img/4cb53bf1f9e08435427ab940babe362d.svg)

GD是一种批处理的方式（Batch），每次更新![](./img/61e9c06ea9a85a5088a499df6458d276.svg)的时候都要扫描所有的样本以计算一个全局的梯度![](./img/44abdf7d95794bf2dd5aa920639b3203.svg)<br />考虑另一种策略

![](./img/801640f65fe6135fb9fe7d44d04c69b1.svg)

每次迭代仅仅根据单个样本更新权重![](./img/61e9c06ea9a85a5088a499df6458d276.svg)，这种算法称作随机梯度下降（SGD，Stochastic Gradient Descent）。与GD相比较，GD每次扫描所有的样本以计算一个全局梯度，SGD则每次只针对一个观测到的样本进行更新。通常情况下，SGD能够比GD“更快”地令![](./img/61e9c06ea9a85a5088a499df6458d276.svg)逼近最优值。当样本数![](./img/69691c7bdcc3ce6d5d8a1361f22d04ac.svg)特别大的时候，SGD的优势更加明显，并且由于SGD针对观测到的“一条”样本更新![](./img/61e9c06ea9a85a5088a499df6458d276.svg)，很适合进行增量计算，实现梯度下降的Online模式（OGD，Online Gradient Descent）

<a name="MWulR"></a>
# 在线最优化求解算法
稀疏性对于高维特征向量以及大数据集特别重要，假设有一亿维特征，但其有良好稀疏性，发现只有其中1000维特征与结果高度相关，那我们训练时模型仅高度关注这1000维特征即可，也就是模型帮我们做了需要业务精通的专业人士下大力才能做的特征工程。但是在Online模式下，不同于Batch，Online中每次![](./img/61e9c06ea9a85a5088a499df6458d276.svg)的更新并不是沿着全局梯度进行下降，而是沿着某个样本的产生的梯度方向进行下降，整个寻优过程变得像是一个“随机”查找的过程（这也就是SGD中Stochastic的来历），这样Online最优化求解即使采用L1正则化的方式，也很难产生稀疏解。所以在我们接下来讨论的各个在线最优化求解算法中，稀疏性是一个重要的追求目标。

<a name="JKBnx"></a>
## TG
为了得到稀疏的特征权重![](./img/61e9c06ea9a85a5088a499df6458d276.svg)，最简单粗暴的方式就是设定一个阈值，当![](./img/61e9c06ea9a85a5088a499df6458d276.svg)的某维度上系数小雨这个阈值时将其设置为![](./img/cfcd208495d565ef66e7dff9f98764da.svg)（称作简单截断）。这种方法实现起来很简单，也很容易理解，即模型判断这个特征与最终结果相关性非常低，那我们可以直接抛弃这个特征。但实际中（尤其在OGD里面）![](./img/61e9c06ea9a85a5088a499df6458d276.svg)的某个系数比较小可能是因为该维度训练不足引起的，简单进行截断会造成这部分特征的丢失。梯度截断法（TG，Truncated Gradient）是由John Langford，Lihong li和Tong Zhang在2009年提出，实际上是对简单截断的一种改进

<a name="MXoXz"></a>
### L1正则法
由于L1正则项在![](./img/cfcd208495d565ef66e7dff9f98764da.svg)处不可导，往往会造成平滑的凸函数变成非平滑凸优化问题，因此在每次迭代中采用次梯度计算L1正则项的梯度。权重更新方式为：

![](./img/ac28f006489d4b43893fed17896c124e.svg)

这里的![](./img/a42cf36b3db2499709e851aa025f0fd7.svg)是一个标量，且![](./img/432b4cfb979f14a140b7205d6bc5dc56.svg)，为L1正则化参数；![](./img/55835086cd3651baee26fbf9def6022b.svg)为符号函数，如果![](./img/e8812f06c9b453c730f834d064cbdd4a.svg)是一个向量，![](./img/1df181eaa1bb40a0067c06ead197170d.svg)是向量的一个维度，那么有![](./img/415d46d9e739edc73bc9b293274e0280.svg)；![](./img/130f3b9c26e9892cba611a7bde4d407f.svg)为学习率，通常将其设置成![](./img/ef6ec803da0a4b6d2289e785dfa07275.svg)的函数；![](./img/cd2eb1003bf183d37e2062421b2fe3c9.svg)代表了第![](./img/e358efa489f58062f10dd7316b65649e.svg)次迭代中损失函数的梯度，由于OGD每次仅根据观测到的一个样本进行权重更新，因此也不再使用区分样本的下标![](./img/363b122c528f54df4a0446b6bab05515.svg)

<a name="xROyf"></a>
### 简单截断法
以![](./img/8ce4b16b22b58894aa86c421e8759df3.svg)为窗口，当![](./img/fff1e206b48a55702c7844111058f42b.svg)不为整数时，采用标准的SGD进行迭代，当![](./img/fff1e206b48a55702c7844111058f42b.svg)为整数时，采用如下权重更新方式

![](./img/6f492f8fab02c6311e7e164b1a8d498b.svg)<br />![](./img/3e7874787df89d7f8ad25770fe5e55a2.svg)

这里![](./img/57defdf0ce79038267b4bd06d3174b6e.svg)是一个标量，且![](./img/ae7583ed871575f83a76f5432e4b44ff.svg)；如果![](./img/e8812f06c9b453c730f834d064cbdd4a.svg)是一个向量，![](./img/1df181eaa1bb40a0067c06ead197170d.svg)是向量的一个维度，那么有![](./img/4f301266c5fec82b5039c2a2125d3e75.svg)

<a name="WedCC"></a>
### 梯度截断法
上述的简单截断法被TG的作者形容为too aggressive，因此TG在此基础上进行了改进，同样是采用截断的方式，但是比较不那么粗暴。采用相同的方式表示为：

![](./img/91488a935cdb1a49f48d6922d3bfd04d.svg)<br />![](./img/c9bdba17919b5c40ec86624b01fbcd3f.svg)

其中![](./img/c5f8bdac390b8e27238544b99fdccc07.svg)且![](./img/74bf7d372711ac3af1089b6d774aaff4.svg)。TG同样是以![](./img/8ce4b16b22b58894aa86c421e8759df3.svg)为窗口，每![](./img/8ce4b16b22b58894aa86c421e8759df3.svg)步进行一次截断。当![](./img/fff1e206b48a55702c7844111058f42b.svg)不为整数时![](./img/58ebbd1be5a6035fdb936f13ac4e3b8d.svg)，当![](./img/fff1e206b48a55702c7844111058f42b.svg)为整数时![](./img/acb848cfc2f8a0b4852ec87c923f2a0d.svg)。从上述公式可以看出，![](./img/c6a6eb61fd9c6c913da73b3642ca147d.svg)和![](./img/2554a2bb846cffd697389e5dc8912759.svg)决定了![](./img/61e9c06ea9a85a5088a499df6458d276.svg)的稀疏程度，这两个值越大，则稀疏性越强。尤其令![](./img/06784ca824c98577b71e436087f71841.svg)时，只需要通过调节一个参数就能控制稀疏性。通过公式，我们很容易写出TG的算法逻辑：<br />![WX20210223-111203.png](./img/1614049934407-67665440-6186-4aa7-8f58-7eef5b9da5ff.png)

<a name="e26T1"></a>
### TG与简单截断以及L1正则化的关系
简单截断和截断梯度的区别在于采用了不同的截断公式![](./img/ab4ffa55f688360e0c12aef543c18351.svg)和![](./img/2452fee413f58bb9509e88d80d4b9f8d.svg)，如下图所示<br />![WX20210223-111314.png](./img/1614050004491-b7db58b5-5fc2-444c-93ed-fe362a945903.png)<br />为了清晰地进行比较，我们将TG的公式进行改写，描述特征权重每个维度的更新方式：

![](./img/aa6d73f7a8640b1f288021a7ddd70798.svg)

![](./img/450f112bcd1106e145d1ca01d2f8a13e.svg)

![](./img/4bf4dcdf1d6d23e34872e4e5982f097f.svg)

如果令![](./img/cfc48db3bdf04ac961d546e28ffec948.svg)，截断公式![](./img/c4dde5b7faf79d21df485807702db32c.svg)变成

![](./img/4466000e40b3f86c23286220635e7834.svg)

此时TG退化成简单截断法。<br />如果令![](./img/443fdd0d2d5619d0f028777878e332bf.svg)截断公式![](./img/3f2d925b7aa0b19f3c8f1fb413bbd66b.svg)变成：

![](./img/f1873c54c3c7d0cb2511ab3809fb01e7.svg)

如果再令![](./img/ceef78b61bf01306cc7e80344c92c19d.svg)，那么特征权重维度更新公式变成：

![](./img/ba8e9e9444802c6328db9c43411fb68b.svg)

此时，TG退化为L1正则化法。
<a name="BKSAg"></a>
## Forward-Backward Splitting
<a name="DtvTw"></a>
### 算法原理
前向后向切分（Forward-Backward splitting）将权重的更新分为两个步骤：

![](./img/2ddfa6c00ad7b10bd3d70ec40dc545b4.svg)<br />![](./img/dff36835cf5d76ae727fa2467eacf21f.svg)

前一个步骤实际上是一个标准的梯度下降步骤，后一个步骤可以理解为对梯度下降的结果进行微调。<br />观察第二个步骤，发现对![](./img/61e9c06ea9a85a5088a499df6458d276.svg)的微调也分为两部分：

1. 前一部分保证微调发生在梯度下降的结果附近
2. 后一部分则用于处理正则化，产生稀疏性

如果将两个步骤合二为一，即将![](./img/1e2e870f40afda1101ff75db2bacfb92.svg)的计算代入![](./img/c6d231942b6a24eaf76414bc7c3a85a1.svg)中，有：

![](./img/32c028ce83223f7e3d0acbd36a798ed3.svg)

令![](./img/4ddb9955da9f789da1aef455ca9d4666.svg)，如果![](./img/c6d231942b6a24eaf76414bc7c3a85a1.svg)存在一个最优解，那么可以推断![](./img/cfcd208495d565ef66e7dff9f98764da.svg)向量一定属于![](./img/cc56d94e2254e5251d2cacddea2933fc.svg)的次梯度集合：

![](./img/2960ca8a04af4fe2ac4c6bba14fc61a6.svg)

由于![](./img/6f86a3ebe652376ddf69fc7386e136bf.svg)，那么有：

![](./img/1277b597f891aacd5e77dce11f14a787.svg)

上式实际上给出了权重更新的另一种形式：

![](./img/90a3c29d23e304eb4662846463b5dfda.svg)

我们这里可以看到，![](./img/c6d231942b6a24eaf76414bc7c3a85a1.svg)不仅仅与迭代前的状态![](./img/b96af25f9f72fbb3c84e9557f3d7df1d.svg)有关，而且与迭代后的![](./img/c5c36f4f105c1e009c61fcc24c5a1664.svg)有关，这就是前后向名称的由来。

<a name="OxtYW"></a>
### L1-Forward-Backward Splitting
在L1正则化下，有![](./img/607ab1b8f70ed33dfa595a2e41283b2d.svg)。为了简化描述，用向量![](./img/91a253380bbdaa56b9bb3de92a85911b.svg)来表示![](./img/1e2e870f40afda1101ff75db2bacfb92.svg)，用标量![](./img/ba23edf478e8ff1c8796f94105937aba.svg)来表示![](./img/76b2ccc7b46dabdcd8683df7251b4d69.svg)，并将公式等号右边按维度展开：

![](./img/d10219bd0c04a0688e726b6fcf2cfe52.svg)

我们可以看到，在求和公式![](./img/8e31e8a8010780a409245cd20b70427a.svg)中的每一项都是大于等于![](./img/cfcd208495d565ef66e7dff9f98764da.svg)的，所以上式可以拆解成对特征权重![](./img/61e9c06ea9a85a5088a499df6458d276.svg)每一维度单独求解：

![](./img/14588348819694f8ac98a6ea368f52b9.svg)

首先，假设![](./img/509ac8cca3e0bf401a4808df447e4799.svg)是![](./img/b96a7b2591841c1791ecd5959d20a872.svg)的最优解，则有![](./img/7af38d29ca8dc43b848ea3956806104d.svg)，这是因为：<br />反证法：假设![](./img/41bfb892d2375f17413983c3df34aded.svg)，那么有：

![](./img/e676daf7775817dde5defa22e8c1ee67.svg)

这与![](./img/9292b735936331cb3d842779705bb1ad.svg)是![](./img/b96a7b2591841c1791ecd5959d20a872.svg)的最优解矛盾，故假设不成立，![](./img/7af38d29ca8dc43b848ea3956806104d.svg)

综合上面的分析，可以得到Forward-Backward Splitting在L1正则化的条件下，特征权重的各个维度更新：

![](./img/80a0f3e8f916f9af8afe5f8ab87031a3.svg)

其中![](./img/0775ab367e42f529b9966c2e0a2a2c60.svg)为梯度![](./img/88b1bac0993542519f6c285f34f3adaa.svg)在维度![](./img/865c0c0b4ab0e063e5caa3387c1a8741.svg)上的取值。根据上式我们很容易就可以设计出其算法逻辑<br />![WX20210224-170706.png](./img/1614157634488-8901d2ec-64f2-4adc-9244-cbbc893a371b.png)

<a name="3Jq5f"></a>
### L1-Forward-Backward Splitting与TG的关系
从上面的分析可以看出，L1-Forward-Backward Splitting在每次更新![](./img/61e9c06ea9a85a5088a499df6458d276.svg)的时候，对![](./img/61e9c06ea9a85a5088a499df6458d276.svg)的每个维度都会进行判定，当![](./img/3ca9a83e90d52855e792109a3f51ba7d.svg)时对该维度进行“截断”，令![](./img/51001800527ba7e1eeada96962e37722.svg)。那么我们怎么去理解这个判定条件呢？如果我们把判定条件写成![](./img/b9a9d05c67ccd3672ddf156d87a9edf1.svg)，那么这个含义就很清晰了：当一条样本产生的梯度不足以令对应维度上的权重值发生足够大的变化![](./img/76b2ccc7b46dabdcd8683df7251b4d69.svg)，认为在本次更新过程中该维度不够重要，应当令其权重为![](./img/cfcd208495d565ef66e7dff9f98764da.svg)。L1-Forward-Backward Splitting特征权重的各个维度更新公式也可以写作如下形式：

![](./img/483232c802638fc4193a8aa500a72add.svg)

比较上式与TG的特征权重维度更新公式，我们发现如果令![](./img/443fdd0d2d5619d0f028777878e332bf.svg)，![](./img/ceef78b61bf01306cc7e80344c92c19d.svg)，![](./img/23d58bce26c3e8c5ff4b312e6b8d799f.svg)，L1-Forward-Backward Splitting与TG完全一致。我们可以认为L1-Forward-Backward Splitting是TG在特殊条件下的特殊形式。

<a name="CUCsS"></a>
## RDA
<a name="5oAUo"></a>
### 算法原理
不论怎样，简单截断、TG、FOBOS 都还是建立在 SGD 的基础之上的，属于梯度下降类型的方法，这类型方法的优点就是精度比较高，并且 TG、FOBOS 也都能在稀疏性上得到升。但是有些其它类型的算法，例如 RDA，是从另一个方面来求解 Online Optimization 并且更有效地升了特征权重的稀疏性。

正则对偶平均（RDA，Regularized Dual Averaging）是微软十年的研究成果，RDA是Simple Dual Averaging Scheme一个扩展，其特征权重的更新策略为：

![](./img/cfc48228e2eff7cf441a3b218d83c60e.svg)

其中，![](./img/47f873dc9b40d11653f964be72d48959.svg)表示梯度![](./img/431fc17f6019398b6030e6b273f1be66.svg)对![](./img/61e9c06ea9a85a5088a499df6458d276.svg)的积分平均值（积分中值）；![](./img/977110f860ebb2bfcc6b75a327d45f07.svg)为正则项；![](./img/322ce3c7e8e1188917b2c30ddd058a2c.svg)为一个辅助的严格凸函数；![](./img/6755d46ef6de6e164d03733eae4c9a8d.svg)是一个非负非自减序列。本质上，上式包含了3个部分：

1. 线性函数![](./img/058f84f31fc0e49c25d3c791f41f25e6.svg)，包含了之前所有梯度（或次梯度）的平均值（dual average）
2. 正则项![](./img/977110f860ebb2bfcc6b75a327d45f07.svg)
3. 额外正则项![](./img/f430086d76ba6c16db9d1351e2692298.svg)，它是一个严格凸函数

<a name="5SqjE"></a>
### L1-RDA
我们下面来看看在L1正则化下，RDA中的特征权重更新具有什么样的形式以及如何产生稀疏性。令![](./img/607ab1b8f70ed33dfa595a2e41283b2d.svg)，并且由于![](./img/322ce3c7e8e1188917b2c30ddd058a2c.svg)是一个关于![](./img/61e9c06ea9a85a5088a499df6458d276.svg)的严格凸函数，不妨令![](./img/1b38bcb00607667687022ee1c84268c7.svg)，此外，将非负非自减序列![](./img/6755d46ef6de6e164d03733eae4c9a8d.svg)定义为![](./img/c47b5008812673f00e8b70fb87ac2aad.svg)，将L1正则化代入公式有：

![](./img/958fda03ae17d7418c7bfd7c7da53339.svg)

针对特征权重的各个维度将其拆解成![](./img/8d9c307cb7f3c4a32822a51922d1ceaa.svg)个独立的标量最小化问题，我们可以得到L1-RDA各维度更新方式：

![](./img/399f4804743b4443700b9c67a532f479.svg)

这里我们发现，当某个维度上积累梯度平均值的绝对值![](./img/2ef6ac6445409f5e090087c84bf25018.svg)小于阈值![](./img/c6a6eb61fd9c6c913da73b3642ca147d.svg)的时候，该维度权重将被置为![](./img/cfcd208495d565ef66e7dff9f98764da.svg)，特征权重的稀疏性由此产生。根据上式，可以设计出L1-RDA的算法逻辑<br />![WX20210225-154400.png](./img/1614239051571-8cd848d7-bfd8-437f-be5e-4e661ab527fb.png)

<a name="jzwk2"></a>
### L1-RDA与L1-Forward-Backward Splitting的比较
我们之前分析出L1-Forward-Backward Splitting实际上是TG的一种特殊形式，在L1-Forward-Backward Splitting中，进行“截断”的判定条件是![](./img/3154a16824e7e1901c04efe9404338f0.svg)。通常会定义![](./img/ffe9f913124f345732e9f00fa258552e.svg)为与![](./img/52d0a18851642b0a75d51116866cae3e.svg)正相关的函数（![](./img/f359dda7d289a4156fc6170a81c1c310.svg)），因此L1-Forward-Backward Splitting的“截断阈值”为![](./img/64130a6a775120f7a347506c136d4960.svg)，随着![](./img/e358efa489f58062f10dd7316b65649e.svg)的增加，这个阈值会逐渐降低。

相较而言，L1-RDA的“截断阈值”为![](./img/c6a6eb61fd9c6c913da73b3642ca147d.svg)，是一个常数，并不随着![](./img/e358efa489f58062f10dd7316b65649e.svg)而变化，因此可以认为L1-RDA比L1-Forward-Backward Splitting在截断判定上更加aggressive，这种性质使得L1-RDA更容易产生稀疏性；此外，RDA中判定对象是梯度的累加平均值![](./img/7ebdd73d8ee949f533789c874ba5d905.svg)，不同于TG或L1-Forward-Backward Splitting中针对单次梯度计算的结果进行判定，避免了由于某些维度由于训练不足导致截断的问题。并且通过调节![](./img/c6a6eb61fd9c6c913da73b3642ca147d.svg)一个参数，很容易在精度和稀疏性上进行权衡。

<a name="ju4mQ"></a>
## FTRL
有实验证明，L1-Forward-Backward Splitting这一类基于梯度下降的方法有比较高的精度，但是L1-RDA却能在损失一定精度的情况下产生更好的稀疏性。那么这两者的优点能不能在一个算法上体现出来？这就是FTRL 要解决的问题。

FTRL（Follow the Regularized Leader）是由 Google 的 H. Brendan McMahan 在 2010 年出的，后来在 2011 年发表了一篇关于 FTRL 和 AOGD、Forward-Backward Splitting、RDA 比较的论文，2013年又和 Gary Holt, D. Sculley, Michael Young 等人发表了一篇关于 FTRL 工程化实现的论文。

<a name="zF654"></a>
### FTRL算法原理
FTRL综合考虑Forward-Backward Splitting和RDA对于正则项和![](./img/61e9c06ea9a85a5088a499df6458d276.svg)限制的区别，其特征权重的更新公式为：

![](./img/d69f3e5128bdf25b1fea3384808199bb.svg)

注意，上式出现了L2正则项![](./img/44419b1de906dc3ed85c7f762c0ee26b.svg)，在论文公式中并没有这一项，但在2013年发表的FTRL工程化实现的论文中却用了L2正则项。事实上该项的引入并不影响FTRL的稀疏性，L2正则项的引入仅仅相当于对最优化过程多了一个约束，使得结果求解结果更加“平滑”。

公式看上去很复杂，更新特征权重貌似非常困难的样子。不妨将其进行改写，将最后一项展开，等价于求下面这样一个最优化问题：

![](./img/d9643d9b9b5c9f7a4c4873e0a569b5aa.svg)

由于![](./img/0f7a2afc99e7a998945f5dd0c1e596fb.svg)相对于![](./img/61e9c06ea9a85a5088a499df6458d276.svg)来说是一个常数，并且令![](./img/1223af478977a65c3ef8adc530cccc85.svg)，上式等价于：

![](./img/5a3f35605d9b539a287359a3020e5de7.svg)

针对特征权重的各个维度将其拆解成![](./img/8d9c307cb7f3c4a32822a51922d1ceaa.svg)个独立的标量最小化问题：

![](./img/411390ff3e2b5dfdbe6be649ca8bf507.svg)

到这里，我们可得：

![](./img/79a8681e65a156c11eb84ac7404f0aa2.svg)

由上式可以看出，引入L2正则化并没有对FTRL结果的稀疏性产生任何影响。
<a name="tIbLi"></a>
### Per-Coordinate Learning Rates
前面介绍了FTRL的基本推导，但是这里还有一个问题是一直没有被讨论到的：关于学习率![](./img/130f3b9c26e9892cba611a7bde4d407f.svg)的选择和计算。事实上在FTRL中，每个维度上的学习率都是单独考虑的（Per-Coordinate Learning Rates）。

在一个标准的OGD里面使用的是一个全局的学习率策略![](./img/176d283ada1c65f4d822c18c2fd0b4f6.svg)，这个策略保证了学习率是一个正的非增长序列，对于每一个特征维度都是一样的。

考虑特征维度的变化率：如果特征1比特征2的变化更快，那么在维度1上的学习率应该下降的更快。我们很容易就可以想到可以用某个维度上梯度分量来反应这种变化率。在FTRL中，维度![](./img/865c0c0b4ab0e063e5caa3387c1a8741.svg)上的学习率是这样计算的：

![](./img/3fb6ec79f9efc6e11a3ab327815ef732.svg)

由于![](./img/75d02014115caea0871bbed84b115360.svg)，所以![](./img/79a8681e65a156c11eb84ac7404f0aa2.svg)中![](./img/c0cc2db9370436bf138e4c65de2f1712.svg)。这里的![](./img/7b7f9dbfea05c83784f8b85149852f08.svg)和![](./img/b0603860fcffe94e5b8eec59ed813421.svg)是需要输入的参数，学习率写成累加的形式，是为了方便理解后面FTRL的迭代计算逻辑。

<a name="GJ3Gg"></a>
### FTRL算法逻辑
到现在为止。我们已经得到了FTRL的特征权重维度的更新方法，每个特征维度的学习率计算方法，那么很容易写出FTRL的算法逻辑：<br />![WX20210301-110139.png](./img/1614567709751-ace4abf9-cb83-4ebf-a9ff-8d42017f6863.png)
<a name="LtdIu"></a>
# Source
在线最优化求解(Online Optimization)-冯扬<br />[https://zhuanlan.zhihu.com/p/61724627](https://zhuanlan.zhihu.com/p/61724627)

