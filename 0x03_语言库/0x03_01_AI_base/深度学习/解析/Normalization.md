Normalization，即标准化，和普通的数据标准化类似，是将分散的数据统一的一种做法，也是优化神经网络的一种方法。Normalization可以将数据统一规格，能让机器学习更容易学习到数据之中的规律。

如果我们仅仅停留在使用Normalization上，那么现成的框架只需要一行就可以添加到模型中。我们真正想知道的是，隐藏在BN的背后深度学习的问题，以及这样简单的操作是如何奏效的。

<a name="a41c56a0"></a>
## 深度学习的Normalization具体做什么

由于有多种Normalization的方法，如Batch Normalization（BN），Layer Norm（LN），Weight Norm（WN），Cosine Norm(CN)等，我这里以最经典的Batch Normalization为例说明Normalization到底是做的什么操作。需要知道的是，BN可以在激活函数![](./img/a2ab7d71a0f07f388ff823293c147d21.svg)之前，也可以在![](./img/a2ab7d71a0f07f388ff823293c147d21.svg)之后。有如下图的神经网络：

![Normalization1.png](./img/1595838725839-a4e3d9c1-e07d-4091-b842-4e3aebed3008.png)

我们定义![](./img/1f50926d4dbba9f96453fe3e74efee40.svg)：![](./img/cc8f515b68987304bace719208a77e17.svg)

因此有：![](./img/9c5032fcf6bb0617346597f4bfcb8ca2.svg)

我们做的BN，就是对![](./img/1f50926d4dbba9f96453fe3e74efee40.svg)进行标准化。假设Batch Size是1000，那么就会有1000个![](./img/1f50926d4dbba9f96453fe3e74efee40.svg)，我们对这1000个![](./img/1f50926d4dbba9f96453fe3e74efee40.svg)求标准化，使得这1000个值满足均值为0，标准差为1的高斯分布。然后进行激活，激活后的值，继续前向传播。这就是BN的过程，很简单。

我们再举一个例子，形象地说明做Normalization的好处。我们知道，在神经网络中，数据分布对训练会产生影响。比如某个神经元![](./img/9dd4e461268c8034f5c8564e155c67a6.svg)的值为![](./img/c4ca4238a0b923820dcc509a6f75849b.svg)，某个![](./img/61e9c06ea9a85a5088a499df6458d276.svg)的初始值为![](./img/cb5ae17636e975f9bf71ddf5bc542075.svg)，这样后一层神经元计算结果就是![](./img/a99c5e3f314d55934e81a8b4966d32be.svg)；又或者![](./img/e938dabd6a6ad9d59417ccd7ceb1a309.svg)，这样![](./img/34177b0374165f089e37401cbd15664f.svg)的结果就为![](./img/c81e728d9d4c2f636f067f89cc14862c.svg)。现在还不能看出什么问题，但是，当我们加上一层激励函数，激活这个![](./img/34177b0374165f089e37401cbd15664f.svg)值的时候，问题就来了。如果使用像tanh的激活函数，![](./img/34177b0374165f089e37401cbd15664f.svg)的激活值就变成了![](./img/9f6cf3924a9d3bc7838adc84fe017e5f.svg)和![](./img/597e2808db2635ed5d838580c04accb1.svg)，接近于![](./img/c4ca4238a0b923820dcc509a6f75849b.svg)的部分已经处在了激活函数的饱和阶段，也就是如果![](./img/9dd4e461268c8034f5c8564e155c67a6.svg)无论再怎么扩大，tanh激活函数输出值也还是接近![](./img/c4ca4238a0b923820dcc509a6f75849b.svg)。换句话说，神经网络在初始阶段已经不对那些比较大的![](./img/9dd4e461268c8034f5c8564e155c67a6.svg)特征范围敏感了。这样很糟糕，想象我轻轻拍自己的感觉和重重打自己的感觉居然没什么差别，这就证明我的感官系统失效了。当然，![](./img/9dd4e461268c8034f5c8564e155c67a6.svg)不仅可以是输入层，在隐含层也可能出现这样的情况。

通过下图我们可以看到BN的效果：

![Normalization2.png](./img/1595839331616-29a19c85-d5b2-413e-8e5f-62892deaa49a.png)<br />![Normalization3.png](./img/1595839354586-b8e50a4e-8d8c-46e8-8a67-ce4d7a408dcb.png)

之前说过，计算结果在进入激活函数前的值很重要，如果我们不单单看一个值，我们可以说，计算结果值的分布对于激活函数很重要。对于数据值大多分布在这个区间的数据，才能进行更有效的传递。对比这两个在激活之前的值的分布。上者没有进行 normalization，下者进行了 normalization，这样当然是下者能够更有效地利用 tanh 进行非线性化的过程。

没有 normalize 的数据使用 tanh 激活以后，激活值大部分都分布到了饱和阶段，也就是大部分的激活值不是![](./img/6bb61e3b7bce0931da574d19d1d82c88.svg)，就是![](./img/c4ca4238a0b923820dcc509a6f75849b.svg)。而 normalize 以后，大部分的激活值在每个分布区间都还有存在。再将这个激活后的分布传递到下一层神经网络进行后续计算，每个区间都有分布的这一种对于神经网络就会更加有价值。

我们需要知道得是，Batch normalization 不仅仅 normalize 了一下数据，它还进行了反 normalize，这个会在后面提到。

<a name="9d753e75"></a>
## 为什么深度学习中需要Normalization

上面论述中我们已经看到了Normalization的好处。这里我们深入解释一下为什么深度学习中需要Normalization。其中一个很重要的原因是在深度学习中会存在Internal Covariate Shift（ICS）。我们先从Covariate Shift说起，Covariate Shift是缘于统计学的一个概念，它描述了源域（![](./img/5dbc98dcc983a70728bd082d1a47546e.svg)）和目标域（![](./img/b9ece18c950afbfa6b0fdbfa4ff731d3.svg)）边缘分布的不一致，即![](./img/2d3a1fbde39ca1c2915e5dabd757cf63.svg)，但是他们的条件分布却是相同的，即![](./img/b4372815a637010e10d4c30f148fb2d1.svg)。

简单在机器学习中来说，从概率的视角，条件分布![](./img/fe9c25dd98e0e2b44985de7c0f39f53c.svg)是我们得到的模型，如果我们的训练集![](./img/9aac297f8f9259d0c629a33f956ce5b2.svg)分布与测试集的![](./img/473479452bcc53bf959736243504fc6c.svg)分布存在差异，那么就会出现Covariate Shift，此时会出现两个结果

- 我们利用从训练集得到的模型，去在测试集上做性能评估，得到的并不会是模型的真实水平。
- 训练集和测试集的分布差异太大，我们训练出的模型并不是真实的模型。

![Normalization4.jpg](./img/1595839654184-be5890bb-76b5-4b4f-937f-89d77c1fa2cc.jpeg)

如图，在样本空间中，红色点为训练集，黑色点为测试集，真实的拟合直线是黑线，而我们学习到的却是红线。

我们期望数据是“独立同分布”的，即independent and identically distributed，简称为 i.i.d. 独立同分布并非所有机器学习模型的必然要求（比如 Naive Bayes 模型就建立在特征彼此独立的基础之上，而Logistic Regression 和神经网络则在非独立的特征数据上依然可以训练出很好的模型），但独立同分布的数据可以简化常规机器学习模型的训练、提升机器学习模型的预测能力，已经是一个共识。

独立同分布要求训练集和测试集的样本都从同一个分布独立采样而来，这在理论上是一个强力的保证。但在实际过程中，我们无法做出完全的iid分布，我们一般会采用权重分布来参与学习，使得训练集和测试集分布差异较小的样本点来占据更大的权重。

最重要的是，所谓的标准化就是减小分布差异的一种方式，因为预处理会让每个特征服从标准高斯分布。我们在数据预处理的时候，往往会得到训练集的均值和标准差，并将其直接用在测试集上，这种信息共享的方式可以说是将独立同分布用到了极致。

但在深度学习中，这个现象加剧为Internal Covariate Shift。从表示学习的角度来看，神经网络前面的所有层，都可以看做获得一个更好的表示，隐藏单元的最大作用是非线性，使得神经网络在最基本的乘法中获得足够的复杂性，只有最后一层将表示转化为输出，所以只有最后一层可以看作统计学习中的学习器。

正因为前面所有的层都是在获得一个更好的表示，而非直接做学习，所以经过层和激活函数的处理，我们获得的还是![](./img/c19f6b6a7bae1fd5b14f578c6edc3454.svg)，而非![](./img/fe9c25dd98e0e2b44985de7c0f39f53c.svg)，而这样是非常有可能加剧Covariate Shift的程度（当然，也有可能减弱），这就是Internal Covariate Shift中Internal（内部的）的含义。

大家细想便会发现，的确，对于神经网络的各层输出，由于它们经过了层内操作作用，其分布显然与各层对应的输入信号分布不同，而且差异会随着网络深度增大而增大，可是它们所能“指示”的样本标记（label）仍然是不变的，这便符合了Internal Covariate Shift的定义。

<a name="c32dde14"></a>
## Normalization 的通用框架与基本思想

我们以神经网络中的一个普通神经元为例。神经元接收一组输入向量![](./img/7bf1d11f23bf1e99422581f60ee3ab96.svg)，在将![](./img/9dd4e461268c8034f5c8564e155c67a6.svg)送给神经元之前，先对其做平移和伸缩变换，将![](./img/9dd4e461268c8034f5c8564e155c67a6.svg)的分布标准化成在固定区间范围的标准分布。

通用变换框架就如下所示：

![](./img/a63eea1be544d3d08f69a8e71ef61e46.svg)

我们来看看这个公式中的各个参数。

（1）![](./img/c9faf6ead2cd2c2187bd943488de1d0a.svg)是平移参数(shift parameter)，![](./img/a2ab7d71a0f07f388ff823293c147d21.svg)是缩放参数(scale parameter)。通过这两个参数进行shift和scale变换：

![](./img/63d0033ca1f80d0e1adede56d807e236.svg)

得到的数据符合均值为![](./img/cfcd208495d565ef66e7dff9f98764da.svg)，方差为![](./img/c4ca4238a0b923820dcc509a6f75849b.svg)的标准分布。

（2）![](./img/92eb5ffee6ae2fec3ad71c777531578f.svg)是再平移参数(re-shift parameter)，![](./img/b2f5ff47436671b6e533d8dc3614845d.svg)是再缩放参数(re-scale parameter)。将上一步得到的![](./img/2a95aaaf954c2187999c6357b04a58dd.svg)再变换：

![](./img/9189705147f957a49e52fb8c89bf7438.svg)

最终得到的数据符合均值为![](./img/92eb5ffee6ae2fec3ad71c777531578f.svg)，方差为![](./img/cd14f203f7fa35bf9bcb2abd0bf82247.svg)的分布。

可能有些读者会有疑问，第一步都已经得到了标准分布，第二步怎么又给变走了？答案是——为了保证模型的表达能力不因为标准化而下降。

我们可以看到，第一步的变换将输入数据限制到了一个全局统一的确定范围（均值为![](./img/cfcd208495d565ef66e7dff9f98764da.svg)、方差为![](./img/c4ca4238a0b923820dcc509a6f75849b.svg)）。下层神经元可能很努力地在学习，但不论其如何变化，其输出的结果在交给上层神经元进行处理之前，将被粗暴地重新调整到这一固定范围。

所以，为了尊重底层神经网络的学习结果，我们将标准化后的数据进行再平移和再缩放，使得每个神经元对应的输入范围是针对该神经元量身定制的一个确定范围（均值为![](./img/92eb5ffee6ae2fec3ad71c777531578f.svg)、方差为![](./img/cd14f203f7fa35bf9bcb2abd0bf82247.svg)）。rescale和reshift的参数都是可学习的，这就使得Normalization层可以学习如何去尊重底层的学习结果。

除了充分利用底层学习的能力，另一方面的重要意义在于保证获得非线性的表达能力。Sigmoid等激活函数在神经网络中有着重要作用，通过区分饱和区和非饱和区，使得神经网络的数据变换具有了非线性计算能力。而第一步的标准化会将几乎所有数据映射到激活函数的非饱和区（线性区），仅利用到了线性变化能力，从而降低了神经网络的表达能力。而进行再变换，则可以将数据从线性区变换到非线性区，恢复模型的表达能力。

那么问题又来了——经过这么的变回来再变过去，会不会跟没变一样？

不会。因为，再变换引入的两个新参数![](./img/92eb5ffee6ae2fec3ad71c777531578f.svg)和![](./img/b2f5ff47436671b6e533d8dc3614845d.svg)，可以表示旧参数作为输入的同一族函数，但是新参数有不同的学习动态。在旧参数中，![](./img/9dd4e461268c8034f5c8564e155c67a6.svg)的均值取决于上一层神经网络的复杂关联；但在新参数中，![](./img/52e6c607ff079d72778c519a2d451b52.svg)仅由![](./img/92eb5ffee6ae2fec3ad71c777531578f.svg)来确定，去除了与上一层计算的密切耦合。简单来说，原始的![](./img/c9faf6ead2cd2c2187bd943488de1d0a.svg)和![](./img/a2ab7d71a0f07f388ff823293c147d21.svg)将输入数据限制到了一个均值为![](./img/cfcd208495d565ef66e7dff9f98764da.svg)、方差为![](./img/c4ca4238a0b923820dcc509a6f75849b.svg)的范围，这太严格了，会使得模型的表达能力下降，现在的![](./img/b2f5ff47436671b6e533d8dc3614845d.svg)和![](./img/92eb5ffee6ae2fec3ad71c777531578f.svg)将输入数据限制到了一个均值为![](./img/92eb5ffee6ae2fec3ad71c777531578f.svg)、方差为![](./img/cd14f203f7fa35bf9bcb2abd0bf82247.svg)的范围，既使得数据在一定的范围内，缓解了ICS的问题，又在一定程度上保证了模型的表达能力。

<a name="90a0e135"></a>
## 主流 Normalization 方法梳理

对照下面公式，我们来梳理主流的四种标准化方法：

![](./img/a63eea1be544d3d08f69a8e71ef61e46.svg)

<a name="26d6e601"></a>
### Batch Normalization(纵向标准化)

Batch Normalization 其标准化针对单个神经元进行，利用网络训练时一个 mini-batch 的数据来计算该神经元![](./img/5a5ae0760dc3dac91e546c0ea25586b0.svg)的均值和方差，因而称为 Batch Normalization。符号与前面的符号定义相同。

![](./img/6db2c1964c9749a78366e552becec160.svg)<br />![](./img/0321d3937b35736799a69ff70592270f.svg)

其中![](./img/69691c7bdcc3ce6d5d8a1361f22d04ac.svg)是mini-batch的大小。

如果把一层神经元看成是水平排列，BN 可以看做一种纵向的标准化，即 mini-batch 个数据叠在一起进行的标准化。由于 BN 是针对单个神经元定义的，因此标准公式中的计算均为 element-wise 的。

BN 独立地标准化每一个神经元的![](./img/5a5ae0760dc3dac91e546c0ea25586b0.svg)，但标准化的参数是一个 mini-batch 的均值和标准差。这就要求 每一个 mini-batch 的统计量是整体统计量的近似估计，或者说每一个 mini-batch 彼此之间，以及和整体数据，都应该是近似同分布的。如果每个 mini-batch的原始分布差别很大，那么不同 mini-batch 的数据将会进行不一样的数据变换，这就增加了模型训练的难度。

因此，BN 比较适用的场景是：每个 mini-batch 比较大，数据分布比较接近。在进行训练之前，要做好充分的 shuffle。否则效果会差很多。

<a name="27ea47b4"></a>
### Layer Normalization(横向标准化)

层标准化就是针对 BN 的上述不足而提出的。与 BN 不同，LN 是一种横向的标准化，即对一层的神经元进行的标准化。它综合考虑一层所有神经元的输入，计算该层的平均输入值和输入方差，然后用同一个标准化操作来转换各个神经元的输入。

![](./img/828afd9f39bfe4dee0340a0b6f79bb61.svg)<br />![](./img/39598974814cd927170b53706364606c.svg)

其中![](./img/865c0c0b4ab0e063e5caa3387c1a8741.svg)枚举了该层所有的输入神经元。对应到标准公式![](./img/a63eea1be544d3d08f69a8e71ef61e46.svg)中，四大参数![](./img/c9faf6ead2cd2c2187bd943488de1d0a.svg)，![](./img/a2ab7d71a0f07f388ff823293c147d21.svg)，![](./img/92eb5ffee6ae2fec3ad71c777531578f.svg)，![](./img/b2f5ff47436671b6e533d8dc3614845d.svg)均为标量（BN中是向量），是所有输入共享一个标准化变换。

LN 针对单个训练样本进行，不依赖于其他数据，因此可以避免 BN 中受 mini-batch 数据分布影响的问题，可以用于 小mini-batch场景、动态网络场景和 RNN，特别是自然语言处理领域。此外，LN 不需要保存 mini-batch 的均值和方差，节省了额外的存储空间。

但是，BN 的转换是针对单个神经元可训练的——不同神经元的输入经过再平移和再缩放后分布在不同的区间，而 LN 对于一整层的神经元训练得到同一个转换——所有的输入都在同一个区间范围内。如果不同输入特征不属于相似的类别（比如颜色和大小），那么 LN 的处理可能会降低模型的表达能力。

<a name="aa2d6260"></a>
### Weight Normalization(参数标准化)

前面我们说到模型框架![](./img/a63eea1be544d3d08f69a8e71ef61e46.svg)中，最普遍的变换是线性变换，即![](./img/ce0f17144a4c310042d309663aa43313.svg)。这里特别需要注意的是：![](./img/b81a39f8a7f169bf5f01a699975846ee.svg)中的![](./img/9dd4e461268c8034f5c8564e155c67a6.svg)是已经激活后的值，而公式![](./img/a63eea1be544d3d08f69a8e71ef61e46.svg)是激活前的值，这是两个不同的![](./img/9dd4e461268c8034f5c8564e155c67a6.svg)，或者说公式![](./img/a63eea1be544d3d08f69a8e71ef61e46.svg)的![](./img/9dd4e461268c8034f5c8564e155c67a6.svg)先进行标准化，再进行激活，就形成了![](./img/b81a39f8a7f169bf5f01a699975846ee.svg)中的![](./img/9dd4e461268c8034f5c8564e155c67a6.svg)。

BN和LN均将标准化应用于输入的特征数据![](./img/9dd4e461268c8034f5c8564e155c67a6.svg)，而WN则另辟蹊径，将标准化应用于线性变换函数的权重![](./img/f1290186a5d0b1ceab27f4e77c0c5d68.svg)，这就是WN名称的来源。

具体而言，WN提出的方案是，将权重向量![](./img/f1290186a5d0b1ceab27f4e77c0c5d68.svg)分解为向量方向![](./img/c6f3f6d15cd187c3008aec7e4d2cc386.svg)和向量模![](./img/b2f5ff47436671b6e533d8dc3614845d.svg)两部分：

![](./img/afe0fb699ed8eb70ae11481bca75e2e2.svg)

其中![](./img/9e3669d19b675bd57058fd4664205d2a.svg)是与![](./img/f1290186a5d0b1ceab27f4e77c0c5d68.svg)同维度的向量，![](./img/e9b7de66fd0147a62978e0ab6922d4cb.svg)是二范数，因此![](./img/c6f3f6d15cd187c3008aec7e4d2cc386.svg)是单位向量，决定了![](./img/f1290186a5d0b1ceab27f4e77c0c5d68.svg)的方向；![](./img/b2f5ff47436671b6e533d8dc3614845d.svg)是标量，决定了![](./img/f1290186a5d0b1ceab27f4e77c0c5d68.svg)的长度。由于![](./img/71046b2d19b079cf3ecb8ac7b1c76a35.svg)(恒等于)，因此这一权重分解的方式将权重向量的欧式范数进行了固定，从而实现了正则化的效果。

乍一看，这一方法似乎脱离了我们前文所讲的通用框架![](./img/a63eea1be544d3d08f69a8e71ef61e46.svg)？

并没有。其实从最终实现的效果来看，异曲同工。我们来推导一下看。

![](./img/4bcba5019f02c2ecb2f45c292c849503.svg)<br />![](./img/c792cbcf9489058a8189f87fd0877d59.svg)

对照一下前述框架

![](./img/a63eea1be544d3d08f69a8e71ef61e46.svg)

我们只需令![](./img/eb98e6b328b3b57183d6914558786f5f.svg)，![](./img/3950a2beed8f89819daa872126e7ff5d.svg)，![](./img/f6d5eef5ee5e51fc839bb54201c62e3b.svg)，就完美地对号入座了。

回忆一下，BN 和 LN 是用输入的特征数据的方差对输入数据进行scale，而 WN 则是用神经元的权重的欧氏范式对输入数据进行 scale。虽然在原始方法中分别进行的是特征数据标准化和参数的标准化，但本质上都实现了对数据的标准化，只是用于 scale 的参数来源不同。

另外，我们看到这里的标准化只是对数据进行了scale，而没有进行shift，因为我们简单地令![](./img/64105bcc4e5b87e14d6ed0e44569013e.svg)。但事实上，这里留下了与 BN 或者 LN 相结合的余地——那就是利用 BN 或者 LN 的方法来计算输入数据的均值![](./img/c9faf6ead2cd2c2187bd943488de1d0a.svg)。

WN 的标准化不直接使用输入数据的统计量，因此避免了 BN 过于依赖 mini-batch 的不足，以及 LN 每层唯一转换器的限制，同时也可以用于动态网络结构。

<a name="7760e281"></a>
### Cosine Normalization(余弦标准化)

Normalization还能怎么做？我们再来看看神经元的经典变换![](./img/130339598eedf8ebc874cf5a026b9f0c.svg)。对输入数据![](./img/9dd4e461268c8034f5c8564e155c67a6.svg)的变换已经做过了，横着来是 LN，纵着来是 BN。对模型参数![](./img/f1290186a5d0b1ceab27f4e77c0c5d68.svg)的变换也已经做过了，就是 WN。好像没啥可做的了

然而天才的研究员们盯上了中间的那个点，对，就是![](./img/571ca3d7c7a5d375a429ff5a90bc5099.svg)。他们说，我们要对数据进行标准化的原因，是数据经过神经网络的计算之后可能会变得很大，导致数据分布的方差爆炸，而这一问题的根源就是我们的计算方式——点积，权重向量![](./img/f1290186a5d0b1ceab27f4e77c0c5d68.svg)和特征数据向量![](./img/9dd4e461268c8034f5c8564e155c67a6.svg)的点积。向量点积是无界的啊！

那怎么办呢？我们知道向量点积是衡量两个向量相似度的方法之一。哪还有没有其他的相似度衡量方法呢？有啊，很多啊！夹角余弦就是其中之一啊！而且关键的是，夹角余弦是有确定界的啊，![](./img/d060b17b29e0dae91a1cac23ea62281a.svg)的取值范围，多么的美好！

于是，Cosine Normalization就出世了。他们不处理权重向量![](./img/f1290186a5d0b1ceab27f4e77c0c5d68.svg)，也不处理特征数据向量![](./img/9dd4e461268c8034f5c8564e155c67a6.svg) ，就改了一下线性变换的函数：

![](./img/3b2cb79c0e6fd29c0466ff0d7086d868.svg)

其中![](./img/2554a2bb846cffd697389e5dc8912759.svg)是![](./img/f1290186a5d0b1ceab27f4e77c0c5d68.svg)和![](./img/9dd4e461268c8034f5c8564e155c67a6.svg)的夹角。然后就没有然后了，所有的数据都是![](./img/d060b17b29e0dae91a1cac23ea62281a.svg)区间范围之内了。

不过，回过头来看，CN 与 WN 还是很相似的。我们看到上式中，分子还是![](./img/f1290186a5d0b1ceab27f4e77c0c5d68.svg)和![](./img/9dd4e461268c8034f5c8564e155c67a6.svg)的内积，而分布则可以看做用![](./img/f1290186a5d0b1ceab27f4e77c0c5d68.svg)和![](./img/9dd4e461268c8034f5c8564e155c67a6.svg)二者的模之积进行标准化。对比一下WN的公式：

![](./img/0244945a4fca05d36cff5e328bff0f00.svg)

一定程度上可以理解为，WN 用权重的模![](./img/e9b7de66fd0147a62978e0ab6922d4cb.svg)对输入向量进行 scale，而 CN 在此基础上用输入向量的模![](./img/62dfc1d87f0f4946a5c557ce60a7d4fe.svg)对输入向量进行了进一步的scale。

CN 通过用余弦计算代替内积计算实现了标准化，但是这其中又一些隐患。原始的内积计算，其几何意义是输入向量在权重向量上的投影，既包含二者的夹角信息，也包含两个向量的scale信息。去掉scale信息，可能导致表达能力的下降，因此也引起了一些争议和讨论。具体效果如何，可能需要在特定的场景下深入实验。

<a name="08af45d8"></a>
## Normalization 为什么会有效

<a name="5c317e32"></a>
### 不使用Batch Normalization

1、首先，对某层的前向传播过程有：

![](./img/6f1d8802c93a56c352fa41a696959e94.svg)

2、针对该层的反向传播过程为（由于我们关心的是梯度的连续反向传播过程，故不关注权重的梯度）：

![](./img/5afd9f63d3e23e1c07ce0b4be55d344a.svg)

3、进一步推导可得，连续多层的梯度反向传播过程为：

![](./img/8ec4d0f50629bebe12ee381c24b1ecec.svg)

由此我们可以初步看出，在梯度的连续反向传播过程中，是通过权重![](./img/b0e0273c1caa597ede00c58a94473ace.svg)的连乘进行的。因此，如果权重![](./img/b0e0273c1caa597ede00c58a94473ace.svg)的值总是较小的（广义上与![](./img/c4ca4238a0b923820dcc509a6f75849b.svg)相比），则在反向过程中，梯度呈指数级衰减，就出现了梯度消失的问题；反之，如果如果权重 总是较大，则相应的就会出现梯度爆炸的问题。结论就是，在反向传播过程中，权值![](./img/b0e0273c1caa597ede00c58a94473ace.svg)的大小会极大的影响梯度的有效传播，而在训练过程中，权重并不总是受人为控制的。因此，我们有必要在一定程度上限制甚至消除权值![](./img/b0e0273c1caa597ede00c58a94473ace.svg)对梯度反向传播的不良影响，BN就可以起到这么一个作用。

<a name="f8f3852c"></a>
### 使用Batch Normalization

1、带有BN的前向传播过程如下所示：

![](./img/efbae2a304c3809f93afa6a2025b6e7b.svg)

其中![](./img/1f1075171f902477316871750e2cffff.svg)为向量，![](./img/42cb6558c85d7a226fe9873e846b681f.svg)为对角矩阵![](./img/68cd9355223b23ff1c21999fd841ccf0.svg)

2、则其反向传播有：

![](./img/4ca3e7989d2feedf3cd476be92af2d8a.svg)

3、相应的，连续多层的梯度反向传播过程为：

![](./img/1b78244626abf245f012edad52e98b36.svg)

可以看出，与不使用BN相比，每层的反向传播过程的，增加了一个基于标准差的矩阵![](./img/f993754cedafc784a858ecf16e9e3cb2.svg)对权重![](./img/b0e0273c1caa597ede00c58a94473ace.svg)进行缩放。这样的缩放能够产生什么效果？让我们分析一下，如果权重![](./img/b0e0273c1caa597ede00c58a94473ace.svg)较小，那必然![](./img/2912730daaaa7c1f28c14a1f838c5d2d.svg)较小，从而使得其标准差![](./img/5e96fb0462d53646ebc51eac65b738e1.svg)较小，相对的![](./img/f993754cedafc784a858ecf16e9e3cb2.svg)较大，所以![](./img/1bce23f7d6a854850d1da4adeb2f1b60.svg)相对于原本的![](./img/b0e0273c1caa597ede00c58a94473ace.svg)就放大了，避免了梯度的衰减；同样的，如果权重![](./img/b0e0273c1caa597ede00c58a94473ace.svg)较大，可以很容易得到![](./img/1bce23f7d6a854850d1da4adeb2f1b60.svg)相对于原本的![](./img/b0e0273c1caa597ede00c58a94473ace.svg)缩小了，避免了梯度的膨胀。于是，加入了BN的反向传播过程中，就不易出现梯度消失或梯度爆炸，梯度将始终保持在一个合理的范围内。而这样带来的好处就是，基于梯度的训练过程可以更加有效的进行，即加快收敛速度，减轻梯度消失或爆炸导致的无法训练的问题。

一句话总结Normalization 为什么会有效：就是BN解决了反向传播过程中的梯度问题（梯度消失和爆炸），同时使得不同scale的![](./img/61e9c06ea9a85a5088a499df6458d276.svg)整体更新步调更一致。

<a name="2cd28d6b"></a>
## Batch Normalization的分析

总的来说，Batch Normalization有以下特点：

- 随着网络层数的加深，BN的效果更加显著。
- BN可以改善梯度流，解决在网络训练过程中梯度消失的问题。
- BN可以减轻过拟合，在一定程度上起到![](./img/11cc37d6dae6b905a0a0eb2ff087ae24.svg)正则的作用。

<a name="1c48c348"></a>
### 对前向传播影响

Batch Normalization在前向传播时有三个主要任务：

- 计算出每批训练数据的统计量
- 对数据进行标准化
- 对标准化后的数据进行扭转，将其映射到表征能力更大的空间上

<a name="ffbf7e2d"></a>
#### Batch Normalization在Mini-Batch上的变换算法

![Normalization5.png](./img/1595843115322-4051c1cd-539e-4fda-8741-6fb76b035909.png)

<a name="c0131da5"></a>
#### 1、从Mini-Batch计算均值与方差

SGD通过最小化以下损失函数来求解神经网络的最优值：

![](./img/5665c69f66a0083d6e93c99bc463a668.svg)

其中![](./img/b9dce96eb3d5a71b28f9f198c28d2d1b.svg)是神经网络中的参数；![](./img/de2ac793288429334b5ee7446c981e26.svg)是训练数据集。

为了综合随机梯度下降和批量梯度下降这两种算法的优点，Mini-Batch梯度下降在单个样本迭代和全部样本迭代之间找到了一个折中点，既加快了参数的迭代速度，也避免了单个样本数据带来的波动性。当Mini-Batch中的数据量为![](./img/6f8f57715090da2632453988d9a1501b.svg)时，可以通过如下公式计算出梯度：

![](./img/c9f353e2c9749997c60ba3c4da374719.svg)

其中![](./img/ffe9f913124f345732e9f00fa258552e.svg)为学习率。

可以证明，通过小批量样本计算出的梯度可以正确地表示全部训练数据的梯度，平切没皮的数据量越大，样本梯度越接近于总体梯度。另外，得益于现代平行计算技术，使用Mini-Batch比![](./img/6f8f57715090da2632453988d9a1501b.svg)词单样本的计算效率更高，因此比原始的随机梯度下降方法更快。

<a name="ff910c67"></a>
#### 2、从批量样本推断总体均值与方差

网络模型在训练阶段与测试阶段使用的统计量是不同的。训练时，每个batch使用本批的统计量来进行标准化，测试时则需要总体的统计量对数据进行转换。总体统计量可以通过如下公式从每批的样本统计量的移动平均数中推断出来。值得注意的是，推断方差时需要加上![](./img/4e5de69e540ac7e6331d721e3abf45d8.svg)的校正，因为样本方差均值的![](./img/37a674ac7f3ba341bb09d3400486db9a.svg)倍才是总体方差的无偏估计。

对于多个Mini-Batch的训练集![](./img/02129bb861061d1a052c592e2dc6b383.svg)，每个batch大小为![](./img/6f8f57715090da2632453988d9a1501b.svg)：

总体均值：![](./img/5480340cff8021d124436027a4cc7c37.svg)    总体方差：![](./img/517507dd254911a3777b2a06d46e5bba.svg)

<a name="29bfa34f"></a>
#### 3、数据标准化

数据标准化又叫作数据归一化，是数据挖掘过程中常用的数据预处理方式。当我们使用真实世界中的数据进行分析时，会遇到两个问题：

- 特征变量之间的量纲单位不同
- 特征变量之间的变化尺度(scale)不同

特征变量的尺度不同导致参数尺度规模也不同，带来的最大问题就是在优化阶段，梯度变化会产生震荡，减慢收敛速度。经过标准化的数据，各个特征变量对梯度的影响变得统一，梯度的变化会更加稳定。

![Normalization6.png](./img/1595844548246-b69a7817-4667-4b51-81e5-7ec50ca13cb4.png)

总结起来，数据标准化有以下三个优点：

- 数据标准化能够是数值变化更稳定，从而使梯度的数量级不会变化过大。
- 在某些算法中，标准化后的数据允许使用更大的步长，以提高收敛地速度。
- 数据标准化可以提高被特征尺度影响较大的算法的精度，比如k-means、kNN、带有正则的线性回归。

<a name="dcc9cb87"></a>
#### 4、缩放与偏移(Scale-Shift)

标准化后的数据还需要进行一定的缩放与偏移等变换，即在标准化后的数据上放大或缩小一定的比例并向上或向下平移一定的距离，变换公式为![](./img/e0cbd01c04440111b11cedf493c64616.svg)。

<a name="ff8efc69"></a>
### 对后向传播影响

<a name="01080605"></a>
#### 1、后向传播过程

后向传播是一种在训练时与梯度下降方法结合使用的优化算法，是训练人工神经网络的常用方法。该方法计算网络中所有权重的损失函数的梯度。计算出的梯度会被送给优化方法，优化方法又使用梯度来更新权重，以试图使损失函数最小化。

通过前面的介绍可知，前向传播可以分为三步：将数据标准化、对数据进行线性偏移、将数据输出到下一层，即![](./img/611b3d6af492f9c09b8231986abe5004.svg)（其中，![](./img/2a95aaaf954c2187999c6357b04a58dd.svg)是标准化后的输入，![](./img/415290769594460e2e485922904f345d.svg)是![](./img/2a95aaaf954c2187999c6357b04a58dd.svg)的线性变换，![](./img/2db95e8e1a9267b7a1188556b2013b33.svg)代表BN的下一层）。同样地，反向传播就是按照相反地三步传递误差，即![](./img/2e5df5361b188ca7d0be8bd6220a38ab.svg)。所以求导得过程也依次为![](./img/41cd1c7813771eefa6f07e6f7af0ffe4.svg)。如下图所示：<br />![Normalization7.png](./img/1595844595455-187dbcd3-a6ab-4785-855a-c5e889ca7199.png)

<a name="45c6a7f5"></a>
#### 2、梯度的推导

以下是链式法则的基本公式。

假设![](./img/465d3c939d696a35c998438b1c56f299.svg)，![](./img/23ea0bb75c1169d4bbfdaeba0e36bd3a.svg)，![](./img/7036d08d6b27e98738abd27d8418e631.svg)，则![](./img/f02786a46d5bfb69ce4e853d0d8be6d7.svg)

假设从下一层产生的误差为![](./img/2db95e8e1a9267b7a1188556b2013b33.svg)，则本层产生的误差为![](./img/68dc0438c08697998e900d8ea488a765.svg)

（1）计算![](./img/863ec328e9c26dc6559d272d55b7a841.svg)，![](./img/011911cc0d36dec0d131dc9da4c892f1.svg)，![](./img/6ea0970f8922f796adf7e15aec902130.svg)

- （a）![](./img/87ad1f3c0b5111bb018eba4ecb86f7c4.svg)
- （b）![](./img/bf89b0dc1826a7db748e9a39b81d994c.svg)
- （c）![](./img/443a5ac54d928b7189ce8735fc9be125.svg)

（2）计算![](./img/79892b1d076357a46086a174973c2691.svg)，![](./img/022a1d4b4b3eb527464d83bbf4afeef5.svg)，![](./img/8ed5634120684319112848d387eb1468.svg)

- （a）![](./img/ed8c389690e2e75bc3ac6ebb63e66abd.svg)，其中![](./img/d22760453b52098b2017a7218498d887.svg)<br />（b）![](./img/77bb9d55ba92b51e08721262843783d8.svg)，其中![](./img/84803e05cae24aeaaa095772eef4c5b4.svg)，![](./img/1c11e77006c3a17f66e7458b6e461d89.svg)，![](./img/ff612767547bce993017ce81c33d2b89.svg)，![](./img/4da240802de253fa3d9bd2cea79d6188.svg)
- （c）![](./img/401840511debc38e1747d9f85219425b.svg)，其中![](./img/e2d79ae0220325054222f2ba3ff26e7c.svg)，![](./img/6e89f6f12d4b3731e855bc2c66154fa3.svg)，![](./img/cfab5d985ce05d3bb35725c5e5c9c35a.svg)

<a name="cdec0301"></a>
### 有效性分析

从前面的算法介绍中可以看出，BN的原理十分简单，就是对每批的训练数据进行标准化，再做适当的线性变换，但是却可以显著提高深度神经网络的训练效果。一方面，BN可以有效地减轻神经网络的内部协移(Internal Covariate Shift)；另一方面，BN可以有效地改善训练过程中的梯度流的变化，解决梯度消失的问题，加快收敛。同时，BN也在一定程度上起到了![](./img/11cc37d6dae6b905a0a0eb2ff087ae24.svg)正则的作用。

<a name="107c4fab"></a>
#### 1、内部协移

内部协移(Internal Covariate Shift)是由于神经网络中每层的输入发生了变化，造成每层的参数要不断地适应新分布的问题。传统的Covariate Shift问题是指经过学习系统后的输入数据的分布发生改变，是典型的迁移学习的问题。Internal Covariate Shift与其相似，数据分布变化的来源从学习系统变成神经网络层。假设有一个两层的神经网络模型，第一层的映射函数是![](./img/bc6b0efd3bed4dfabe15757cf4089d87.svg)，第二层的映射函数是![](./img/adfa0c88ec236f64b0c078015d65db2b.svg)，由此得到的输出为：![](./img/fa1991cd3ad973284d14eb3163d54f97.svg)。于是，我们可以得到梯度下降的公式

![](./img/7f04e2d3d7c1978d35d6cdf81d5554e0.svg)

其中，![](./img/c125e133e9d2c5c43d482b270c0da97d.svg)，![](./img/6f8f57715090da2632453988d9a1501b.svg)依然是批量数据的大小，![](./img/7b7f9dbfea05c83784f8b85149852f08.svg)是学习率。

可以看出，经过两层神经网络之后，数据已经发生很复杂的变化，这将导致每层神经元的参数都要不断地调整适应这种输入数据分布的变化，不仅使网络的收敛速度变慢，也使得每层超参数的设定变得更加复杂

BN可以在数据经过多层神经网络后，重新回到均值为![](./img/cfcd208495d565ef66e7dff9f98764da.svg)、方差为![](./img/c4ca4238a0b923820dcc509a6f75849b.svg)的分布上，解决了以上问题，使数据的变化分布变得稳定，训练过程也随之变得平稳，超参数的调整变得简单。

<a name="c843581c"></a>
#### 2、梯度流

梯度流(Gradient Flow)，是指梯度按照最陡峭的路径逐渐减小的流动变化，在梯度下降方法中用来描述梯度的变化过程。

（1）梯度去哪了

深度神经网络主要是通过反向传播算法来寻找最优解的，即将梯度从损失函数层向各层反向传递。在传递过程中，如果梯度没有稳定地下降，就有可能导致产生梯度爆炸或梯度消失的现象。这个问题是由于梯度通过多层神经网络传递导致最后一层产生级数积累的结果。假设网络模型中的每一层接收的梯度都是上一层的![](./img/a5f3c6a11b03839d46af9fb43c97c188.svg)倍，那么![](./img/d20caec3b48a1eef164cb4ca81ba2587.svg)层神经网络将会产生相对原有输入![](./img/796053d3ae29414bed4e0968f29c1aef.svg)倍的变化，当![](./img/88a6a1e565fb4d11cd9693bb78dd67be.svg)时，最后一层的输入会非常大；而当![](./img/1a918e3df04a5cb68468f0bfa0f2d379.svg)时，最后一层的输入会变得非常小，比如![](./img/9fdde496920f10bb14b18f8ae53bb9d8.svg)。所以经过多层的影响后，前面的神经网络层接收到梯度可能会趋近于零，消失了。

传统的网络模型有三种方法来解决梯度消失的问题：

（a）使用ReLU激活函数。Sigmoid函数在数值过大或过小时都会进入梯度饱和区域，而且梯度的计算![](./img/5e858af0bf8d1d2b39dfed653dcda09b.svg)衰减得特别快。相对地，ReLU采用分段激活的方法，令参数结果变得稀疏，在激活阶段，梯度稳定为![](./img/c4ca4238a0b923820dcc509a6f75849b.svg)，有助于网络模型的收敛。不过ReLU存在两个问题：

- 网络模型的结果并不是越稀疏越好，过于稀疏的参数会导致欠拟合
- ReLU可能会过早地关闭一些输入的神经元，使其得不到更新

（b）仔细地初始化与调试参数。参数的初始化选择对模型训练的影响很大，适合的初始值可以使模型较快地收敛到理想的结果。但由于深度学习理论尚处于快速发展的阶段，很多深度神经网络模型的实际应用没有有效的理论解释，参数的初始化与调试在一定程度上依赖于经验，仍属于复杂的黑盒问题。

（c）使用较小的学习率。当使用Sigmoid激活时，较大的学习率会使权重参数变大，导致层间输入变大，较大的数值位于激活函数的饱和区域，从而使梯度趋近于零。另外，学习率属于网络模型中的超参数，完全依靠外部设置，也会带来其他的影响。过大的学习率会使梯度产生过多的抖动，无法收敛；而较小的学习率会减少参数更新的幅度，拖慢收敛速度。当使用ReLU激活时，过大的学习率还会使模型在训练初期产生梯度爆炸，也完全无法收敛。

（2）Batch Normalization带来更好的梯度流

与之前提及的数据标准化的道理类似，BN能够减少训练时每层梯度的变化幅度，使得梯度稳定在理想的变化范围内，从而改善梯度流，产生以下收益：

- BN能够减少梯度对参数的尺度或初始值的依赖，使得调参更加容易。
- BN允许网络接受更大的学习率，这是因为![](./img/b11d701d3a1bf5f596354417b4adc2e4.svg)，所以学习率的尺度不会明显影响所产生的梯度的尺度。
- 由于梯度流的改善，模型能够更快地达到较高的精度。

<a name="db6fac53"></a>
### 使用与优化方法

BN并不是一种新的神经网络模型，它是与神经网络模型结合使用的数据处理方式。在实际应用时，可以独立模块的方式灵活嵌入到神经网络的各层之间，Caffe的官方版本采用的就是这种方式。在与卷积层结合时，BN层一般置于卷积层与激活函数之间。

为了最大的发挥BN的优势，在使用BN的网络中，可以采用以下几种优化方法。

（1）增大学习率。在BN模型中，增大学习率可以加快收敛速度，但不会对梯度流产生副作用。<br />（2）去掉Dropout。如前所述，BN可以实现Dropout的作用，所以可以去掉Dropout，以加快训练速度。<br />（3）减少![](./img/11cc37d6dae6b905a0a0eb2ff087ae24.svg)正则的权重。如前所述，BN有一定的正则作用，所以可以适当地减少![](./img/11cc37d6dae6b905a0a0eb2ff087ae24.svg)惩罚。<br />（4）提高学习率的衰减速度。使用了BN后模型会更快收敛，所以学习率也应该相应地减小到较低的值。<br />（5）更加彻底地随机化训练数据，以防止每批数据总是出现相同的样本。<br />（6）减少图片扭曲。因为BN的训练速度更快，能够观察到的图片变少，所以应让模型尽可能地观察真实图片。

<a name="Source"></a>
## Source

[https://blog.csdn.net/anshuai_aw1/article/details/84975689#Batch_Normalization_284](https://blog.csdn.net/anshuai_aw1/article/details/84975689#Batch_Normalization_284)<br />[http://cs231n.github.io/neural-networks-2/](http://cs231n.github.io/neural-networks-2/)
