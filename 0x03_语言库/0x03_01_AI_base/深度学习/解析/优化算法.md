<a name="VdLwu"></a>
# [算法对比与选择](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers)
<a name="yY1Nf"></a>
## 算法对比
在下图中，我们看到不同算法在损失曲面的等高线上走的不同路线。所有的算法都是从同一个点出发并选择不同路径到达最优点。注意：Adagrad，Adadelta和RMSprop能够立即转移到正确的移动方向上并以类似的速度收敛，而动量法和NAG会导致偏离，想像一下球从山上滚下的画面。然而，NAG能够在偏离之后快速修正其路线，因为NAG通过对最优点的预见增强其响应能力。

![优化算法1.gif](./img/1594274912802-98599928-72ff-4fc6-8fc1-254a11a7abd6.gif)

下图展示了不同算法在鞍点出的行为，鞍点即为一个点在一个维度上的斜率为正，而在其他维度上的斜率为负，正如我们前面提及的，鞍点对SGD的训练造成很大困难。这里注意，SGD，动量法和NAG在鞍点处很难打破对称性，尽管后面两个算法最终设法逃离了鞍点。而Adagrad，RMSprop和Adadelta能够快速想着梯度为负的方向移动，其中Adadelta走在最前面。

![优化算法2.gif](./img/1594274928913-60e323a4-c3b6-4495-8c6b-d7648ea269cc.gif)

<a name="GnqYL"></a>
## 算法选择
我们应该选择使用哪种优化算法呢？如果输入数据是稀疏的，选择任一自适应学习率算法可能会得到最好的结果。选用这类算法的另一个好处是无需调整学习率，选用默认值就可能达到最好的结果。

总的来说，RMSprop是Adagrad的扩展形式，用于处理在Adagrad中急速递减的学习率。RMSprop与Adadelta相同，所不同的是Adadelta在更新规则中使用参数的均方根进行更新。最后，Adam是将偏差校正和动量加入到RMSprop中。在这样的情况下，RMSprop、Adadelta和Adam是很相似的算法并且在相似的环境中性能都不错。Kingma等人[9]指出在优化后期由于梯度变得越来越稀疏，偏差校正能够帮助Adam微弱地胜过RMSprop。综合看来，Adam可能是最佳的选择。

有趣的是，最近许多论文中采用不带动量的SGD和一种简单的学习率的退火策略。已表明，通常SGD能够找到最小值点，但是比其他优化的SGD花费更多的时间，与其他算法相比，SGD更加依赖鲁棒的初始化和退火策略，同时，SGD可能会陷入鞍点，而不是局部极小值点。因此，如果你关心的是快速收敛和训练一个深层的或者复杂的神经网络，你应该选择一个自适应学习率的方法。

下文中各个调用里 **kwargs 是keyword参数，可以是“clipnorm”或“clipvalue”，是为了避免梯度消失/爆炸而采用的[梯度裁剪](https://www.yuque.com/angsweet/machine-learning/nge5ca#wSLfX)，是在反向传播期间裁剪梯度，使它们永远不会超过某个阈值。<br />![kwargs.png](./img/1645757629721-3da87fc7-67cb-4983-8043-558885081c9f.png)
<a name="DZnHi"></a>
# [基本算法](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/SGD)
<a name="x7mFf"></a>
## 随机梯度下降(SGD)
算法过程：初始化：学习率![](./img/85b078da772ee9e0773bcb4f0041b586.svg)，初始参数![](./img/2554a2bb846cffd697389e5dc8912759.svg)

- while 停止准则为满足 do：
-         从训练集中采包含![](./img/6f8f57715090da2632453988d9a1501b.svg)个样本![](./img/3eadec0f36068f743565c35a942156ea.svg)的小批量，其中![](./img/e8fa5b806940d1b4d0059fba40646506.svg)对应目标为![](./img/4be66ad2cf5c98540db20bd7df0c0413.svg)
-         计算梯度估计：![](./img/d9cd04f04e9987f88ffb4ad0ab4af8cf.svg)
-         应用更新：![](./img/04e859d710ad9566c5db0cddb866d34e.svg)
```python
# 调用
tf.keras.optimizers.SGD(
    learning_rate=0.01, momentum=0.0, nesterov=False, name='SGD', **kwargs
)

# 定义 Update rule for parameter w with gradient g when momentum is 0:
w = w - learning_rate * g

# 例子
opt = tf.keras.optimizers.SGD(learning_rate=0.1)
var = tf.Variable(1.0)
loss = lambda: (var ** 2)/2.0         # d(loss)/d(var1) = var1
step_count = opt.minimize(loss, [var]).numpy()
# Step is `- learning_rate * grad`
var.numpy()
```
随机梯度下降(SGD)及其变种很可能是一般机器学习中应用最多的优化算法，特别是在深度学习中。与批梯度下降使用全部数据不同，随机梯度下降按照数据生成分布抽取![](./img/6f8f57715090da2632453988d9a1501b.svg)个小批量（独立同分布的）样本，通过它们梯度均值，我们可以得到梯度的无偏估计。

SGD算法中的一个关键参数是学习率。之前，我们介绍的SGD使用固定的学习率。在实践中，有必要随着时间的推移降低学习率，因此我们将第![](./img/8ce4b16b22b58894aa86c421e8759df3.svg)步迭代的学习率记作![](./img/85b078da772ee9e0773bcb4f0041b586.svg)。

这是因为SGD中梯度估计引入的噪声源（![](./img/6f8f57715090da2632453988d9a1501b.svg)个训练样本的随机采样）并不会在极小点处消失。相比之下，当我们使用批量梯度下降到极小点时，整个代价函数的真实梯度会变得很小，之后为![](./img/cfcd208495d565ef66e7dff9f98764da.svg)，因此批量梯度下降可以使用固定的学习率。保证SGD收敛地一个充分条件是![](./img/03c488d88e78e9d27510fc82838c3234.svg)且![](./img/fd894dacd2647c953284e557f2ba02e1.svg)。

实践中，一般会线性衰减学习率知道第![](./img/a6f317b268ae825d94f832f970af607c.svg)次迭代：

![](./img/f55c60a153299851cf082634ebdd7dee.svg)

其中![](./img/e1cabe31890f4dc0e32c704e133d9008.svg)。在![](./img/a6f317b268ae825d94f832f970af607c.svg)步迭代之后，一般使![](./img/92e4da341fe8f4cd46192f21b6ff3aa7.svg)保持常数。

学习率可通过试验和误差来选取，通常最好的选择方法是监测目标函数值随时间变化的学习曲线。与其说是科学，这更像是一门艺术，我们应该谨慎地参考关于这个问题的大部分指导。使用线性策略时，需要选择的参数为![](./img/3e5f26bad30f6df4ec55bdbc94a97902.svg)，![](./img/66b2d49da8b9b2d03d0d8aceb30dcdaf.svg)，![](./img/a6f317b268ae825d94f832f970af607c.svg)。通常![](./img/a6f317b268ae825d94f832f970af607c.svg)被设为需要反复遍历训练集几百次的迭代数。通常![](./img/66b2d49da8b9b2d03d0d8aceb30dcdaf.svg)应设为大约![](./img/3e5f26bad30f6df4ec55bdbc94a97902.svg)的![](./img/edce3bc75b860a30e6aa2144a995312a.svg)。主要问题是如何设置![](./img/3e5f26bad30f6df4ec55bdbc94a97902.svg)：若![](./img/3e5f26bad30f6df4ec55bdbc94a97902.svg)太大，学习曲线将会剧烈震荡，代价函数值通常会明显增加；温和的震荡是良好的，容易在训练随机代价函数（例如使用Dropout的代价函数）时出现；如果学习率太小，那么学习过程会很缓慢。如果初始学习率太低，那么学习可能会卡在一个相当高的代价值。通常，就总训练时间和最终代价值而言，最优初始学习率会高于大约迭代![](./img/f899139df5e1059396431415e770c6dd.svg)次左右后达到最佳效果的学习率。因此，通常最好是检测最早的几轮迭代，选择一个比在效果上表现最佳的学习率更大的学习率，但又不能太大导致严重的震荡。

SGD及相关的小批量亦或更广义的基于梯度优化的在线学习算法，一个重要的性质是每一步更新的计算时间不依赖训练样本数目的多寡。即使训练样本数目非常大时，它们也能收敛。对于足够大的数据集，SGD可能会在处理整个训练集之前就收敛到最终测试集误差的某个固定容差范围内。

研究优化算法的收敛率，一般会衡量额外误差(excess error)![](./img/d91035974b557a4bdd2024c89423a802.svg)，即当前代价函数超过最低可能代价的量。SGD应用于凸问题时，![](./img/8ce4b16b22b58894aa86c421e8759df3.svg)步迭代后的额外误差量级是![](./img/5f94a67826486786428890d1dd905c24.svg)，在强凸情况下式![](./img/438b7c3b630b5c605b5988d59c0c78a6.svg)。除非假定额外的条件，否则这些界限不能进一步改进。批量梯度下降在理论上比随机梯度下降有更好的收敛率，然而，泛化误差的下降速度不会快于![](./img/438b7c3b630b5c605b5988d59c0c78a6.svg)。对于大数据集，SGD只需非常少量样本计算梯度从而实现初始快速更新，远远超过了其缓慢地渐进收敛。我们也可以在学习过程中逐渐增大小批量的大小，以此权衡批梯度下降和随机梯度下降两者的优点。
<a name="XSh0Y"></a>
## 动量(Momentum)
```python
# 调用
tf.keras.optimizers.SGD(
    learning_rate=0.01, momentum=0.9, nesterov=False, name='momentum', **kwargs
)

# 定义 Update rule when momentum is larger than 0:
velocity = momentum * velocity - learning_rate * g
w = w + velocity

# 例子
opt = tf.keras.optimizers.SGD(learning_rate=0.1, momentum=0.9)
var = tf.Variable(1.0)
val0 = var.value()
loss = lambda: (var ** 2)/2.0         # d(loss)/d(var1) = var1
# First step is `- learning_rate * grad`
step_count = opt.minimize(loss, [var]).numpy()
val1 = var.value()
(val0 - val1).numpy()

# On later steps, step-size increases because of momentum
step_count = opt.minimize(loss, [var]).numpy()
val2 = var.value()
(val1 - val2).numpy()
```
虽然随机梯度下降仍然是非常受欢迎的优化方法，但其学习过程有时会很慢。动量方法旨在加速学习，特别是处理高曲率、小但一致的梯度，或是带噪声的梯度。动量算法积累了之前梯度指数级衰减的移动平均，并且继续沿该方向移动。

SGD很难通过陡谷，即在一个维度上的表面弯曲程度远大于其他维度的区域，这种情况通常出现在局部最优点附近。在这种情况下，SGD摇摆地通过陡谷的斜坡，同时，沿着底部到局部最优点的路径上只是缓慢地前进，这个过程如下图黑线所示。动量法是一种帮助SGD在相关方向上加速并抑制摇摆的一种方法。动量法将历史步长的更新向量的一个分量增加到当前的更新向量中。

![优化算法3.png](./img/1594275503098-6f5261f1-8150-49ec-b2b2-2d901606f8a5.png)

动量的主要目的是解决两个问题：Hessian矩阵的病态条件和随机梯度的方差。我们通过上图说明动量如何客服这两个问题的第一个。等高线描绘了一个二次损失函数（具有病态条件的Hessian矩阵）。横跨轮廓的红色路径表示动量学习规则所遵循的路径，它使该函数最小化。我们在该路径的每个步骤画一个箭头，表示梯度下降将在该店采取的步骤。我们可以看到，一个病态的二次目标函数看起来想一个长而窄的山谷或具有陡峭边的峡谷。动量正确地纵向穿过峡谷，而普通的梯度步骤则会浪费时间在峡谷的窄轴上来回移动

从形式上看，动量算法引入了变量![](./img/9e3669d19b675bd57058fd4664205d2a.svg)充当速度角色——它代表参数在参数空间移动的方向和速率。速度被设为负梯度的指数衰减平均。名称动量(momentum)来自物理类比，根据牛顿运动定律，负梯度是移动参数空间中粒子的力。动量在物理学上定义为质量乘以速度。在动量学习算法中，我们假设是单位质量，因此速度向量![](./img/9e3669d19b675bd57058fd4664205d2a.svg)也可以看作是粒子的动量。超参数![](./img/e8d040e1ff260ef5977fe2dfd14a7673.svg)决定了之前梯度的贡献衰减得有多快。更新规则如下：

![](./img/3e8525f6a1d3a936760f08c7f8a9f167.svg)<br />![](./img/46c00945963ba8af38715f10323dfb74.svg)

速度![](./img/9e3669d19b675bd57058fd4664205d2a.svg)积累了梯度元素![](./img/80c9ee94191578f067725a345f139afc.svg)。相对于![](./img/92e4da341fe8f4cd46192f21b6ff3aa7.svg)，![](./img/7b7f9dbfea05c83784f8b85149852f08.svg)越大，之前梯度对现在方向的影响也越大。第一个式子有两项。第一项是上一次迭代的梯度，乘上一个被称为「Momentum 系数」的值，可以理解为取上次梯度的比例。

![优化算法4.jpeg](./img/1594275650837-1b665912-b436-4141-8011-249ba270c421.jpeg)

我们设![](./img/9e3669d19b675bd57058fd4664205d2a.svg)的初始为0，动量系数为0.9，那么迭代过程如下：

![](./img/a06ef7ea4fc45a724f50115019f5c78a.svg)<br />![](./img/769bec05d1d2ad2a4378e1b36e8cc4a9.svg)<br />![](./img/d35bcb07fcc72d5a1e59c5cafa6b91ea.svg)

我们可以看到之前的梯度会一直存在后面的迭代过程中，只是越靠前的梯度其权重越小。（说的数学一点，我们取的是这些梯度步长的指数平均）这对我们的例子有什么帮助呢？观察下图，注意到大部分的梯度更新呈锯齿状。我们也注意到，每一步的梯度更新方向可以被进一步分解为 w1 和 w2 分量。如果我们单独的将这些向量求和，沿 w1 方向的的分量将抵消，沿 w2 方向的分量将得到加强。

![优化算法5.jpeg](./img/1594275767566-8639cc6a-9af5-454d-82e6-71e97c194119.jpeg)

对于权值更新来说，将沿着 w2 方向进行，因为 w1 方向已抵消。这就可以帮助我们快速朝着极小值方向更新。所以，动量也被认为是一种抑制迭代过程中锯齿下降问题的技术。这种方法还可以提高收敛速度，但如果超过极小值，可能需要使用模拟退化算法我们通常初始化动量为![](./img/d310cb367d993fb6fb584b198a2fd72c.svg)，并且在一定循环次数后逐渐退火到![](./img/a894124cc6d5c5c71afe060d5dde0762.svg)。

之前，步长只是梯度范数乘以学习率。现在，步长取决于梯度序列的大小和排列。当许多连续的梯度指向相同方向时，步长最大。如果动量算法总是观测到梯度![](./img/b2f5ff47436671b6e533d8dc3614845d.svg)，那么它会在方向![](./img/ef07c065bf3b6a37452f81f6ce4e7ea2.svg)上不停加速，知道达到最终速度，其中步长大小为![](./img/62bc0618a440530b3a0c40d3cd9e5dba.svg)。因此将动量的超参数视为![](./img/0d48ec952eeec573b45b5790077ee1b0.svg)有助于理解。例如，![](./img/32be96e2c59d87ea0b2f1e5de81105a3.svg)对应着最大速度![](./img/d3d9446802a44259755d38e6d163e820.svg)倍于梯度下降算法。

在实践中，![](./img/7b7f9dbfea05c83784f8b85149852f08.svg)的一般取值为![](./img/d310cb367d993fb6fb584b198a2fd72c.svg)，![](./img/a894124cc6d5c5c71afe060d5dde0762.svg)和![](./img/9b185eba3656e2a8aad8774a278b083f.svg)。和学习率一样，![](./img/7b7f9dbfea05c83784f8b85149852f08.svg)也会随着时间不断调整。一般初始值是一个较小的值，随后会慢慢变大。随着时间推移调整![](./img/7b7f9dbfea05c83784f8b85149852f08.svg)没有收缩![](./img/92e4da341fe8f4cd46192f21b6ff3aa7.svg)重要。

我们可以将动量算法视为模拟连续时间下牛顿动力学下的粒子。这种物理类比有助于直觉上理解动量和梯度下降算法是如何表现的。粒子在任意时间点的为止由![](./img/3e8d7c7f7f41deeec693dcfa3d030572.svg)给定。粒子会受到净力![](./img/d6e3af948a34fd5f432cb9d377a98ef0.svg)。该力会导致粒子加速

![](./img/cee9a4830f0f77f71f68592a3c0b1720.svg)

将其视为位置的二阶微分方程，引入表示粒子在时间![](./img/e358efa489f58062f10dd7316b65649e.svg)处速度的变量![](./img/273a383345e167ee1791232c40eaf917.svg)，将牛顿动力学重写为一阶微分方程：

![](./img/618f2bafbd3dfaf0a08d79889ec76acd.svg)<br />![](./img/073202430a574208fbf9db753fab7742.svg)

由此，动量算法包括通过数值模型求解微分方程。求解微分方程的一个简单数值方法是欧拉方法，通过在每个梯度方向上小且有限的步来简单模拟该等式定义的动力学。

这解释了动量更新的基本形式，但具体什么是力呢？力正比于代价函数的负梯度![](./img/53db8915d39d90f995b0f26fc0e4e446.svg)。该力推动粒子沿着代价函数表面下坡的方向移动。梯度下降算法基于每个梯度简单地更新一步，而使用动量算法的牛顿方案则使用该力改变粒子的速度。我们可以将粒子视作在冰面上滑行的冰球。每当它沿着表面最陡的部分下降时，它会积累继续在该方向上滑行的速度，直到其开始向上滑动为止。

另一个力也是必要的。如果代价函数的梯度是唯一的力，那么粒子可能永远不会停下来。想象一下，假设理想情况下冰面没有摩擦，一个冰球从山谷的一端下滑，上升到另一端，永远来回震荡。要解决这个问题，我们添加另一个正比于![](./img/5e798a9df482331f0138cb5f37360f69.svg)的力。在物理术语中，此力对应于粘性阻力，就像例子必须通过一个抵抗介质，如糖浆。这会导致粒子随着时间推移逐渐失去能量，最终收敛到局部极小点。

为什么要特别使用![](./img/5e798a9df482331f0138cb5f37360f69.svg)和粘性阻力呢？部分原因是因为![](./img/5e798a9df482331f0138cb5f37360f69.svg)在数学上的变量——速度的整数幂很容易处理。然而，其他物理系统具有基于速度的整数幂类型的阻力，例如，颗粒通过空气时会收正比于速度平方的湍流阻力，而颗粒沿着地面移动时会受到恒定大小的摩擦力。这些选择都不合适。湍流阻力，正比于速度的平方，在速度很小时会很弱。不够强到使粒子停下来。非零值初始速度的粒子仅受到湍流阻力，会从初始位置永远地移动下去，和初始位置的距离大概正比于![](./img/f6b0b7d8634cfbb7b988ed9ded3827e6.svg)。因此我们必须使用速度较低幂次的力。如果幂次为零，相当于干摩擦，那么力太强了。当代价函数的梯度表示的力很小但非零时，由于摩擦导致的恒力会使得粒子在达到局部极小点之前就停下来。粘性阻力避免了这两个问题——它足够弱，可以使梯度引起的运动直到达到最小，但有足够强，使得坡度不够时可以阻止运动。

算法过程：初始化：学习率![](./img/85b078da772ee9e0773bcb4f0041b586.svg)，动量参数![](./img/7b7f9dbfea05c83784f8b85149852f08.svg)，初始速度![](./img/9e3669d19b675bd57058fd4664205d2a.svg)，初始参数![](./img/2554a2bb846cffd697389e5dc8912759.svg)

- while 停止准则为满足 do：
-         从训练集中采包含![](./img/6f8f57715090da2632453988d9a1501b.svg)个样本![](./img/3eadec0f36068f743565c35a942156ea.svg)的小批量，其中![](./img/e8fa5b806940d1b4d0059fba40646506.svg)对应目标为![](./img/4be66ad2cf5c98540db20bd7df0c0413.svg)
-         计算梯度估计：![](./img/5d5641552900f31afdeecd405dbc80ab.svg)
-         计算速度更新：![](./img/963d52c38ceebd87f02c746b928a9ded.svg)
-         应用更新：![](./img/c3068b9348c17c1a523563b4d408e89f.svg)

<a name="JI4ZC"></a>
## Nesterov动量
```python
# 调用
tf.keras.optimizers.SGD(
    learning_rate=0.01, momentum=0.9, nesterov=True, name='nesterov', **kwargs
)

# 定义 When nesterov=True, this rule becomes:
velocity = momentum * velocity - learning_rate * g
w = w + momentum * velocity - learning_rate * g
```
受Nesterov加速梯度下降法（Nesterov accelerated gradient，NAG）启发，提出了动量算法的一个变种。这种情况的更新规则如下：

![](./img/3284d2f082035eb56869ef31e53c8b0a.svg)<br />![](./img/758bf5b6beb3c877894e77e67485bc5f.svg)

其中参数![](./img/7b7f9dbfea05c83784f8b85149852f08.svg)和![](./img/92e4da341fe8f4cd46192f21b6ff3aa7.svg)发挥了和标准动量方法中类似的作用。Nesterov动量和标准动量之间的区别体现在梯度计算上。Nesterov动量中，梯度计算在施加当前速度之后。因此，Nesterov动量可以解释为往标准动量方法中添加了一个校正因子。

![nesterov.jpeg](./img/1594695289690-d8ab69f5-5136-4046-ac0f-a752e2afb96a.jpeg)

在凸批量梯度的情况下，Nesterov动量将额外误差收敛从![](./img/438b7c3b630b5c605b5988d59c0c78a6.svg)（![](./img/8ce4b16b22b58894aa86c421e8759df3.svg)步后）改进到![](./img/7ba767d1a7982a400acd008d9c36d683.svg)。遗憾的是，在随机梯度的情况下，Nesterov动量没有改进收敛率。

算法过程：初始化：学习率![](./img/85b078da772ee9e0773bcb4f0041b586.svg)，动量参数![](./img/7b7f9dbfea05c83784f8b85149852f08.svg)，初始速度![](./img/9e3669d19b675bd57058fd4664205d2a.svg)，初始参数![](./img/2554a2bb846cffd697389e5dc8912759.svg)

- while 停止准则为满足 do：
-         从训练集中采包含![](./img/6f8f57715090da2632453988d9a1501b.svg)个样本![](./img/3eadec0f36068f743565c35a942156ea.svg)的小批量，其中![](./img/e8fa5b806940d1b4d0059fba40646506.svg)对应目标为![](./img/4be66ad2cf5c98540db20bd7df0c0413.svg)
-         应用临时更新：![](./img/a93e478f8a6172dc03ee6551855c71ab.svg)
-         计算梯度（在临时点）：![](./img/918cebdca6fb5cd89c243b5c4403751b.svg)
-         计算速度更新：![](./img/963d52c38ceebd87f02c746b928a9ded.svg)
-         应用更新：![](./img/c3068b9348c17c1a523563b4d408e89f.svg)

<a name="YrlcH"></a>
## 批梯度下降
Vanilla梯度下降法，又称为批梯度下降法（batch gradient descent），在整个训练数据集上计算损失函数关于参数![](./img/2554a2bb846cffd697389e5dc8912759.svg)的梯度。因为在执行每次更新时，我们需要在整个数据集上计算所有的梯度，所以批梯度下降法的速度会很慢，同时，批梯度下降法无法处理超出内存容量限制的数据集。批梯度下降法同样也不能在线更新模型，即在运行的过程中，不能增加新的样本。

对于给定的迭代次数，首先，我们利用全部数据集计算损失函数关于参数向量的梯度向量。然后，我们利用梯度的方向和学习率更新参数，学习率决定我们将以多大的步长更新参数。对于凸误差函数，批梯度下降法能够保证收敛到全局最小值，对于非凸函数，则收敛到一个局部最小值。
<a name="F6VJa"></a>
# 自适应学习率算法
<a name="vD2p9"></a>
## [AdaGrad](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Adagrad)
```python
# 调用
tf.keras.optimizers.Adagrad(
    learning_rate=0.001, initial_accumulator_value=0.1, epsilon=1e-07,
    name='Adagrad', **kwargs
)
```
AdaGrad是这样的一种基于梯度的优化算法：让学习率适应参数，对于出现次数较少的特征，我们对其采用更大的学习率，对于出现次数较多的特征，我们对其采用较小的学习率。因此，AdaGrad非常适合处理稀疏数据。Dean等人发现AdaGrad能够极大提高了SGD的鲁棒性并将其应用于Google的大规模神经网络的训练，其中包含了YouTube视频中的猫的识别。此外，Pennington等人利用AdaGrad训练Glove词向量，因为低频词比高频词需要更大的步长。

前面，我们每次更新所有的参数![](./img/2554a2bb846cffd697389e5dc8912759.svg)时，每个参数![](./img/9fe4f6a929e86b5c5a7d19d4a18fc304.svg)都使用相同的学习率![](./img/ffe9f913124f345732e9f00fa258552e.svg)。由于AdaGrad在![](./img/e358efa489f58062f10dd7316b65649e.svg)时刻对每一个参数![](./img/9fe4f6a929e86b5c5a7d19d4a18fc304.svg)使用了不同的学习率，我们首先介绍AdaGrad对每一个参数的更新，然后我们对其向量化。为了简洁，令![](./img/35a7904831f028b6c01096af6b19356c.svg)为在![](./img/e358efa489f58062f10dd7316b65649e.svg)时刻目标函数关于参数![](./img/9fe4f6a929e86b5c5a7d19d4a18fc304.svg)的梯度：

![](./img/7bf14b9709bbf5e70bb676ae7f87d18b.svg)

在![](./img/e358efa489f58062f10dd7316b65649e.svg)时刻，对每个参数![](./img/9fe4f6a929e86b5c5a7d19d4a18fc304.svg)的更新过程变为：

![](./img/c0a56fef468879295a7781d271b30539.svg)

对于上述的更新规则，在![](./img/e358efa489f58062f10dd7316b65649e.svg)时刻，基于对![](./img/9fe4f6a929e86b5c5a7d19d4a18fc304.svg)计算过的历史梯度，Adagrad修正了对每个参数![](./img/9fe4f6a929e86b5c5a7d19d4a18fc304.svg)的学习率

![](./img/c98120e83fd27016dd0d49b702adc118.svg)

其中，![](./img/c02ab9bb62135d535204487cbfced801.svg)是一个对角矩阵，对角线上的元素![](./img/0baa850c2c57448419623d741b695bf2.svg)是直到![](./img/e358efa489f58062f10dd7316b65649e.svg)时刻为止，所有关于![](./img/9fe4f6a929e86b5c5a7d19d4a18fc304.svg)的梯度的平方和（Duchi等人将该矩阵作为包含所有先前梯度的外积的完整矩阵的替代，因为即使是对于中等数量的参数![](./img/8277e0910d750195b448797616e091ad.svg)，矩阵的均方根的计算都是不切实际的。），![](./img/92e4da341fe8f4cd46192f21b6ff3aa7.svg)是平滑项，用于防止除数为![](./img/cfcd208495d565ef66e7dff9f98764da.svg)。比较有意思的是，如果没有平方根的操作，算法的效果会变得很差。

由于![](./img/61d76e8966a8ce612c9b61d7b40e9ad6.svg)的对角线上包含了关于所有参数![](./img/2554a2bb846cffd697389e5dc8912759.svg)的历史梯度的平方和，现在，我们可以通过![](./img/61d76e8966a8ce612c9b61d7b40e9ad6.svg)和![](./img/f752008a78c8519e9f8f178faf0b87ed.svg)之间的元素向量乘法![](./img/319d584a4a5166ee6c51f4b8348856ea.svg)向量化上述操作：

![](./img/bfbb312e2b99d1f51ce44f50064f4b04.svg)

AdaGrad算法的一个主要优点是无需手动调整学习率。在大多数的应用场景中，通常采用常数![](./img/04817efd11c15364a6ec239780038862.svg)。AdaGrad的一个主要缺点是它在分母中累加梯度的平方：由于没增加一个正项，在整个训练过程中，累加的和会持续增长。这会导致学习率变小以至于最终变得无限小，在学习率无限小时，AdaGrad算法将无法取得额外的信息。接下来的算法旨在解决这个不足。

算法过程：初始化：全局学习率![](./img/92e4da341fe8f4cd46192f21b6ff3aa7.svg)，初始参数![](./img/2554a2bb846cffd697389e5dc8912759.svg)，小常数![](./img/77a3b715842b45e440a5bee15357ad29.svg)(大约设为![](./img/3907c05eab507b9f90ead1144d524c7c.svg))，初始化梯度累积变量![](./img/6252ab7a75ac12de93c41861a9d69a8c.svg)

- while 停止准则为满足 do：
-         从训练集中采包含![](./img/6f8f57715090da2632453988d9a1501b.svg)个样本![](./img/3eadec0f36068f743565c35a942156ea.svg)的小批量，其中![](./img/e8fa5b806940d1b4d0059fba40646506.svg)对应目标为![](./img/4be66ad2cf5c98540db20bd7df0c0413.svg)
-         计算梯度：![](./img/5d5641552900f31afdeecd405dbc80ab.svg)
-         累积平法梯度：![](./img/4ebe760184407d6479bdaa9b269e102d.svg)
-         计算更新：![](./img/6a530bc18000b5e5cc3f31a76d9ef868.svg)
-         应用更新：![](./img/991c0572d5f3b2b3ce4f2e83dc464c79.svg)

<a name="LCeMB"></a>
## [Adadelta](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Adadelta)
```python
# 调用
tf.keras.optimizers.Adadelta(
    learning_rate=0.001, rho=0.95, epsilon=1e-07, name='Adadelta',
    **kwargs
)
```
Adadelta是Adagrad的一种扩展算法，以处理Adagrad学习速率单调递减的问题。不是计算所有的梯度平方，Adadelta将计算历史梯度的窗口大小限制为一个固定值![](./img/f1290186a5d0b1ceab27f4e77c0c5d68.svg)。

在Adadelta中，无需存储先前的![](./img/f1290186a5d0b1ceab27f4e77c0c5d68.svg)个平方梯度，而是将梯度的平方递归地表示成所有历史梯度平方的均值。在![](./img/e358efa489f58062f10dd7316b65649e.svg)时刻的均值![](./img/df734545c7e7baf3b50f630ed1d7e0a1.svg)只取决于先前的均值和当前的梯度（分量![](./img/ae539dfcc999c28e25a0f3ae65c1de79.svg)类似于动量项）：

![](./img/2311ae39408749af8a673aa4ffcf7e39.svg)

我们将![](./img/ae539dfcc999c28e25a0f3ae65c1de79.svg)设置成与动量项相似的值，即![](./img/a894124cc6d5c5c71afe060d5dde0762.svg)左右。简单起见，利用参数更新向量![](./img/4acec7237b7875e6af4b442917cd4aa3.svg)重新表示SGD的更新过程：

![](./img/03fff8d1575c79ef9bd62ea527132092.svg)<br />![](./img/0bd062b98b430d02f0d7dcab38359060.svg)

我们先前得到的Adagrad参数更新向量变为：

![](./img/abd09aa7b76a963471e40c8aaeb5750d.svg)

现在，我们简单讲对角矩阵![](./img/61d76e8966a8ce612c9b61d7b40e9ad6.svg)替换成历史梯度的均值![](./img/df734545c7e7baf3b50f630ed1d7e0a1.svg)：

![](./img/323d5d0574ddcf9bdf86ae05c87bf9f6.svg)

由于分母仅仅是梯度的均方根（root mean squared，RMS）误差，我们可以简写为：

![](./img/366b11fc019717458b584c4ba18185be.svg)

作者指出上述更新公式中的每个部分（与SGD，动量法或者Adagrad）并不一致，即更新规则中必须与参数具有相同的假设单位。为了实现这个要求，作者首次定义了另一个指数衰减均值，这次不是梯度平方，而是参数的平方的更新：

![](./img/b6fea542a849fccea2c55c0857c4ef83.svg)

因此，参数更新的均方根误差为：

![](./img/86e6902ff4efa89f3ce32b1f00ffe13e.svg)

由于![](./img/13a0bbcf7e4303d5a2d13296313f8983.svg)是未知的，我们利用参数的均方根误差来近似更新。利用![](./img/ba5c93858c0ef6742723872d6cb3b114.svg)替换先前的更新规则中的学习率![](./img/ffe9f913124f345732e9f00fa258552e.svg)，最终得到Adadelta的更新规则：

![](./img/2f074d01ffa724d7f77fd8a1162e07be.svg)<br />![](./img/d3d3d11a1a9fce560afe3013dc581c92.svg)

使用Adadelta算法，我们甚至都无需设置默认的学习率，因为更新规则中已经移除了学习率。

<a name="kAjTE"></a>
## [RMSprop](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/RMSprop)
```python
# 调用
tf.keras.optimizers.RMSprop(
    learning_rate=0.001, rho=0.9, momentum=0.0, epsilon=1e-07, centered=False,
    name='RMSprop', **kwargs
)

# 例子
opt = tf.keras.optimizers.RMSprop(learning_rate=0.1)
var1 = tf.Variable(10.0)
loss = lambda: (var1 ** 2) / 2.0    # d(loss) / d(var1) = var1
step_count = opt.minimize(loss, [var1]).numpy()
var1.numpy()
```
RMSprop 算法也旨在抑制梯度的锯齿下降，但与动量相比， RMSprop 不需要手动配置学习率超参数，由算法自动完成。 更重要的是，RMSprop 可以为每个参数选择不同的学习率。在 RMSprop 算法中，每次迭代都根据下面的公式完成。 它是对每个参数单独迭代。

![RMSProp.jpeg](./img/1594698993977-0208cc96-6e81-43d4-bdc3-ef53ecd52f53.jpeg)

在第一个方程中，我们计算一个梯度平方的指数平均值。由于我们需要针对每个梯度分量分别执行平方，所以此处的梯度向量![](./img/f752008a78c8519e9f8f178faf0b87ed.svg)对应的是正在更新的参数方向的梯度各个方向的投影分量。为此，我们将上一次更新的超参数乘![](./img/d2606be4e0cd2c9a6179c8f2e3547a85.svg)。然后将当前的梯度平方乘![](./img/07ec0e0f9bcb8a692d9aa19fd3b994df.svg)。最后我们将他们加到一起得到这一时刻的指数平均

我们之所以使用指数平均是因为在 momentum 例子中看到的那样，它可以使得间隔和权重成正比例变化。实际上使用「指数」一词是因为前面项的权重呈指数级下降（最近的项权重是![](./img/d2606be4e0cd2c9a6179c8f2e3547a85.svg)，次近的![](./img/d2606be4e0cd2c9a6179c8f2e3547a85.svg)方，然后是![](./img/d2606be4e0cd2c9a6179c8f2e3547a85.svg)立方，以此类推）。注意我们表示病态曲率的图，梯度沿 w1 方向的分量比沿 w2 方向的分量大的多。我们以平方的方式将 w1 和 w2 叠加，w1 不会发生抵消，w2 在指数平均后会更小。

第二个方程定义了步长，我们沿负梯度方向移动，但是步长受到指数平均值的影响。我们设置了一个初始学习率![](./img/ffe9f913124f345732e9f00fa258552e.svg)，用它除指数平均值。在我们的例子中，因为 w1 平均后比 w2 大很多，所以 w1 的迭代步长就比 w2 要小很多。因此这将避免我们在山脊之间跳跃而朝着正确的方向移动。

第三个方程是更新操作，超参![](./img/d2606be4e0cd2c9a6179c8f2e3547a85.svg)通常选![](./img/a894124cc6d5c5c71afe060d5dde0762.svg)，但可能需要调整它。方程2中的![](./img/92e4da341fe8f4cd46192f21b6ff3aa7.svg)是为了防止被![](./img/cfcd208495d565ef66e7dff9f98764da.svg)除，通常取![](./img/1ec79e91d92df04471972727d4bb9404.svg)。

还要注意的是，RMSprop隐含的执行模拟退火，假设我们正朝着极小值前进并且我们想要放慢速度避免越过极小值。当步长很大时 RMSProp 将自动减小梯度更新的步长（大步长容易越过极小值点）。

算法过程(RMSprop算法)：初始化：全局学习率![](./img/92e4da341fe8f4cd46192f21b6ff3aa7.svg)，衰减速率![](./img/d2606be4e0cd2c9a6179c8f2e3547a85.svg)，初始参数![](./img/2554a2bb846cffd697389e5dc8912759.svg)，小常数![](./img/77a3b715842b45e440a5bee15357ad29.svg)(通常设为![](./img/1ec79e91d92df04471972727d4bb9404.svg)，用于被小数除时的数值稳定)，初始化累积变量![](./img/6252ab7a75ac12de93c41861a9d69a8c.svg)

- while 停止准则为满足 do：
-          从训练集中采包含![](./img/6f8f57715090da2632453988d9a1501b.svg)个样本![](./img/3eadec0f36068f743565c35a942156ea.svg)的小批量，其中![](./img/e8fa5b806940d1b4d0059fba40646506.svg)对应目标为![](./img/4be66ad2cf5c98540db20bd7df0c0413.svg)
-          计算梯度：![](./img/5d5641552900f31afdeecd405dbc80ab.svg)
-          累积平方梯度：![](./img/b7060512129f20f68eb560da03e71e1b.svg)
-          计算参数更新：![](./img/bfbe7cd193205d8cf598ae29ba95f2f7.svg)（![](./img/9ae23666210a8b354b33d51a0fc65ce3.svg)逐元素应用）
-          应用更新：![](./img/991c0572d5f3b2b3ce4f2e83dc464c79.svg)

算法过程(使用Nesterov动量的RMSprop算法)：初始化：全局学习率![](./img/92e4da341fe8f4cd46192f21b6ff3aa7.svg)，衰减速率![](./img/d2606be4e0cd2c9a6179c8f2e3547a85.svg)，动量系数![](./img/7b7f9dbfea05c83784f8b85149852f08.svg)，初始参数![](./img/2554a2bb846cffd697389e5dc8912759.svg)，初始参数![](./img/9e3669d19b675bd57058fd4664205d2a.svg)，初始化累积变量![](./img/6252ab7a75ac12de93c41861a9d69a8c.svg)

- while 停止准则为满足 do：
-          从训练集中采包含![](./img/6f8f57715090da2632453988d9a1501b.svg)个样本![](./img/3eadec0f36068f743565c35a942156ea.svg)的小批量，其中![](./img/e8fa5b806940d1b4d0059fba40646506.svg)对应目标为![](./img/4be66ad2cf5c98540db20bd7df0c0413.svg)
-          计算临时更新：![](./img/f01cf5233d6ea18fcb441fe59e79a960.svg)
-          计算梯度：![](./img/1e989795b5b6cd755cb2debc8de84aa5.svg)
-          累积梯度：![](./img/b7060512129f20f68eb560da03e71e1b.svg)
-          计算速度更新：![](./img/39da809864df854f8163a0c33fe40c93.svg) （![](./img/90ab060f348dae7d50d691ffa2400ae7.svg)逐元素应用）
-          应用更新：![](./img/c3068b9348c17c1a523563b4d408e89f.svg)

<a name="NINBc"></a>
## [Adam](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Adam)
```python
# 调用
tf.keras.optimizers.Adam(
    learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-07, amsgrad=False,
    name='Adam', **kwargs
)

# 例子
opt = tf.keras.optimizers.Adam(learning_rate=0.1)
var1 = tf.Variable(10.0)
loss = lambda: (var1 ** 2)/2.0       # d(loss)/d(var1) == var1
step_count = opt.minimize(loss, [var1]).numpy()
# The first step is `-learning_rate*sign(grad)`
var1.numpy()
```
到目前为止，我们已经对比了 RMSprop 和 Momentum 两种方法。尽管 Momentum 加速了我们对极小值方向的搜索，但 RMSprop 阻碍了我们在振荡方向上的搜索。Adam 或 Adaptive Moment Optimization 算法将 Momentum 和 RMSprop 两种算法结合了起来。这里是迭代方程。

![Adam.jpeg](./img/1594708598564-82281790-f23b-42e1-af72-c75c586cef3f.jpeg)

我们计算了每个梯度分量的指数平均和梯度平方指数平均（方程1、方程2）。为了确定迭代步长我们在方程3中用梯度的指数平均乘学习率（如 Momentum 的情况）并除以根号下的平方指数平均（如 Momentum 的情况），然后方程4执行更新步骤。超参数![](./img/b4ceec2c4656f5c1e7fc76c59c4f80f3.svg)一般取![](./img/a894124cc6d5c5c71afe060d5dde0762.svg)左右，![](./img/d9f51e864a6151f57e727294da7ac28c.svg)取![](./img/9b185eba3656e2a8aad8774a278b083f.svg)。![](./img/92e4da341fe8f4cd46192f21b6ff3aa7.svg)一般取![](./img/6472b2f9e70aac629d7e2763a59f0066.svg)。

算法过程：初始化：步长![](./img/92e4da341fe8f4cd46192f21b6ff3aa7.svg)(建议默认为：![](./img/8f4e22c93710008b7ea050514a32ecdd.svg))，矩估计的指数衰减速率![](./img/39cbe564daf7ba0866dc9566aca305a5.svg)，![](./img/aaf3b373e2fb7352d302d7d79c90afcf.svg)在区间![](./img/84037ac2ca001c0ec4ddbca41884f8a1.svg)内（建议默认为：超参数![](./img/39cbe564daf7ba0866dc9566aca305a5.svg)一般取![](./img/a894124cc6d5c5c71afe060d5dde0762.svg)左右，![](./img/aaf3b373e2fb7352d302d7d79c90afcf.svg)取![](./img/9b185eba3656e2a8aad8774a278b083f.svg)），用于数值稳定的小常数![](./img/77a3b715842b45e440a5bee15357ad29.svg)（建议默认为：![](./img/6472b2f9e70aac629d7e2763a59f0066.svg)），初始化参数![](./img/2554a2bb846cffd697389e5dc8912759.svg)，初始化一阶和二阶矩阵变量![](./img/d1a3fa1aba369e7b2a03ad68c55a05dc.svg)，![](./img/2c3db681686c1b080e21688bf57b256a.svg)，初始化时间步![](./img/1f48e973d6a9075dbaaf41a9e85f034e.svg)

- while 停止准则为满足 do：
-          从训练集中采包含![](./img/6f8f57715090da2632453988d9a1501b.svg)个样本![](./img/3eadec0f36068f743565c35a942156ea.svg)的小批量，其中![](./img/e8fa5b806940d1b4d0059fba40646506.svg)对应目标为![](./img/4be66ad2cf5c98540db20bd7df0c0413.svg)
-          计算梯度：![](./img/5d5641552900f31afdeecd405dbc80ab.svg)
-          ![](./img/248ace9225f2366a829b9625cb004d46.svg)
-          更新有偏一阶矩估计：![](./img/6b37d33ed0e18470182c06dab2559058.svg)
-          更新有偏二阶矩估计：![](./img/d89fd967fd0238fbb4a9a71c5faa21cc.svg)
-          修正一阶矩的偏差：![](./img/8c7ae169e83a1c049390d03c73308fa1.svg)
-          修正二阶矩的偏差：![](./img/987cb35332a0074b79399dde2f087490.svg)
-          计算更新：![](./img/091f5f81ce9915a27faeeb696595223e.svg)（逐元素应用）
-          应用更新：![](./img/991c0572d5f3b2b3ce4f2e83dc464c79.svg)


<a name="KL5jW"></a>
# 二阶近似方法
<a name="Eex0x"></a>
## 牛顿法
牛顿法时基于二阶泰勒级数展开在某点![](./img/dbc9011a370bca098d4752346ba71d5c.svg)附近来近似![](./img/698e62e708a07405c6154a447463ef91.svg)的优化方法，其忽略了高阶导数：

![](./img/edfe86946fc8a666d63a99181d3b99f8.svg)

其中![](./img/c1d9f50f86825a1a2302ec2449c17196.svg)是![](./img/ff44570aca8241914870afbc310cdb85.svg)相对于![](./img/2554a2bb846cffd697389e5dc8912759.svg)的Hessian矩阵在![](./img/dbc9011a370bca098d4752346ba71d5c.svg)处的估计。如果我们再求解这个函数的临界点，我们将得到牛顿参数更新规则：

![](./img/f6a09fe6261f7bfa13b992484e54344e.svg)

因此，对于局部的二次函数（具有正定的![](./img/c1d9f50f86825a1a2302ec2449c17196.svg)），用![](./img/5c5ca72dc2cacdf240bf25622dc47f76.svg)重新调整梯度，牛顿法会直接跳到极小值。如果目标函数是凸的但非二次的（有高阶项），该更新将是迭代的，得到和牛顿法相关的算法。

在深度学习中，目标函数的表面通常非凸（有很多特征），如鞍点。因此使用牛顿法时有问题的。如果Hessian矩阵的特征值并不都是正的，例如，靠近鞍点处，牛顿法实际上会导致更新朝错误的方向移动。这种情况可以通过正则化Hessian矩阵来避免。常用的正则化策略包括在Hessian矩阵对角线上增加常数![](./img/7b7f9dbfea05c83784f8b85149852f08.svg)。正则化更新变为

![](./img/c443db03b379716db2f18e5b0e1e58b3.svg)

这个正则化策略用于牛顿法的近似，例如Levenberg-Marquardt算法，只要Hessian矩阵的负特征值仍然相对接近零，效果就会很好。在曲率方向更极端的情况下，![](./img/7b7f9dbfea05c83784f8b85149852f08.svg)的值必须足够大，以抵消负特征值。然而， 如果![](./img/7b7f9dbfea05c83784f8b85149852f08.svg)持续增加，Hessian矩阵会变得由对角矩阵![](./img/b163865afcf54c170d803882c3864b85.svg)主导，通过牛顿法所选择的方向会收敛到普通梯度除以![](./img/7b7f9dbfea05c83784f8b85149852f08.svg)。当很强的负曲率存在时，![](./img/7b7f9dbfea05c83784f8b85149852f08.svg)可能需求特别大，以致于牛顿法比选择合适学习率的梯度下降的步长更小。

除了目标函数的某些特征带来的挑战，如鞍点，牛顿法用于训练大型神经网络还受限于其显著的计算负担。Hessian矩阵中元素数目时参数数量的平方，因此，如果参数数目为![](./img/8ce4b16b22b58894aa86c421e8759df3.svg)（甚至是在非常小的神经网络中![](./img/8ce4b16b22b58894aa86c421e8759df3.svg)也可能是百万级别），牛顿法需要计算![](./img/5b5f0112171324a6f9fc5df06f6019dd.svg)矩阵的逆，计算复杂度为![](./img/3c3e7b91cce026230299d545a8b418a8.svg)。另外，由于参数将每次更新都会改变，每次训练迭代都需要计算Hessian矩阵的逆。其结果是，只有参数很少的网络才能在实际中用牛顿法训练。

算法过程：初始化：初始参数![](./img/dbc9011a370bca098d4752346ba71d5c.svg)，包含![](./img/6f8f57715090da2632453988d9a1501b.svg)个样本的训练集

- while 停止准则为满足 do：
-          计算梯度：![](./img/5d5641552900f31afdeecd405dbc80ab.svg)
-          计算Hessian矩阵：![](./img/0013139f755a4dff5d8c15adca8ab4c0.svg)
-          计算Hessian逆：![](./img/5c5ca72dc2cacdf240bf25622dc47f76.svg)
-          计算更新：![](./img/785ed468a76e31c96de37a6572d198a6.svg)
-          应用更新：![](./img/391158c812255c07720e6ff2557e8a2b.svg)

<a name="CjZfr"></a>
## 共轭梯度
共轭梯度是一种通过迭代下降的共轭方向以有效避免Hessian矩阵求逆计算的方法。这种方法的灵盖来自于最速下降法弱点的仔细研究，其中先搜索迭代地用于梯度相关的方向上。下图说明了该方法在二次碗型目标中如何表现得，是一个相当抵消的来回往复，锯齿模式。这是因为每一个由梯度给定的线搜索方向，都保证正交于上一个线搜索方向。

![共轭梯度.png](./img/1594716918986-069fb27d-af8e-4b01-9ff9-209bf27ce3c4.png)

假设上一个搜索方向是![](./img/b8418ed096247b0d575004f429792b0f.svg)。在极小值处，线搜索终止，方向![](./img/b8418ed096247b0d575004f429792b0f.svg)处的方向导数为零：![](./img/f93476eb14ad7d800d1df26bcf0a22ee.svg)。因为该点的梯度定义了当前的搜索方向，![](./img/d542b50d264b6bef15a96f4bb2f53924.svg)将不会贡献于方向![](./img/b8418ed096247b0d575004f429792b0f.svg)。因此方向![](./img/9ebe20e92a653c5eb4f8cf32d11d267a.svg)正交于![](./img/b8418ed096247b0d575004f429792b0f.svg)。最速下降多次迭代中，方向![](./img/b8418ed096247b0d575004f429792b0f.svg)和![](./img/9ebe20e92a653c5eb4f8cf32d11d267a.svg)之间的关系如上图所示。如图战士的，下降正交方向的选择不会保持前一搜索方向上的最小值。这产生了锯齿形的过程。在当前梯度方向下降到极小值，我们必须重新最小化之前梯度方向上的目标。因此，通过遵循每次线搜索结束时的梯度，我们在某种程度上撤销了在之前线搜索的方向上取得的进展。共轭梯度试图解决这个问题。

在共轭梯度法中，我们寻求一个和先前线搜索方向共轭的搜索方向，即它不会撤销该方向上的进展。在训练迭代![](./img/e358efa489f58062f10dd7316b65649e.svg)时，下一步的搜索方向![](./img/9ebe20e92a653c5eb4f8cf32d11d267a.svg)的形式如下：

![](./img/75f936c85f2a2f3bac523a5add90ee7e.svg)

其中，系数![](./img/1ddbcb41c4a6695cf0cd32dc0387313b.svg)的大小控制我们应沿方向![](./img/b8418ed096247b0d575004f429792b0f.svg)加回多少到当前搜索方向上。

如果![](./img/69a4adf2f668394538f3c57e421eed95.svg)，其中![](./img/c1d9f50f86825a1a2302ec2449c17196.svg)是Hessian矩阵，则两个方向![](./img/9ebe20e92a653c5eb4f8cf32d11d267a.svg)和![](./img/b8418ed096247b0d575004f429792b0f.svg)被称为共轭的。

适应共轭的直接方法会涉及到![](./img/c1d9f50f86825a1a2302ec2449c17196.svg)特征向量的计算以选择![](./img/1ddbcb41c4a6695cf0cd32dc0387313b.svg)。这将无法满足我们的开发目标：寻找在大问题比牛顿法计算更加可行的方法。我们能否不进行这些计算而得到共轭方向？幸运的是这个问题的答案是肯定的。两种用于计算![](./img/1ddbcb41c4a6695cf0cd32dc0387313b.svg)的流形方法是：

1、Fletcher-Reeves： ![](./img/2c891a3a1d9c29a44c395fa3c398e720.svg)

2、Polak-Ribiere： ![](./img/a0833d0222660d250e2e51d92e1375e1.svg)

对于二次曲面而言，共轭方向确保梯度沿着前一方向大小不变。因此，我们在前一方向上仍然是极小值。其结果是，在![](./img/8ce4b16b22b58894aa86c421e8759df3.svg)维参数空间中，共轭梯度只需要至多![](./img/8ce4b16b22b58894aa86c421e8759df3.svg)次线搜索就能达到极小值。

算法过程：初始化：初始参数![](./img/dbc9011a370bca098d4752346ba71d5c.svg)，包含![](./img/6f8f57715090da2632453988d9a1501b.svg)个样本的训练集，初始化![](./img/e67fa0da747119735261943d0dc965ad.svg)，![](./img/77478491a7b54d17bbfc44cc1d8051d0.svg)，![](./img/3f3d5118e374c670258e6e2b2cfb1b0c.svg)

- while 停止准则为满足 do：
-           初始化梯度：![](./img/1350f044f7b499b4cbbf0ca3b17f2c67.svg)
-           计算梯度：![](./img/5d5641552900f31afdeecd405dbc80ab.svg)
-           计算![](./img/6345ac3a4cade25d86feb00bd54b6658.svg)(非线性共轭梯度：视情况可重置![](./img/1ddbcb41c4a6695cf0cd32dc0387313b.svg)为零，例如![](./img/e358efa489f58062f10dd7316b65649e.svg)是常数![](./img/8ce4b16b22b58894aa86c421e8759df3.svg)的倍数时)
-           计算搜索方向：![](./img/057cfea5f365a1e48bccca5b85e90da1.svg)
-           执行线搜索寻找：![](./img/6be3a0f9bcd7538497e3a7ea973913a4.svg)(对于真正二次的代价函数，存在解析解，无需显式地搜索)
-           应用更新：![](./img/8279d3ae51a2cb5a3b95c9361cf4bd3c.svg)
- ![](./img/248ace9225f2366a829b9625cb004d46.svg)

<a name="hiP5F"></a>
## BFGS
Broyden-Fletcher-Goldfarb-Shanno(BFGS)算法具有牛顿法的一些优点，但没有牛顿法的计算负担。在这方面，BFGS和CG很像。然而，BFGS使用了一个更直接的方法近似牛顿更新。回顾牛顿更新由下式给出：

![](./img/f6a09fe6261f7bfa13b992484e54344e.svg)

其中，![](./img/c1d9f50f86825a1a2302ec2449c17196.svg)是![](./img/ff44570aca8241914870afbc310cdb85.svg)相对于![](./img/2554a2bb846cffd697389e5dc8912759.svg)的Hessian矩阵在![](./img/dbc9011a370bca098d4752346ba71d5c.svg)处的估计。运用牛顿法的主要计算难点在于计算Hessian逆![](./img/5c5ca72dc2cacdf240bf25622dc47f76.svg)。拟牛顿法所采用的方法（BFGS是其中最突出的）是使用矩阵![](./img/4ef051f63460d198faa75d72a9e45518.svg)近似逆，迭代地低秩更新精度以更好地近似![](./img/5c5ca72dc2cacdf240bf25622dc47f76.svg)。

当Hessian逆近似![](./img/4ef051f63460d198faa75d72a9e45518.svg)更新时，下降方向![](./img/cbf827f0bda25dc86bfc5814511db684.svg)为![](./img/5fb5da4142636deef93ace4de33dd8eb.svg)。该方向上的线搜索用于决定该方向上的步长![](./img/cc05b0e3116f2a3220803046ce1e99e2.svg)。参数的最后更新为：

![](./img/7f441080891143bb646af30e63b0c3d8.svg)

和共轭梯度法相似，BFGS算法迭代一系列线搜索，其方向含二阶信息。然而和共轭梯度不同的是，该方法的成功并不严重依赖于线搜索寻找该方向上和真正极小值很近的一点。因此，相比于共轭梯度，BFGS的优点是其花费较少的时间改进每个线搜索。在另一方面，BFGS算法必须存储Hessian逆矩阵![](./img/69691c7bdcc3ce6d5d8a1361f22d04ac.svg)，需要![](./img/9f84a66d88d24c3b1bc91df5b5346a13.svg)的存储空间，使BFGS不适用于大多数具有百万级参数的现代深度学习模型。

存储受限的BFGS（或L-BFGS）通过避免存储完整的Hessian逆近似![](./img/69691c7bdcc3ce6d5d8a1361f22d04ac.svg)，BFGS算法的存储代价可以明显降低。L-BFGS算法使用核BFGS算法相同的方法计算![](./img/69691c7bdcc3ce6d5d8a1361f22d04ac.svg)的近似，但起始假设是![](./img/271f2c7a763781b8d4560ba30e3839a4.svg)是单位矩阵，而不是一步一步都要存储近似。如果使用精确的线搜索，L-BFGS定义的方向会使相互共轭的。然而，不同于共轭梯度法，即使只是近似线搜索的极小值，该过程的效果仍然不错。这里描述的无存储的L-BFGS方法可以扩展为包含Hessian矩阵更多的信息，每步存储一些用于更新![](./img/69691c7bdcc3ce6d5d8a1361f22d04ac.svg)的向量，且每步的存储代价是![](./img/7ba55e7c64a9405a0b39a1107e90ca94.svg)。
<a name="YkojC"></a>
# Source
[https://www.tensorflow.org/api_docs/python/tf/keras/optimizers](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers)<br />[梯度下降优化算法综述_null的专栏-CSDN博客_梯度下降算法](https://blog.csdn.net/google19890102/article/details/69942970)<br />[CS231n Convolutional Neural Networks for Visual Recognition](http://cs231n.github.io/neural-networks-3/)<br />[http://wemedia.ifeng.com/69799959/wemedia.shtml](http://wemedia.ifeng.com/69799959/wemedia.shtml)
