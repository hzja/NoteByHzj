<a name="9391d264"></a>
## RNN长期记忆的依赖问题

RNN的一个特点就是可以结合之前的信息解决当前的任务，比如利用视频前几帧来帮助理解视频当前帧的内容。有时，我们只需要关注近邻的信息就可以解决当前任务，比如我们预测“云彩飘在天”这句话最后一个词，我们很容易得到“天”。这种情况下，相关信息与当前任务距离很小，RNN完全能利用之前信息解决问题

![LSTM1.png](./img/1594261121285-45a6fca1-d1e3-41be-be01-272c8767b8ae.png)

然而，有很多情况是需要更多地信息才能解决任务。比如我们还是预测最后一个词，但是这次是一大段话“我成长在法国....我能说一口流利的法语”。近邻相关信息确定最后这个词要给一种语言，然而需要这一段的第一句话，一开头的“法国”作为有效信息才能预测出“法语”。这个距离间隔就很大了。很不幸的是，随着间隔距离增大，RNN有时不能学到那些关联的有效信息。

![LSTM2.png](./img/1594261139928-cf6c9c3e-5442-4de9-ac3b-d887b58c9477.png)

理论上，RNN完全能解决这个长期记忆的依赖问题，只需要设置合理的参数就可以。然而实际中，很困难，具体原因可见这两篇paper：[Hochreiter (1991) [German]](http://people.idsia.ch/~juergen/SeppHochreiter1991ThesisAdvisorSchmidhuber.pdf) and [Bengio, et al. (1994)](http://www-dsi.ing.unifi.it/~paolo/ps/tnn-94-gradient.pdf)。

<a name="3e75e2c8"></a>
## LSTM网络

长短期记忆网络（Long Short Term Memory networks(LSTMs)）是一种特殊的RNN，旨在避免长期记忆的依赖问题。所有循环神经网络都具有神经网络重复模块链的形式。在标准RNNs中，这个重复模块有很简单的结构，比如tanh层，如下图。

![LSTM3.png](./img/1594261166120-6150f4b2-ecbe-424d-9353-1f2cafb749a4.png)

LSTMs也有这样的链式结构，但是重复模块也些许区别。不同于标准RNN的一层（比如上例tanh），LSTM的更新模块具有4个不同的层相互作用，如下图。

![LSTM4.png](./img/1594261186753-47df39ba-b953-4ba8-877b-e16d0587c442.png)

![LSTM5.png](./img/1594261200703-1e070988-69cf-4388-b62a-aa358ff06e5f.png)

在上图中，每一行都携带一个完整的向量，从一个节点的输出到其他节点的输入。粉色圆圈表示逐点运算，如矢量加法，而黄色框表示神经网络层。行合并表示连接，而行分叉表示其内容被复制，副本将转移到不同的位置。

<a name="bb2660da"></a>
## LSTM的核心思想

从上一部分的图中可以看出，在每个序列索引位置![](./img/e358efa489f58062f10dd7316b65649e.svg)时刻向前传播的除了和RNN一样的隐藏状态![](./img/6ece45e3f78470bcf0e7db1d3c539a09.svg)，还多了另一个隐藏状态，如下图中上面的长横线。这个隐藏状态我们一般称为细胞状态(Cell State)，是LSTM的关键。细胞状态有点像传送带。它直接沿着整个链运行，只有一些微小的线性相互作用。信息很容易沿着它不变地流动。

![LSTM6.png](./img/1594261218897-2a641b45-150e-45ad-b0fb-04f9c3c37c25.png)

LSTM能够移除或添加信息到细胞状态，由称为门(Gates)的结构进行调节。门会选择性的让信息穿过。它们是由sigmoid神经网络层和逐点乘法运算组成。sigmoid层输出![](./img/cfcd208495d565ef66e7dff9f98764da.svg)到![](./img/c4ca4238a0b923820dcc509a6f75849b.svg)之间的数字，描述每个组件应该通过多少。值为![](./img/cfcd208495d565ef66e7dff9f98764da.svg)意味着“不让任何东西通过”，而值为![](./img/c4ca4238a0b923820dcc509a6f75849b.svg)则意味着“让一切都通过！”

![LSTM7.png](./img/1594261294920-ac9bb2a0-3726-4df4-88e2-290b4cbf4f27.png)

LSTM在在每个序列索引位置![](./img/e358efa489f58062f10dd7316b65649e.svg)的门一般包括遗忘门，输入门和输出门三种来保护和控制细胞状态。

<a name="57b39c96"></a>
## LSTM整体流程

<a name="285cabdc"></a>
### 步骤一：遗忘

LSTM的第一步是确定我们将从细胞状态中丢弃的信息。这个决定是由称为“遗忘门层”的Sigmoid层决定的。这里的![](./img/a2ab7d71a0f07f388ff823293c147d21.svg)是指Sigmoid函数，对于状态![](./img/b2236e0874d0f2ae943cd6bd45752b03.svg)矩阵当中每个输入的值，都会有对应的一个输出的值，输出的值在![](./img/ccfcd347d0bf65dc77afe01a3306a96b.svg)之间，相当于是决定了遗忘多少部分。如果输出值为![](./img/c4ca4238a0b923820dcc509a6f75849b.svg)，说明全部保留，不删除原本的记忆，如果是![](./img/cfcd208495d565ef66e7dff9f98764da.svg)，说明状态![](./img/b2236e0874d0f2ae943cd6bd45752b03.svg)矩阵对应的这个值全部删除。

![LSTM8.png](./img/1594261373571-c425f80f-f764-4467-9dbf-a7adda36b3e8.png)

<a name="bde9f13f"></a>
### 步骤二：输入

下一步是确定我们将在细胞状态中存储哪些新信息。这里有两部分同时进行：第一部分使用了sigmoid激活函数，输出为![](./img/56e99db17308c13a71cfc5da5a3165df.svg)，决定哪些值我们将进行更新；第二部分使用了tanh函数创建新候选值的向量![](./img/3fabb2bbfec4e86bd37b19a4c876b193.svg)，此向量可被加至状态。下一步，我们结合这两者给状态来创建一个更新。

![LSTM9.png](./img/1594261408381-6e05f564-4153-44bb-ab50-101660f11cef.png)

<a name="ee225bbc"></a>
### 步骤三：更新

现在更新旧细胞状态![](./img/b2236e0874d0f2ae943cd6bd45752b03.svg)至![](./img/eda52b14c608f00393693a057a6fea5e.svg)：我们将旧状态乘以![](./img/bae155438877126b42cbddee193c048a.svg)，忘记我们之前决定忘记的事情。然后我们添加![](./img/a9c46028333b8d92889f8bfe7ca26a58.svg)。这是新的候选值，根据我们决定更新每个状态的值来缩放。

![LSTM10.png](./img/1594261460353-3a96acf7-7206-426d-b9b1-6e3215fa51ec.png)

<a name="d6b7fa13"></a>
### 步骤四：输出

最后，我们需要决定我们要输出的内容。此输出将基于我们的细胞状态，但将是过滤版本。首先，我们运行一个sigmoid来决定细胞状态的哪些部分会被输出。然后，我们将细胞状态代入tanh（将值转化到介于![](./img/6bb61e3b7bce0931da574d19d1d82c88.svg)和![](./img/c4ca4238a0b923820dcc509a6f75849b.svg)之间）并将其乘以sigmoid门的输出，以便我们只输出我们决定的部分。

![LSTM11.png](./img/1594261498732-d337d77b-852a-48b9-ac54-06326177fb2c.png)

<a name="BiLSTM"></a>
## BiLSTM

双向LSTM（BiLSTM）和双向RNN所述同理，有些时候预测可能需要由前面若干输入和后面若干输入共同决定，这样会更加准确。因此提出了双向循环神经网络，网络结构如下图。

![LSTM12.png](./img/1594261515249-aeb80400-700a-4795-8ed3-593b1fc4d173.png)

可以看到Forward层和Backward层共同连接着输出层，其中包含了![](./img/1679091c5a880faf6fb5e6087eb1b2dc.svg)个共享权值![](./img/3f6e2ddf0638140208660d549fb9786b.svg)。

![LSTM13.png](./img/1594261568595-c22b84bd-40ca-4e1c-9e9f-d72782474b55.png)

![LSTM14.png](./img/1594261589031-75be08e8-8369-4e6a-adab-953abc0cdc12.png)

在Forward层从![](./img/c4ca4238a0b923820dcc509a6f75849b.svg)时刻到![](./img/e358efa489f58062f10dd7316b65649e.svg)时刻正向计算一遍，得到并保存每个时刻向前隐含层的输出。在Backward层沿着时刻![](./img/e358efa489f58062f10dd7316b65649e.svg)到时刻![](./img/c4ca4238a0b923820dcc509a6f75849b.svg)反向计算一遍，得到并保存每个时刻向后隐含层的输出。最后在每个时刻结合Forward层和Backward层的相应时刻输出的结果得到最终的输出，用数学表达式如下：

![](./img/17ce1de3533bb2f071821d2360dcd110.svg)<br />![](./img/de362a9b6157cb1f8be7fed8dc74d857.svg)<br />![](./img/d3da7759df2fb4ef004f851c8abe77c9.svg)

例子：前向的LSTM与后向的LSTM结合成BiLSTM。比如，我们对“我爱中国”这句话进行编码，模型下图所示

![LSTM15.png](./img/1594261746971-e4b41d75-a442-4823-a557-4551d868c512.png)

前向的LSTM依次输入“我”，“爱”，“中国”得到三个向量![](./img/fd00bd1a76def446efe0ccbf8890603d.svg)。后向的LSTM依次输入“中国”，“爱”，“我”得到三个向量![](./img/e6a9267f8bf2189ac5fd2b34ccf52e43.svg)。最后将前向和后向的隐向量进行拼接得到![](./img/439ba239a94a7ad901d860d81fecdddd.svg)，即![](./img/0b01466d77df26ab4f4350dd880f4a03.svg)。

对于情感分类任务来说，我们采用的句子的表示往往是![](./img/4419e03bc6d78e542104a6575a504791.svg)。因为其包含了前向与后向的所有信息，如下图<br />![LSTM16.png](./img/1594261791454-146471b7-f836-4aef-8968-7af7a88e5112.png)

<a name="Source"></a>
## Source

[http://colah.github.io/posts/2015-08-Understanding-LSTMs/](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)<br />[https://blog.csdn.net/anshuai_aw1/article/details/85168486](https://blog.csdn.net/anshuai_aw1/article/details/85168486)<br />[http://www.tensorflownews.com/2018/05/04/keras_lstm/](http://www.tensorflownews.com/2018/05/04/keras_lstm/)<br />[https://www.jiqizhixin.com/articles/2018-10-24-13](https://www.jiqizhixin.com/articles/2018-10-24-13)
