数据科学 回归模型
<a name="c1nem"></a>
## 线性回归
用量化特征预测连续区间的响应。
<a name="hgmpA"></a>
### 回归
回归：建立因变量y和自变量x之间的函数关系

y：希望被预测或被解释的变量，目标或响应;<br />x：预测变量，一般指容易获得的样本特征;

- x向量
   - 包含一个特征，即一元回归;
   - 包含多个特征，即多元回归;
- 函数关系f
   - f是线性，线性回归;
   - f是非线性，非线性回归;
<a name="sEKiv"></a>
#### 一元线性回归

截距<br />回归系数<br />预测值<br />观测值<br />![image.png](https://cdn.nlark.com/yuque/0/2021/png/396745/1628005948859-de32df63-3a77-4846-b079-601e2e947a3d.png#clientId=u4afaeef1-d808-4&from=paste&height=322&id=u8918fe84&originHeight=429&originWidth=661&originalType=binary&ratio=1&size=174671&status=done&style=shadow&taskId=u87d39845-c2d5-48e6-a27d-6eb77907fa6&width=496)

：残差（选择直线的依据）<br />通过残差平方和最小可以转换为求一阶导数的零点


回归直线使残差平方和最小的直线，且通过点<br />，RMSE与y有相同单位或量纲;<br />这个求回归线的方法也叫最小二乘法。
:::info
Sklearn库<br />—linear_model模块<br />—LinerRegression对象<br />可以用来做线性回归。
:::
```python
import pandas as pd
import numpy as np
from scipy import stats
from matplotlib import pyplot as plt
from sklearn.linear_model import LinearRegression
from sklearn import metrics
my_iris=pd.read_csv('C:\Python\Scripts\my_data\iris.csv',sep=',',decimal='.',
                    header=None,
                    names=['sepal_length','sepal_width',
                           'petal_length','petal_width','target'])
feature_cols='petal_length'
# feature_cols='sepal_width'
x=my_iris[[feature_cols]]
y=np.array(my_iris['sepal_length'])
plt.plot(x,y,'o',alpha=0.5)
linreg=LinearRegression()
linreg.fit(x,y)
print('f(x) = ',linreg.intercept_,'+',linreg.coef_[0],'x')
pred_y=linreg.predict(x)
plt.plot(x,pred_y,'g',alpha=0.5)
plt.plot(np.array(x).mean(),y.mean(),'r*',ms=12)
plt.gca().set_xlabel(feature_cols)
plt.gca().set_ylabel('sepal_length')
print('RMSE = ',np.sqrt(metrics.mean_squared_error(y,pred_y)))
print('\n')
```
![image.png](https://cdn.nlark.com/yuque/0/2021/png/396745/1628081834485-14e0c178-ef11-41cd-a85b-f5ebdd2651f9.png#clientId=uca656970-0f60-4&from=paste&height=419&id=u45926a44&originHeight=1256&originWidth=1588&originalType=binary&ratio=1&size=198373&status=done&style=shadow&taskId=u3ceceec4-22bd-48a6-b6d2-d33b8610ebd&width=529.3333333333334)
<a name="K1xr1"></a>
#### 如何评价回归模型的好与不好?
给出不重复的样本，总是能找到一条回归线<br />![image.png](https://cdn.nlark.com/yuque/0/2021/png/396745/1628084078249-7bf3a5d6-850f-4344-bc55-14a3fae98a89.png#clientId=uca656970-0f60-4&from=paste&height=203&id=u32a3ad26&originHeight=609&originWidth=1555&originalType=binary&ratio=1&size=700587&status=done&style=none&taskId=u93e45431-e193-4254-b628-c89873a0f04&width=518.3333333333334)<br />但是找到的这条回归线真的有意义吗?此时就需要引入回归效果评价参数。<br />定义：回归平方和与样本的总平方和之比，也等于1减去残差平方和和总平方和之比。

总平方和：样本相对于样本均值的总离差。<br />回归平方和：由于回归函数所引入的样本相对于均值的离差，属于样本变异性中可以被回归模型解释的部分。<br />意义：用模型可解释部分与总离差比较，取值在0到1间。<br />>越靠近1，模型可解释成分越多，模型性能越好;<br />>越靠近0，性能越不好;<br />查看刚刚模型的r_square
```javascript
print('r_square = ',linreg.score(x,y))
```
![image.png](https://cdn.nlark.com/yuque/0/2021/png/396745/1628084820305-57658db8-c854-4edb-af9d-df06c9c60f0f.png#clientId=u1f058997-d399-4&from=paste&height=44&id=u33fb00b0&originHeight=132&originWidth=904&originalType=binary&ratio=1&size=21017&status=done&style=shadow&taskId=u7aa3332d-c582-4304-9cc9-a0c603ef363&width=301.3333333333333)
<a name="mzMJH"></a>
### 线性回归与线性相关
| 线性相关分析 | 线性回归分析 |
| --- | --- |
| 线性相关系数反映两个变量的耦合程度，其中两个变量的地位平等，不能用一个变量去预测另一个。 | 强调回归函数的确定，直接建立起两个变量间的函数关系，以期用x去预测y。 |
| 取值:[-1,1] | 取值:[0,1] |


```javascript
print(my_iris[[feature_cols,'sepal_length']].corr())
print('\n')
r=np.array(my_iris[[feature_cols,'sepal_length']].corr()[['sepal_length']].
iloc(0)[0])
print('r = ',r)
print('square of r = ',r**2)
```
![image.png](https://cdn.nlark.com/yuque/0/2021/png/396745/1628085100042-22bff052-ceee-4175-85dd-a8ffb401de58.png#clientId=u1f058997-d399-4&from=paste&height=189&id=u7c59e30e&originHeight=566&originWidth=1138&originalType=binary&ratio=1&size=90906&status=done&style=shadow&taskId=u45d9be6d-24f2-4f85-be20-635901b3a68&width=379.3333333333333)<br />这里可以看到petal_length和sepal_length的相关系数为0.87，他的平方正好是之前求出的r_square。

| —元线性回归 | x有一列特征 |
| --- | --- |
| 多元线性回归 | x有多列特征 |
| 特征越多→模型越复杂→过拟合 |  |

过拟合：模型只在建模的数据上性能好，一旦面对新数据性能就非常糟糕。<br />建模的目的是用模型去预测新的数据。所以过拟合的问题是一定要避免的，先保留这个问题。
<a name="zlCf0"></a>
## 逻辑回归——二分类的实现
办法：在线性回归的基础上，对输出所在的连续区间做阈值划分。<br />如：规定模型输出值低于阈值属于一类，高于阈值则属于另一类。<br />用量化特征预测某事发生的概率。概率取值区间：[0，1]
<a name="xbXJ0"></a>
### Logistic回归
<a name="X3WvP"></a>
#### Logistic回归的图像
![image.png](https://cdn.nlark.com/yuque/0/2021/png/396745/1628085832400-48777ea3-4241-425b-8a5d-04a32a623b25.png#clientId=u1f058997-d399-4&from=paste&height=183&id=u4d10c208&originHeight=548&originWidth=790&originalType=binary&ratio=1&size=314547&status=done&style=none&taskId=u7aaf5e46-25ce-4047-b050-8e852daf3b0&width=263.3333333333333)
<a name="T0UMU"></a>
#### Logistic回归函数

Logistic输出：预测变量-某事件的发生概率<br />输入t是样本特征的线性函数作为自变量。<br />引入另一个概念Odds：<br />某事件发生的概率，记作P;<br />该事件的几率：该事件发生的概率与不发生的概率之比。

对log(Odds)做线性回归：

```python
import pandas as pd
import numpy as np
from matplotlib import pyplot as plt
table=pd.DataFrame({'prob':[0.01,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,0.99]})
table['odds']=table['prob']/(1-table['prob'])
table['log-odds']=np.log(table['odds'])
table
```
![image.png](https://cdn.nlark.com/yuque/0/2021/png/396745/1628086310341-0d7e9cd0-c541-48ee-9b88-d1d251a3013c.png#clientId=u1f058997-d399-4&from=paste&height=259&id=u92308262&originHeight=777&originWidth=1295&originalType=binary&ratio=1&size=207836&status=done&style=shadow&taskId=u74d9637a-1754-4d95-8b0b-a216ad6669f&width=431.6666666666667)<br />通过结果可以看到几率都是正数，而对数几率则有正有负，并且关于.0,0.5对称。还可以画出三个变量的曲线
```python
#plt.subplot(2,2,1)
plt.plot(table['prob'],'g')
plt.plot(table['odds'],'y')
plt.plot(table['log-odds'],'m')
plt.legend({'probability','Odds','log_odds'})
plt.ylim([-6,6])
```
![image.png](https://cdn.nlark.com/yuque/0/2021/png/396745/1628086486356-32745e66-97df-4bef-b67a-f7be5201c354.png#clientId=u1f058997-d399-4&from=paste&height=341&id=u0e98a3dd&originHeight=1023&originWidth=1603&originalType=binary&ratio=1&size=117815&status=done&style=shadow&taskId=u455ee0c6-d52b-45b5-9b32-a703104bfa9&width=534.3333333333334)<br />对数几率的值域：

基于样本的特征构建线性函数，函数值对应事件的对数几率。

:::success
Logistic回归就是对几率做线性回归。
:::
优化准则：极大化所有样本的对数似然函数。


:::info
sklearn库<br />—linear_model模块<br />一LogisticRegression对象<br />可以做Logistic回归
:::
```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LogisticRegression
from sklearn import metrics
from scipy import stats
from sklearn.model_selection import train_test_split
bikes=pd.read_csv("C:\Python\Scripts\my_data\\bikeshare.csv")
# 注意\b 是转义字符，表示退格，所以\\表示\本身
print(bikes.shape)
```
![image.png](https://cdn.nlark.com/yuque/0/2021/png/396745/1628086940367-05502697-5869-42e8-bfd3-4e824405779b.png#clientId=u1f058997-d399-4&from=paste&height=48&id=ue647f8c3&originHeight=145&originWidth=1031&originalType=binary&ratio=1&size=10932&status=done&style=shadow&taskId=ube3a76c9-a625-4dc7-89ba-45ecfd2bf54&width=343.6666666666667)
```python
bikes.head()
```
![image.png](https://cdn.nlark.com/yuque/0/2021/png/396745/1628086968421-e801ba41-39f7-43e2-84fd-48fbbb8e55e3.png#clientId=u1f058997-d399-4&from=paste&height=276&id=u31592d1b&originHeight=828&originWidth=1811&originalType=binary&ratio=1&size=265710&status=done&style=shadow&taskId=ub432f067-835a-42a3-b73e-b07337bfbcb&width=603.6666666666666)<br />阶段1：建模<br />训练集：建立模型时给模型的数据。<br />阶段2：模型应用<br />测试集：模型建立后，用来测试模型性能的数据。
```python
feature_cols=['temp']
x=bikes[feature_cols]
bikes['above_average']=bikes['count']>=bikes['count'].mean()
y=bikes['count']>=bikes['count'].mean()
x_train,x_test,y_train,y_test=train_test_split(x,y)
logreg=LogisticRegression()
logreg.fit(x_train,y_train)
#print((y_test.values))
print(pd.DataFrame(np.transpose([y_test.values,logreg.predict(x_test)]),
columns={'真实值','预测值'}))
print('\n')
print('分类准确率是:',logreg.score(x_test,y_test)) # 评分函数
```
![image.png](https://cdn.nlark.com/yuque/0/2021/png/396745/1628087050887-a9183130-c40e-45b5-a613-004adc0c742f.png#clientId=u1f058997-d399-4&from=paste&height=140&id=u329e1c8e&originHeight=421&originWidth=1026&originalType=binary&ratio=1&size=58519&status=done&style=shadow&taskId=ud2e9be64-65cc-488e-bfaa-97a85dd374a&width=342)<br />![image.png](https://cdn.nlark.com/yuque/0/2021/png/396745/1628087077244-8f89d765-e9c0-4e93-a218-d73552b83d36.png#clientId=u1f058997-d399-4&from=paste&height=500&id=uf83512ec&originHeight=1500&originWidth=1852&originalType=binary&ratio=1&size=276146&status=done&style=shadow&taskId=ufb223232-03b2-49b1-acbe-19d4cfb7715&width=617.3333333333334)<br />对于四季非数值型数据数据的处理，可以做one-hot编码

| 特征取值\\编码 | 编码_1 | 编码_2 | 编码_3 | 编码_4 |
| --- | --- | --- | --- | --- |
| 1 | 1 | 0 | 0 | 0 |
| 2 | 0 | 1 | 0 | 0 |
| 3 | 0 | 0 | 1 | 0 |
| 4 | 0 | 0 | 0 | 1 |

四位二进制码，编码长度=类别个数<br />可以将四季转换为4位的二进制编码。可以使用Pandas中的 `get_dummies` 函数进行四季的one-hot编码转换。
```python
bikes.groupby('season').above_average.mean().plot(kind='bar')
when_dummies=pd.get_dummies(bikes['season'],prefix='season_')
when_dummies.head()
```
![image.png](https://cdn.nlark.com/yuque/0/2021/png/396745/1628087664798-fefa8e08-90fe-48c8-9bd9-31e704fe6183.png#clientId=uc6538502-b106-4&from=paste&height=153&id=uc8b3c0ac&originHeight=460&originWidth=1509&originalType=binary&ratio=1&size=63665&status=done&style=shadow&taskId=u6efb045a-fe31-4ec8-95e0-c64e8f4284b&width=503)<br />![image.png](https://cdn.nlark.com/yuque/0/2021/png/396745/1628087675363-f6424db0-b0b6-4e9c-a83d-0d83453e0cc5.png#clientId=uc6538502-b106-4&from=paste&height=348&id=uaa40abad&originHeight=1044&originWidth=1726&originalType=binary&ratio=1&size=69982&status=done&style=shadow&taskId=u1a666826-9bb8-48ac-ad67-84cd4b447d3&width=575.3333333333334)<br />使用转换后四季的编码重新构建Logistic回归模型
```python
when_dummies=when_dummies.iloc[:,1:] # 去除第一列
when_dummies.head()
#new_bike=pd.concat([bikes[['temp','humidity']],when_dummies],axis=1)
new_bike=pd.concat([bikes[['temp']],when_dummies],axis=1)
x=new_bike
x_train,x_test,y_train,y_test=train_test_split(x,y)
logreg=LogisticRegression()
logreg.fit(x_train,y_train)
y_pred=logreg.predict(x_test)
#print(y_pred)
print('用气温、季节同时作为预测自变量，预测的准确率为：',logreg.score(x_test,y_test))
```
![image.png](https://cdn.nlark.com/yuque/0/2021/png/396745/1628087768041-499ab812-5c71-4a6e-bdf1-e6e41e9c14e0.png#clientId=uc6538502-b106-4&from=paste&height=75&id=uf55b3166&originHeight=226&originWidth=1758&originalType=binary&ratio=1&size=67084&status=done&style=shadow&taskId=uab2e6672-bc5b-4e3b-b07c-f9c11cdda24&width=586)
<a name="v4aqM"></a>
### 逻辑回归的评价依据
分类的准确率，即准确分类的样本数除以样本容量。
