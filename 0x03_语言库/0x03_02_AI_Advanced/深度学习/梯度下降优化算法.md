AI
<a name="j2Zc6"></a>
## 1、简介
梯度下降是一种求**函数最小值**的优化算法。从函数上的一个**随机点开始**，沿着函数**梯度的负方向移动**，以达到**局部/全局最小值**。<br />损失函数让我们量化任何特定的权重集合 ![](https://cdn.nlark.com/yuque/__latex/a36915ecf0b5605493f5aeaf1480a9ac.svg#card=math&code=W&id=QQXNZ) 的质量。优化的目标是找到**使损失函数最小化**的 ![](https://cdn.nlark.com/yuque/__latex/a36915ecf0b5605493f5aeaf1480a9ac.svg#card=math&code=W&id=jcrda)。
<a name="vNvce"></a>
### 策略1：随机搜索
由于时检查给定的参数 ![](https://cdn.nlark.com/yuque/__latex/a36915ecf0b5605493f5aeaf1480a9ac.svg#card=math&code=W&id=CktZp) 的好坏，所以可能想到的第一个想法是简单地**尝试许多不同的随机权重**，并跟踪哪个最有效。
```python
bestloss = float("inf") 
for num in range(1000):
    W = np.random.randn(10, 3073) * 0.0001 # generate random parameters
    loss = Loss(X_train, Y_train, W) # get the loss over the entire training set
    if loss < bestloss: # keep track of the best solution
        bestloss = loss
        bestW = W
```
上述策略是一个非常糟糕的方法，因为随机充满不确定性，可能不包含全局最优 ![](https://cdn.nlark.com/yuque/__latex/a36915ecf0b5605493f5aeaf1480a9ac.svg#card=math&code=W&id=HkK6k)。<br />**核心理念：迭代细化**。其核心思想是，找到**最佳**的权值 ![](https://cdn.nlark.com/yuque/__latex/a36915ecf0b5605493f5aeaf1480a9ac.svg#card=math&code=W&id=L14I6) 集是一个**非常困难甚至不可能**的问题(特别是当 ![](https://cdn.nlark.com/yuque/__latex/a36915ecf0b5605493f5aeaf1480a9ac.svg#card=math&code=W&id=RKjmk) 包含了整个复杂神经网络的权值时)，但将**特定的权值** ![](https://cdn.nlark.com/yuque/__latex/a36915ecf0b5605493f5aeaf1480a9ac.svg#card=math&code=W&id=ZxXQb) 集**细化到稍好一点**的问题就明显不那么困难了。换句话说，将从**随机**的 ![](https://cdn.nlark.com/yuque/__latex/a36915ecf0b5605493f5aeaf1480a9ac.svg#card=math&code=W&id=XuaM8) 开始，然后**迭代地优化**它，使它**每次都稍微好一点**。<br />策略将是从**随机权重开始**，随着时间的推移**迭代改进**它们，以降低损失。<br />**蒙眼下山类比：**把自己想象成在一个丘陵地带徒步旅行，蒙上眼睛，并试图到达底部。在山上的每一个点上，都有一个特殊的损失(地形的高度)。
<a name="B17DK"></a>
### 策略2：随机局部搜索
具体来说，将从一个**随机** ![](https://cdn.nlark.com/yuque/__latex/a36915ecf0b5605493f5aeaf1480a9ac.svg#card=math&code=W&id=XzmLA) 开始，对其产生**随机扰动** ![](https://cdn.nlark.com/yuque/__latex/3fa965cb201e851a8a10e200fce6ef8f.svg#card=math&code=%5Cdelta%20W&id=FrwJo)，如果扰动 ![](https://cdn.nlark.com/yuque/__latex/dda183a43c572d75cdbe174f14a091cb.svg#card=math&code=W%2B%5Cdelta%20W&id=pDzY0) 处的**损失较低**，将**执行更新**。这个过程的代码如下:
```python
W = np.random.randn(10, 3073) * 0.001 # generate random starting W
bestloss = float("inf")
for i in range(1000):
    step_size = 0.0001
    Wtry = W + np.random.randn(10, 3073) * step_size
    loss = L(Xtr_cols, Ytr, Wtry)
    if loss < bestloss:
        W = Wtry
        bestloss = loss
```
仍然浪费和计算昂贵。
<a name="IELYy"></a>
### 策略3：遵循梯度
在上述中，试图**在权重空间中找到一个方向**，以改善权重向量(并降低损失)。<br />事实证明**不需要通过随机搜索寻找一个好方向**：可以通过**数学保证的下降最陡的方向**作为最好的方向计算权向量(至少在步长趋于0时是这样)。<br />这个方向**与损失函数的梯度有关**。在徒步旅行类比中，这种方法大致相当于感觉脚下的山坡坡度，然后沿着感觉**最陡的方向**往下走。<br />在一维函数中，**斜率**是函数在任何**感兴趣的点**上的**瞬时变化率**。梯度是**斜率的泛化**对于那些**不接受单个数字**而是一个**数字向量的函数**。<br />此外，**梯度**只是**输入空间**中**每个维度**的**斜率向量**(通常称为导数)。**一维函数**对其输入**求导的数学表达式**为：<br />![](https://cdn.nlark.com/yuque/__latex/88b60e36aac218b20d432fc890c13d0b.svg#card=math&code=%5Cfrac%7Bd%20f%28x%29%7D%7Bd%20x%7D%3D%5Clim%20_%7Bh%20%5Crightarrow%200%7D%20%5Cfrac%7Bf%28x%2Bh%29-f%28x%29%7D%7Bh%7D&id=CDJNR)<br />当感兴趣的函数取一个向量而不是一个数时，称这些导数为**偏导数**，而梯度就是各维上的**偏导数的向量**。
<a name="dXDJv"></a>
## 2、梯度计算
有两种计算梯度的方法：

1. 一种**缓慢**，近似但简单的方法(**数值梯度**)
2. 一种**快速**，精确但更容易出错的方法，需要微积分(**解析梯度**)。
<a name="TsLFk"></a>
### 用有限差分进行梯度数值计算
上面给出的公式允许用数值方法计算梯度。根据上面的公式，可以对梯度进行数值计算。这是一个泛型函数，它取一个函数 ![](https://cdn.nlark.com/yuque/__latex/18f3c2855f0e85a1ac2257f64d917144.svg#card=math&code=f&id=pukzO)，一个向量 ![](https://cdn.nlark.com/yuque/__latex/712ecf7894348e92d8779c3ee87eeeb0.svg#card=math&code=x&id=StyPa) 来计算梯度，并返回 ![](https://cdn.nlark.com/yuque/__latex/18f3c2855f0e85a1ac2257f64d917144.svg#card=math&code=f&id=pUQFi) 在 ![](https://cdn.nlark.com/yuque/__latex/712ecf7894348e92d8779c3ee87eeeb0.svg#card=math&code=x&id=LQ3cz) 处的梯度：
```python
def eval_numerical_gradient(f, x):
    """
    计算 f 在 x 处的数值梯度
    - f 只有一个输入变量
    - x 是需要计算梯度的点
    """

    y = f(x)
    grad = np.zeros(x.shape)
    h = 1e-5

    # 迭代遍历 x 的索引
    it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])
    while not it.finished:
        # 计算不同维度的偏差数
        ix = it.multi_index
        yh = f(x[ix]+ h)  # 计算 f(x + h)

        # 计算维度 ix 的偏导数
        grad[ix] = (yh - y) / h  # 斜率
        it.iternext()  # 下一个维度
    return grad
```
根据上面给出的梯度公式，上面的代码在**所有维度上逐一迭代**，沿着这个维度对 ![](https://cdn.nlark.com/yuque/__latex/67df0f404d0960fadcc99f6258733f22.svg#card=math&code=h&id=gclSw) 做一个**小的改变**，然后通过观察函数的改变量来计算损失函数沿着这个维度的偏导数。变量 ![](https://cdn.nlark.com/yuque/__latex/a3a06291d68c5e4f57e53afa593380b2.svg#card=math&code=grad&id=c6qn6) 在最后保持完整的梯度。<br />注意，在数学公式中，梯度定义为 ![](https://cdn.nlark.com/yuque/__latex/67df0f404d0960fadcc99f6258733f22.svg#card=math&code=h&id=c1w7X) 趋于 ![](https://cdn.nlark.com/yuque/__latex/22d0feea96d3bb2fc273f7598ce748c1.svg#card=math&code=0&id=frHpo) 时的极限，但在实践中，通常使用一个非常小的值(如示例中所示的1e-5)就足够了。<br />理想情况下，使用不会导致数值问题的最小步长。此外，在实践中，![](https://cdn.nlark.com/yuque/__latex/bd371b3faa4d3844b53eca05cdf38641.svg#card=math&code=%5Bf%28x%2Bh%29-f%28x-h%29%5D%2F2h&id=Noakj)使用**中心差分公式**计算数值梯度通常效果更好。<br />可以用上面给出的函数来计算任意点和任意函数的梯度：
```python
W = np.random.rand(10, 3073) * 0.001 # random weight vector
grad = eval_numerical_gradient(loss_fun, W) # get the gradient
```
梯度说明**损失函数沿每个维度的斜率**，可以用它来进行更新：
```python
for step_size in [-10, -9, -8, -7, -6, -5,-4,-3,-2,-1]:
    step = 10 ** step_size
    W_new = W - step * grad # 新的权重
    loss_new = loss_fun(W_new)
```
**以负梯度方向更新。** 在上面的代码中，为了计算 ![](https://cdn.nlark.com/yuque/__latex/9a8637dead0ee19e76e43b19b12a5790.svg#card=math&code=W_%7Bnew%7D&id=mSGy4)，在梯度 ![](https://cdn.nlark.com/yuque/__latex/a3a06291d68c5e4f57e53afa593380b2.svg#card=math&code=grad&id=np8UJ) 的**负方向上进行更新**，因为希望**损失函数减少**，而不是增加。<br />**步长影响。** 梯度说明函数的**最大增长率**的方向，但它没有说明沿着这个方向**应该走多远**。选择步长(也称为**学习率**)将成为训练神经网络中最重要的(也是最令人头疼的)超参数设置之一。<br />在蒙眼下山类比中，我们感觉脚下的山在向某个方向倾斜，但我们应该走的步长是不确定的。如果小心地拖着脚走，我们可以期望取得一致但**非常小的进步**(这相当于有一个小的步长)。相反地，也可以选择**迈一大步**，自信地走一步，试图更快地下降，但这可能不会有回报。正如在上面的代码示例中所看到的，在某些情况下，当越界时，采取更大的步骤会带来更大的损失。<br />**效率问题。** 计算数值梯度的**复杂度与参数的数量成线性关系**。在示例中，损失函数每次执行一个参数更新，如有上万个参数则要执行上万次。这个问题只会变得更糟，因为现代神经网络可以轻易地拥有数千万个参数。显然，这种策略是不可扩展的，需要更好的策略。
<a name="lHEoR"></a>
### 用微积分解析计算梯度
**数值梯度**是非常简单的使用**有限差分近似**计算，但缺点是它**是近似**(因为必须选择一个小 ![](https://cdn.nlark.com/yuque/__latex/67df0f404d0960fadcc99f6258733f22.svg#card=math&code=h&id=w678j) 的价值，而真正的梯度定义为 ![](https://cdn.nlark.com/yuque/__latex/67df0f404d0960fadcc99f6258733f22.svg#card=math&code=h&id=fmMge) 趋于零的极限)，而且它的计算非常昂贵的计算。<br />计算梯度的第二种方法是**解析微积分**，它允许推导一个梯度的直接公式(没有近似)，计算起来也非常快。与数值梯度不同的是，它在实现时可能**更容易出错**，这就是为什么在实践中，计算解析梯度并将其与数值梯度进行比较，以检查实现的正确性是非常常见的。这叫做**梯度检查**。
<a name="yGbxL"></a>
### 梯度下降
现在可以计算损失函数的梯度，**重复计算梯度然后进行参数更新的过程称为梯度下降**。它的普通版本如下：
```python
# Vanilla Gradient Descent
while True:
    grad = evaluate_gradient(loss_fun, data, weights)
    weights += - step_size * grad # perform parameter update
```
这个简单的**循环**是所有神经网络库的核心。还有其他的优化方法(如LBFGS)，但梯度下降是目前最常用和最成熟的优化神经网络损失函数的方法。<br />**Mini-batch梯度下降法。** 在大规模应用程序(如ILSVRC挑战)中，训练数据可以有数百万个示例。因此，为了只执行一个参数更新，在**整个训练集上**计算完整的损失函数似乎很浪费。解决这一问题的一种非常常见的方法是计算**分批训练数据**的梯度。例如，在当前最先进的ConvNets中，一个典型的批处理包含整个120万个训练集中的256个示例。然后使用该批处理执行参数更新。
```python
# Vanilla Minibatch Gradient Descent
while True:
    data_batch = sample_training_data(data, 256) # sample 256 examples
    grad = evaluate_gradient(loss_fun, data_batch, weights)
    weights += - step_size * grad # perform parameter update
```
这种方法之所以有效，是因为**训练数据中的示例是相关的**。当**某些样本相同**的时候，将得到**完全相同的损失**。在实践中，数据集**不会包含重复的图像**，一个**小批量的梯度**是全目标梯度的一个很好的**近似**。因此，在实际应用中，通过评估小批量梯度来进行**更频繁的参数更新**，可以实现**更快的收敛速度**。<br />极端情况是，minibatch只包含一个示例。这个过程称为**随机梯度下降(SGD)**(有时也称为在线梯度下降)。这种情况比较少见，因为在实践中，由于向量化代码优化，计算100个例子的梯度比计算一个例子100次的梯度要高效得多。
