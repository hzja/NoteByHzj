深度学习<br />目前深度学习中的神经网络种类繁多，用途各异。由于这个分支在指数增长，跟踪神经网络的不同拓扑有助于更深刻的理解。本文将展示神经网络中最常用的拓扑结构，并简要介绍其应用。
<a name="vNcj6"></a>
# 1. 感知器(Perceptron(P)) 
![](https://cdn.nlark.com/yuque/0/2020/png/396745/1601517602067-9b7e161b-9997-4c48-9500-5126f2cf697a.png#align=left&display=inline&height=316&originHeight=316&originWidth=575&size=0&status=done&style=shadow&width=575)<br />感知器模型也称为单层神经网络。这个神经网络只包含两层:

- 输入层<br />
- 输出层<br />

这种类型的神经网络没有隐藏层。它接受输入并计算每个节点的加权。然后，它使用激活函数(大多数是Sigmoid函数)进行分类。<br />应用:

- 分类<br />
- 编码数据库(多层感知器)<br />
- 监控访问数据(多层感知器)<br />
<a name="kXy3Y"></a>
# 2. 前馈(Feed Forward (FF)) 
![](https://cdn.nlark.com/yuque/0/2020/png/396745/1601517602050-a8de7037-6a6c-4cdd-9bee-ff3c3acc0991.png#align=left&display=inline&height=333&originHeight=333&originWidth=676&size=0&status=done&style=shadow&width=676)<br />前馈神经网络是一种其中的节点不会形成循环的人工神经网络。在这种神经网络中，所有的感知器都被安排在输入层接收输入，输出层产生输出。隐藏层与外部世界没有联系，这就是为什么它们被称为隐藏层。在前馈神经网络中，一层的每个感知器与下一层的每个节点连接。因此，所有节点都是完全连接的。需要注意的是，同一层中的节点之间没有可见或不可见的连接。在前馈网络中没有后回路。因此，为了使预测误差最小化，我们通常使用反向传播算法来更新权值。<br />应用:

- 数据压缩<br />
- 模式识别<br />
- 计算机视觉<br />
- 声纳目标识别<br />
- 语音识别<br />
- 手写字符识别<br />
<a name="QjAIU"></a>
# 3. 径向基网络(Radial Basis Network (RBN)) 
![](https://cdn.nlark.com/yuque/0/2020/png/396745/1601517602080-68bfa1a2-fc06-4a7e-8f5b-807fd4a0fd61.png#align=left&display=inline&height=330&originHeight=330&originWidth=643&size=0&status=done&style=shadow&width=643)<br />径向基函数网络通常用于解决函数逼近问题。区别于其它神经网络，它们有更快的学习速度和通用逼近能力。径向基神经网络和前馈神经网络的主要区别在于，径向基神经网络使用径向基函数作为激活函数。Logistic(sigmoid)函数的输出值在0到1之间，用来判断答案是是或否。问题是，如果我们有连续的值，则用不了前馈神经网络。径向基神经网络确定生成的输出和目标输出距离多大。在连续值的情况下非常有用。总之，径向基神经网络使用其它的激活函数表现就和前馈神经网络一样。应用:

- 函数逼近<br />
- 时间序列预测<br />
- 分类<br />
- 系统控制<br />
<a name="smzWD"></a>
# 4. 深度前馈(Deep Feed-forward (DFF)) 
![](https://cdn.nlark.com/yuque/0/2020/png/396745/1601517602071-e50edd30-03f9-4216-b64d-ae255fd599d8.png#align=left&display=inline&height=381&originHeight=381&originWidth=754&size=0&status=done&style=shadow&width=754)<br />深层前馈网络是使用多个隐藏层的前馈网络。只用一层隐藏层的主要问题是过拟合，因此通过增加隐藏层，可以减少过拟合，提高泛化能力。应用:

- 数据压缩<br />
- 模式识别<br />
- 计算机视觉<br />
- 心电图噪声滤波<br />
- 金融预测<br />
<a name="Ek41J"></a>
# 5. 循环神经网络(Recurrent Neural Network (RNN))
![](https://cdn.nlark.com/yuque/0/2020/webp/396745/1601517602061-0b3c63d6-e7ef-4c50-a784-d8098c25f7a9.webp#align=left&display=inline&height=350&originHeight=350&originWidth=831&size=0&status=done&style=shadow&width=831)<br />循环神经网络是前馈神经网络的一种改进形式。在这种类型中，隐藏层中的每个神经元接收具有特定时间延迟的输入。使用这种类型的神经网络，我们需要在当前的迭代中访问之前的信息。例如，当我们试图预测一个句子中的下一个单词时，我们首先需要知道之前使用的单词。循环神经网络可以处理输入并跨时共享任意长度和权重。模型大小不会随着输入的大小而增加，模型中的计算会考虑到历史信息。然而，这种神经网络的问题是计算速度慢。此外，它不能考虑当前状态的任何未来输入。它也无法记住很久以前的信息。应用:

- 机器翻译<br />
- 机器人控制<br />
- 时间序列预测<br />
- 语音识别<br />
- 语音合成<br />
- 时间序列异常检测<br />
- 节奏学习<br />
- 音乐创作<br />
<a name="SDcQa"></a>
# 6. 长/短期记忆(Long / Short Term Memory (LSTM)) 
![](https://cdn.nlark.com/yuque/0/2020/webp/396745/1601517602116-74b85957-77ce-4411-be7b-524c5e2c06ff.webp#align=left&display=inline&height=338&originHeight=338&originWidth=775&size=0&status=done&style=shadow&width=775)<br />LSTM 网络引入了一个记忆单元。他们可以处理间隔记忆的数据。如上可见，我们可以在RNN中考虑时间延迟，但如果我们有大量的相关数据，RNN很容易失败，而LSTMs 正好适合。另外，与 LSTMs 相比，RNN不能记忆很久以前的数据。应用:

- 语音识别<br />
- 写作识别<br />
<a name="ILQG0"></a>
# 7. 门控循环单位(Gated Recurrent Unit (GRU))
![](https://cdn.nlark.com/yuque/0/2020/png/396745/1601517602094-ef078c88-bd95-453a-880d-5cbff32de1ac.png#align=left&display=inline&height=308&originHeight=308&originWidth=737&size=0&status=done&style=shadow&width=737)<br />GRU是LSTM的一个变种，因为它们都有相似的设计，绝大多数时候结果一样好。GRU只有三个门，并且它们不维持内部单元状态。<br />a. 更新门(Update Gate): 决定有多少过去的知识可以传递给未来。<br />b. 重置门(Reset Gate): 决定过去的知识有多少需要遗忘。<br />c. 当前记忆门(Current Memory Gate): 重置命运的子部分。<br />应用:

- 复调音乐模型<br />
- 语音信号建模<br />
- 自然语言处理<br />
<a name="DNepE"></a>
# 8. 自动编码器(Auto Encoder (AE)) :
![](https://cdn.nlark.com/yuque/0/2020/png/396745/1601517602133-917296b8-6b3c-41aa-9e52-a81f7d795cb0.png#align=left&display=inline&height=378&originHeight=378&originWidth=628&size=0&status=done&style=shadow&width=628)<br />自动编码器神经网络是一个非监督式机器学习算法。在自动编码器中，隐藏神经元的数量小于输入神经元的数量。自动编码器中输入神经元的数目等于输出神经元的数目。在自动编码器网络中，我们训练它来显示输出，输出和输入尽可能接近，这迫使自动编码器找到共同的模式和归纳数据。我们使用自动编码器来更小的表示输入。我们还可以从压缩的数据中重建原始数据。该算法相对简单，因为自动编码器要求输出与输入相同。

- 编码器: 转换输入数据到低维<br />
- 解码器: 重构压缩数据<br />

应用:

- 分类<br />
- 聚类<br />
- 特征压缩<br />
<a name="Q5QTu"></a>
# 9. 变分自动编码器(Variational Autoencoder (VAE)) 
![](https://cdn.nlark.com/yuque/0/2020/webp/396745/1601517602133-e96d10b0-ef30-43a0-b6e2-99d4adbe69dd.webp#align=left&display=inline&height=410&originHeight=410&originWidth=667&size=0&status=done&style=shadow&width=667)<br />变分自动编码器(VAE)使用一种概率方法来描述观测。它显示了一个特征集中每个属性的概率分布。<br />应用:

- 在句子之间插入<br />
- 图像自动生成<br />
<a name="YIWd7"></a>
# 10. 去噪自动编码器(Denoising Autoencoder (DAE) 
![](https://cdn.nlark.com/yuque/0/2020/webp/396745/1601517602125-96d44bf4-f655-49d3-aac6-56f61047f0c5.webp#align=left&display=inline&height=376&originHeight=376&originWidth=614&size=0&status=done&style=shadow&width=614)<br />在这种自动编码器中，网络不能简单地将输入复制到其输出，因为输入也包含随机噪声。在 DAE 上，我们制造它是为了降低噪声并在其中产生有意义的数据。在这种情况下，该算法迫使隐藏层学习更鲁棒的特征，以便输出是噪声输入的更精确版本。应用:

- 特征提取<br />
- 降维<br />
<a name="0IPBn"></a>
# 11. 稀疏自动编码器(Sparse Autoencoder (SAE)) 
![](https://cdn.nlark.com/yuque/0/2020/png/396745/1601517602119-89575658-6ded-491e-8c32-0ac40fdb73a2.png#align=left&display=inline&height=387&originHeight=387&originWidth=653&size=0&status=done&style=shadow&width=653)<br />在稀疏自动编码器网络中，我们通过惩罚隐藏层的激活来构造我们的损失函数，这样当我们将一个样本输入网络时，只有少数节点被激活。这种方法背后的直觉是，例如，如果一个人声称自己是A、 B、 C 和 D 学科的专家，那么这个人可能在这些科目上更多的是一个通才。然而，如果这个人只是声称自己专注于学科D，那么大概率预期可以从这个人的学科 D 的知识中得到启发。应用:

- 特征提取<br />
- 手写数字识别<br />
<a name="d0uNO"></a>
# 12. 马尔可夫链(Markov Chain (MC)) 
![](https://cdn.nlark.com/yuque/0/2020/png/396745/1601517602145-225d1f51-a7c3-40f8-986c-0b411e76e586.png#align=left&display=inline&height=404&originHeight=404&originWidth=639&size=0&status=done&style=shadow&width=639)<br />马尔可夫链是一个基于某些概率规则经历从一种状态到另一种状态转换的数学系统。过渡到任何特定状态的概率完全取决于当前状态和经过的时间。<br />例如，一些可能的状态可以是:

- 信件<br />
- 数字<br />
- 天气情况<br />
- 棒球比分<br />
- 股票表现<br />

应用:

- 语音识别<br />
- 信息及通讯系统<br />
- 排队论<br />
- 统计学<br />
<a name="8YPov"></a>
# 13. 霍菲特网络(Hopfield Network (HN)):
![](https://cdn.nlark.com/yuque/0/2020/png/396745/1601517602224-94afb56a-82e5-49ad-ae8b-71fc01bf933d.png#align=left&display=inline&height=385&originHeight=385&originWidth=671&size=0&status=done&style=shadow&width=671)<br />在 Hopfield 神经网络中，每个神经元都与其它神经元直接相连。在这个网络中，神经元要么是开的，要么是关的。神经元的状态可以通过接受其它神经元的输入而改变。我们通常使用 Hopfield 网络来存储模式和记忆。当我们在一组模式上训练一个神经网络，它就能够识别这个模式，即使它有点扭曲或不完整。当我们提供不完整的输入时，它可以识别完整的模式，这将返回最佳的猜测。应用:

- 优化问题<br />
- 图像检测与识别<br />
- 医学图像识别<br />
- 增强 X 射线图像<br />
<a name="TPuIf"></a>
# 14. 波茨曼机(Boltzmann Machine (BM)):
![](https://cdn.nlark.com/yuque/0/2020/png/396745/1601517602155-6971aa94-9e88-4abd-a4c1-eccbc589c140.png#align=left&display=inline&height=347&originHeight=347&originWidth=641&size=0&status=done&style=shadow&width=641)<br />波茨曼机网络包括从一个原始数据集中学习一个概率分布，并使用它来推断没见过的数据。在 BM 中，有输入节点和隐藏节点，一旦所有隐藏节点的状态发生改变，输入节点就会转换为输出节点。例如: 假设我们在核电站工作，安全必须是第一位的。我们的工作是确保动力装置中的所有组件都可以安全使用——每个组件都会有相关的状态，使用布尔值1表示可用，0表示不可用。然而，还有一些组成部分，我们不可能定期测量它们的状态。<br />此外，没有数据可以告诉我们，如果隐藏的部件停止工作，发电厂什么时候会爆炸。在这种情况下，我们构建了一个模型，当组件更改其状态时，它会发出通知。这样，我们将得到通知检查该组件，并确保动力装置的安全。应用：

- 降维<br />
- 分类<br />
- 回归<br />
- 协同过滤<br />
- 特征学习<br />
<a name="qZDos"></a>
# 15. 受限玻尔兹曼机(Restricted Boltzmann Machine (RBM))
![](https://cdn.nlark.com/yuque/0/2020/png/396745/1601517602146-1c249cbc-4ab8-4132-a979-7e81217d72bb.png#align=left&display=inline&height=378&originHeight=378&originWidth=581&size=0&status=done&style=shadow&width=581)<br />RBM 是 BM 的一种变种。在这个模型中，输入层和隐藏层的神经元之间可能有对称的连接。需要注意的一点是，每一层内部都没有内部连接。相比之下，玻尔兹曼机可能有内部连接的隐藏层。这些限制让模型的训练更高效。应用:

- 过滤<br />
- 特征学习<br />
- 分类<br />
- 风险检测<br />
- 商业及经济分析<br />
<a name="ZhSWj"></a>
# 16. 深度信念网络(Deep Belief Network (DBN)) 
![](https://cdn.nlark.com/yuque/0/2020/webp/396745/1601517602204-6bb4b5f6-6db5-49cf-a036-5fd1f6586117.webp#align=left&display=inline&height=302&originHeight=302&originWidth=846&size=0&status=done&style=shadow&width=846)<br />深度信念网络包含许多隐藏层。我们可以使用无监督算法调用 DBN，因为它首先学习而不需要任何监督。DBN 中的层起着特征检测器的作用。经过无监督训练后，我们可以用监督方法训练我们的模型进行分类。我们可以将 DBN 表示为受限玻耳兹曼机(RBM)和自动编码器(AE)的组合，最后的 DBN 使用概率方法得到结果。<br />应用:

- 检索文件/图像<br />
- 非线性降维<br />
<a name="wRh26"></a>
# 17. 深度卷积网络(Deep Convolutional Network (DCN)) 
![](https://cdn.nlark.com/yuque/0/2020/webp/396745/1601517602171-990e44ab-6d03-482b-90e7-57ecadaabec9.webp#align=left&display=inline&height=470&originHeight=470&originWidth=830&size=0&status=done&style=shadow&width=830)<br />卷积神经网络是一种神经网络，主要用于图像分类、图像聚类和目标识别。DNN 允许无监督地构造层次图像表示。DNN 被用来添加更复杂的特征，以便它能够更准确地执行任务。应用:

- 识别面部，街道标志，肿瘤<br />
- 图像识别<br />
- 视频分析<br />
- 自然语言处理<br />
- 异常检测<br />
- 药物发现<br />
- 跳棋游戏<br />
- 时间序列预测<br />
<a name="GYZZE"></a>
# 18. 反卷积神经网络(Deconvolutional Neural Networks (DN))
![](https://cdn.nlark.com/yuque/0/2020/webp/396745/1601517602216-d3e19ee3-77d3-4d00-87cf-af22c91e5005.webp#align=left&display=inline&height=472&originHeight=472&originWidth=685&size=0&status=done&style=shadow&width=685)<br />反卷积网络是一种反向过程的卷积神经网络。尽管反卷积网络在执行方式上类似于 CNN，但它在 AI 中的应用是非常不同的。反卷积网络有助于在以前认为有用的网络中找到丢失的特征或信号。卷积网络可能由于与其它信号卷积而丢失信号。反卷积网络可以接受一个向量输入并还原成照片。<br />应用:

- 图像超分辨率<br />
- 图像的表面深度估计<br />
- 光流估计<br />
<a name="FQOAG"></a>
# 19. 深度卷积逆图形网络(Deep Convolutional Inverse Graphics Network (DC-IGN)) 
![](https://cdn.nlark.com/yuque/0/2020/webp/396745/1601517602151-6da0a66a-7455-41bc-b0cd-fd8f5e0161b2.webp#align=left&display=inline&height=405&originHeight=405&originWidth=866&size=0&status=done&style=shadow&width=866)<br />深度卷积逆图形网络旨在将图形表示与图像联系起来。它使用元素，如照明，对象的位置，纹理，和其它方面的图像设计来进行非常复杂的图像处理。它使用不同的层来处理输入和输出。深度卷积逆图形网络利用初始层通过各种卷积和最大池化进行编码，然后利用后续层进行展开解码。<br />应用:

- 人脸处理<br />
<a name="nEJGB"></a>
# 20. 生成对抗网络(Generative Adversarial Network (GAN)) 
![](https://cdn.nlark.com/yuque/0/2020/webp/396745/1601517602215-9d4bf453-bee1-43e8-8085-2af1c39dbbe9.webp#align=left&display=inline&height=301&originHeight=301&originWidth=873&size=0&status=done&style=shadow&width=873)<br />给定训练数据，GANs 学习用与训练数据相同的统计数据生成新的数据。例如，如果我们对 GAN 模型进行照片训练，那么一个经过训练的模型就能够生成人眼看起来真实可信的新照片。GAN的目标是区分真实结果和合成结果，以便产生更真实的结果。应用:

- 创造新的人体姿势<br />
- 照片变Emoji<br />
- 面部老化<br />
- 超分辨率<br />
- 服装变换<br />
- 视频预测<br />
<a name="TnTig"></a>
# 21. 液态机(Liquid State Machine (LSM))
![](https://cdn.nlark.com/yuque/0/2020/webp/396745/1601517602222-b5d4fe2a-8439-4d11-a9cb-9529ca2e539c.webp#align=left&display=inline&height=392&originHeight=392&originWidth=852&size=0&status=done&style=shadow&width=852)<br />液态机是一种特殊的脉冲神经网络。液态机由大量的神经元组成。这里，每个节点接收来自外部源和其它节点的输入，这些输入可能随时间而变化。请注意，液态机上的节点是随机连接的。在液态机中，激活函数替换为阈值级别。只有当液态机达到阈值水平时，一个特定的神经元才会发出输出。应用:

- 语音识别<br />
- 计算机视觉<br />
<a name="D5XbH"></a>
# 22. 极限学习机(Extreme Learning Machine (ELM)):
![](https://cdn.nlark.com/yuque/0/2020/png/396745/1601517602234-87406031-f84b-4272-9644-bd39a168c45c.png#align=left&display=inline&height=387&originHeight=387&originWidth=828&size=0&status=done&style=shadow&width=828)<br />传统系统处理大量数据的主要缺点是:

- 基于梯度算法学习速度慢<br />
- 迭代调优所有参数<br />

极限学习机随机选择隐藏节点，然后通过分析确定输出权重。因此，这些算法比一般的神经网络算法更快。另外，在极限学习机网络中，随机分配的权重通常不会更新。它只需一步就能学会输出权重。<br />应用:

- 分类<br />
- 回归<br />
- 聚类<br />
- 稀疏逼近<br />
- 特征学习<br />
<a name="vv0Yj"></a>
# 23. 回声状态网络(Echo State Network (ESN)) 
![](https://cdn.nlark.com/yuque/0/2020/png/396745/1601517602233-28c6c3e2-65b9-45d6-af24-32ba005b5583.png#align=left&display=inline&height=406&originHeight=406&originWidth=836&size=0&status=done&style=shadow&width=836)<br />ESN是循环神经网络的一个子类型。这里每个输入节点接收到一个非线性信号。在 ESN 中，隐藏的节点是稀疏连接的。隐节点的连通度和权值是随机分配的。在ESN上，最终的输出权重是可训练更新的。<br />应用:

- 时间序列预测<br />
- 数据挖掘<br />
<a name="VMphc"></a>
# 24. 深度残差网络(Deep Residual Network (DRN)) 
![](https://cdn.nlark.com/yuque/0/2020/png/396745/1601517602271-d1ed147d-f5d1-4069-92ab-bde43b37632c.png#align=left&display=inline&height=319&originHeight=319&originWidth=899&size=0&status=done&style=shadow&width=899)<br />具有多层结构的深层神经网络训练很难，且需要花费大量的时间。它也可能导致结果退化。深度残差网络即使有很多层也可以防止结果退化。使用残差网络，其输入的一些部分会传递到下一层。因此，这些网络可以相当深(它可能包含大约300层)。<br />应用:

- 图像分类<br />
- 目标检测<br />
- 语义分割<br />
- 语音识别<br />
- 语言识别<br />
<a name="dpMj4"></a>
# 25. Kohonen网络(Kohonen Networks (KN) )
![](https://cdn.nlark.com/yuque/0/2020/png/396745/1601517602251-fae9fe57-7fdc-4a14-a9c3-1e2bfed174fa.png#align=left&display=inline&height=350&originHeight=350&originWidth=672&size=0&status=done&style=shadow&width=672)<br />Kohonen 网络是一种无监督算法。Kohonen 网络也称为自组织映射，当我们的数据分散在多个维度，而我们希望它只有一个或两个维度时，这非常有用。它可以认为是一种降维的方法。我们使用 Kohonen 网络可视化高维数据。他们使用竞争学习而不是纠错学习。<br />各种拓扑结构:

- 矩形网格拓扑<br />
- 六边形网格拓扑<br />

应用：

- 降维<br />
- 水质评价与预测<br />
- 沿岸水资源管理<br />
<a name="nl2GK"></a>
# 26. 支持向量机(Support Vector Machines (SVM)):
![](https://cdn.nlark.com/yuque/0/2020/png/396745/1601517602254-50877370-15c4-43eb-8219-d265fb87669e.png#align=left&display=inline&height=320&originHeight=320&originWidth=719&size=0&status=done&style=shadow&width=719)<br />支持向量机神经网络是支持向量机和神经网络的混合算法。对于一组新的样本，它总是试图分为两类: 是或否(1或0)。支持向量机通常用于二分类。这些通常不被认为是神经网络。应用:

- 人脸检测<br />
- 文本分类<br />
- 分类<br />
- 生物信息学<br />
- 手写识别<br />
<a name="6UcKg"></a>
# 27. 神经图灵机(Neural Turing Machine (NTM)) :
![](https://cdn.nlark.com/yuque/0/2020/webp/396745/1601517602279-b01d028e-8ae6-4c61-a678-281932280761.webp#align=left&display=inline&height=524&originHeight=524&originWidth=891&size=0&status=done&style=shadow&width=891)<br />神经图灵机结构包含两个主要组件:

- 神经网络控制器<br />
- 记忆库<br />

在这个神经网络中，控制器通过输入和输出向量与外界进行交互。它还通过与记忆矩阵交互来执行选择性读写操作。图灵机被认为在计算上等同于现代计算机。因此，NTM通过与外部存储的交互，扩展了标准神经网络的能力。<br />应用:

- 机器人<br />
- 制造人造大脑<br />
